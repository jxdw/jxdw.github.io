{"meta":{"title":"思路比试错更重要","subtitle":"记录技术的学习和实践过程","description":null,"author":"游侠","url":"https://youxia999.github.io"},"pages":[{"title":"关于本人","date":"2017-05-31T02:05:56.000Z","updated":"2019-12-12T05:10:22.315Z","comments":true,"path":"about/index.html","permalink":"https://youxia999.github.io/about/index.html","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;游侠，坐标:深圳。博客主要记录和探讨通用技术架构中的后端几块技术的学习心得。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因为无论业务架构怎么变、应用架构怎么规划，技术架构的一些具体技术都不会过时(至于业务架构和应用架构，涉及所处公司的一些利益关系、行业的业务知识和术语，则尽量不涉及)。目前focus on监控技术(或者工具)、容器技术、持续集成技术、微服务治理技术的使用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果有什么要探讨的，欢迎联系我jingzhouyouxia@gmail.com。"},{"title":"分类","date":"2019-02-19T14:25:24.000Z","updated":"2019-03-05T01:48:25.574Z","comments":true,"path":"categories/index.html","permalink":"https://youxia999.github.io/categories/index.html","excerpt":"","text":""},{"title":"全部标签","date":"2017-02-19T14:38:24.000Z","updated":"2019-03-05T01:45:29.248Z","comments":false,"path":"tags/index.html","permalink":"https://youxia999.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"e6_introduce","date":"2019-12-13T13:09:14.438Z","updated":"2019-12-11T02:45:11.469Z","comments":true,"path":"2019/12/13/e6_introduce/","link":"","permalink":"https://youxia999.github.io/2019/12/13/e6_introduce/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Prometheus监控rabbitmq3.8.0","slug":"prometheus_monitor_rabbitmq","date":"2019-10-07T04:46:48.000Z","updated":"2020-02-06T04:13:50.757Z","comments":true,"path":"2019/10/07/prometheus_monitor_rabbitmq/","link":"","permalink":"https://youxia999.github.io/2019/10/07/prometheus_monitor_rabbitmq/","excerpt":"","text":"背景官方介绍链接: https://www.rabbitmq.com/prometheus.html#overview-prometheusAs of 3.8.0, RabbitMQ ships with built-in Prometheus &amp; Grafana support.Support for Prometheus metric collector ships in the rabbitmq_prometheus plugin. The plugin exposes all RabbitMQ metrics on a dedicated TCP port, in Prometheus text format.These metrics provide a deep insights into the state of RabbitMQ nodes and the runtime. They make reasoning about the behaviour of RabbitMQ, applications that use it and various infrastructure elements a lot more informed. 版本依赖 安装过程安装erlangvim /etc/yum.repos.d/rabbitmq-erlang.repo[rabbitmq-erlang]name=rabbitmq-erlangbaseurl=https://dl.bintray.com/rabbitmq-erlang/rpm/erlang/22/el/7gpgcheck=1gpgkey=https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascrepo_gpgcheck=0enabled=1 输入 命令，安装erlang。rpm -import https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascyum install erlangyum install rabbitmq-server 安装rabbitmq-servervim /etc/yum.repos.d/rabbitmq.repo[bintray-rabbitmq-server]name=bintray-rabbitmq-rpmbaseurl=https://dl.bintray.com/rabbitmq/rpm/rabbitmq-server/v3.8.x/el/7/gpgcheck=0repo_gpgcheck=0enabled=1 修改配置，并授权vim /etc/rabbitmq/rabbitmq.conf#log.dir = /data/rabbitmq/loglog.file = rabbit.loglog.file.rotation.date=$D0log.file.rotation.size=0 vim /etc/rabbitmq/rabbitmq-env.confRABBITMQ_MNESIA_BASE=/data/rabbitmq/mnesiaRABBITMQ_LOG_BASE=/data/rabbitmq/log 授权给rabbitmq用户。chown -R rabbitmq:rabbitmq /var/lib/rabbitmq/mkdir -p data/rabbitmq/logmkdir -p /data/rabbitmq/mnesiachown -R rabbitmq:rabbitmq /data/rabbitmq/ 启动服务，并启动插件1.启动rabbitmq-server服务systemctl start rabbitmq-serversystemctl status rabbitmq-server 2.启动management插件。rabbitmq-plugins enable rabbitmq_management 3.admin用户授权rabbitmqctl add_user admin adminrabbitmqctl set_user_tags admin administratorrabbitmqctl set_permissions -p / admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;rabbitmqctl status 4.查看插件输入命令:rabbitmq-plugins list 5.启动rabbitmq_prometheus插件rabbitmq-plugins enable rabbitmq_prometheus 效果命令行输入rabbitmqctl status 浏览器访问","categories":[],"tags":[{"name":"监控技术","slug":"监控技术","permalink":"https://youxia999.github.io/tags/监控技术/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"跟踪平台jaeger安装、配置","slug":"jaeger-install-config","date":"2019-10-01T11:00:00.000Z","updated":"2020-02-06T04:12:50.437Z","comments":true,"path":"2019/10/01/jaeger-install-config/","link":"","permalink":"https://youxia999.github.io/2019/10/01/jaeger-install-config/","excerpt":"","text":"Jaeger项目背景和整体架构项目背景Jaeger项目是一套受到Dapper与OpenZipkin启发构建而成的分布式跟踪平台，由Uber公司最初以开源方式公布，随后加入了云原生计算基金会。该项目主要用于:监控并诊断基于微服务架构的分布式系统，具体涵盖： 分布式上下文传播 分布式事务监控 根本原因分析 服务依赖关系分析 性能/延迟优化 整体架构 下载安装和配置安装前置条件 已经安装elasticsearch集群 安装好prometheus监控 从github下载二进制包下载地址：https://github.com/jaegertracing/jaeger/releases/download/v1.14.0/jaeger-1.14.0-linux-amd64.tar.gz 解压和配置编写collector启动脚本start-colloector.shexport SPAN_STORAGE_TYPE=elasticsearchnohup ./jaeger-collector --es.server-urls http://elasticsearch节点1的ip和端口/,http://elasticsearch节点2的ip和端口/,http=://elasticsearch节点3的ip和端口/ --log-level=debug &gt; collector.log 2&gt;&amp;1 &amp; 编写query启动脚本start-query.shexport SPAN_STORAGE_TYPE=elasticsearchnohup ./jaeger-query --span-storage.type=elasticsearch --es.server-urls=http://elasticsearch节点1的ip和端口/,http://elasticsearch节点2的ip和端口/,http=://elasticsearch节点3的ip和端口/ &gt; query.log 2&gt;&amp;1 &amp; 编写agent启动脚本start-agent.shexport SPAN_STORAGE_TYPE=elasticsearchnohup ./jaeger-agent --collector.host-port=192.168.172.9:14267 --discovery.min-peers=1 --log-level=debug &gt; agent.log 2&gt;&amp;1 &amp; 配置prometheus配置监听14269端口 启动效果","categories":[],"tags":[{"name":"链路跟踪技术","slug":"链路跟踪技术","permalink":"https://youxia999.github.io/tags/链路跟踪技术/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"apache dubbo 2.7.3版本与zipkin结合进行接口监控","slug":"apache_dubbo_zipkin_merge","date":"2019-08-11T03:02:34.000Z","updated":"2020-02-06T04:11:33.323Z","comments":true,"path":"2019/08/11/apache_dubbo_zipkin_merge/","link":"","permalink":"https://youxia999.github.io/2019/08/11/apache_dubbo_zipkin_merge/","excerpt":"","text":"本文背景因为:1.dubbo已经从apache毕业，而zipkin官方的brave-instrumentation-dubbo-rpc的5.6版本支持的dubbo版本是2.6.6版本、且对2.7.3版本支持不太友好(2.6.6这个版本是未毕业前的版本，不太喜欢），只能把brave-instrumentation-dubbo-rpc抠出来，用以支撑2.7.3版本。2.dubbo官方提供的例子还是xml配置版本，现在都9102年了，谁还用XML配置版本。因为以上两点，有了此文。 provider方工程代码添加pom依赖1.brave对dubbo的集成:brave-instrumentation-dubbo-rpc2.brave的spring bean支持:brave-spring-beans3.在SLF4J的MDC(Mapped Diagnostic Context) 中支持 traceId 和 spanId4.使用okhttp3作为 reporter:zipkin-sender-okhttp3当然，最好是同时添加bom，做好版本控制。 将TracingFilter扣出来进行改造将brave-instrumentation-dubbo-rpc的5.6.6版本的TracingFilter抠出来进行改造，即实现org.apache.dubbo.rpc.Fiter的invoke方法。brave-instrumentation-dubbo-rpc的5.6.6版本的TracingFilter实现的是com.alibaba.dubbo.rpc.Fiter。package com.youxia.userinfo.config;import brave.Span;import brave.Tracer;import brave.Tracing;import brave.internal.Platform;import brave.propagation.Propagation;import brave.propagation.TraceContext;import brave.propagation.TraceContextOrSamplingFlags;import com.alibaba.dubbo.common.Constants;import com.alibaba.dubbo.remoting.exchange.ResponseCallback;import com.alibaba.dubbo.rpc.protocol.dubbo.FutureAdapter;import org.apache.dubbo.common.extension.Activate;import org.apache.dubbo.rpc.*;import org.apache.dubbo.rpc.support.RpcUtils;import java.net.InetSocketAddress;import java.util.Map;import java.util.concurrent.Future;@Activate(group = &#123;Constants.PROVIDER, Constants.CONSUMER&#125;, value = &quot;tracing&quot;)public final class TracingFilter implements Filter &#123; Tracer tracer; TraceContext.Extractor&lt;Map&lt;String, String&gt;&gt; extractor; TraceContext.Injector&lt;Map&lt;String, String&gt;&gt; injector; volatile boolean isInit = false; public void setTracing(Tracing tracing) &#123; tracer = tracing.tracer(); extractor = tracing.propagation().extractor(GETTER); injector = tracing.propagation().injector(SETTER); isInit = true; &#125; static void parseRemoteAddress(RpcContext rpcContext, Span span) &#123; InetSocketAddress remoteAddress = rpcContext.getRemoteAddress(); if (remoteAddress == null) return; span.remoteIpAndPort(Platform.get().getHostString(remoteAddress), remoteAddress.getPort()); &#125; static void onError(Throwable error, Span span) &#123; span.error(error); if (error instanceof RpcException) &#123; span.tag(&quot;userinfo.error_code&quot;, Integer.toString(((RpcException) error).getCode())); &#125; &#125; static final Propagation.Getter&lt;Map&lt;String, String&gt;, String&gt; GETTER = new Propagation.Getter&lt;Map&lt;String, String&gt;, String&gt;() &#123; @Override public String get(Map&lt;String, String&gt; carrier, String key) &#123; return carrier.get(key); &#125; @Override public String toString() &#123; return &quot;Map::get&quot;; &#125; &#125;; static final Propagation.Setter&lt;Map&lt;String, String&gt;, String&gt; SETTER = new Propagation.Setter&lt;Map&lt;String, String&gt;, String&gt;() &#123; @Override public void put(Map&lt;String, String&gt; carrier, String key, String value) &#123; carrier.put(key, value); &#125; @Override public String toString() &#123; return &quot;Map::set&quot;; &#125; &#125;; @Override public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; if (isInit == false) return invoker.invoke(invocation); RpcContext rpcContext = RpcContext.getContext(); Span.Kind kind = rpcContext.isProviderSide() ? Span.Kind.SERVER : Span.Kind.CLIENT; final Span span; if (kind.equals(Span.Kind.CLIENT)) &#123; span = tracer.nextSpan(); injector.inject(span.context(), invocation.getAttachments()); &#125; else &#123; TraceContextOrSamplingFlags extracted = extractor.extract(invocation.getAttachments()); span = extracted.context() != null ? tracer.joinSpan(extracted.context()) : tracer.nextSpan(extracted); &#125; if (!span.isNoop()) &#123; span.kind(kind); String service = invoker.getInterface().getSimpleName(); String method =RpcUtils.getMethodName(invocation); span.name(service + &quot;/&quot; + method); parseRemoteAddress(rpcContext, span); span.start(); &#125; boolean isOneway = false, deferFinish = false; try (Tracer.SpanInScope scope = tracer.withSpanInScope(span)) &#123; Result result = invoker.invoke(invocation); if (result.hasException()) &#123; onError(result.getException(), span); &#125; isOneway = RpcUtils.isOneway(invoker.getUrl(), invocation); Future&lt;Object&gt; future = rpcContext.getFuture(); // the case on async client invocation if (future instanceof FutureAdapter) &#123; deferFinish = true; ((FutureAdapter) future).getFuture().setCallback(new TracingFilter.FinishSpanCallback(span)); &#125; return result; &#125; catch (Error | RuntimeException e) &#123; onError(e, span); throw e; &#125; finally &#123; if (isOneway) &#123; span.flush(); &#125; else if (!deferFinish) &#123; span.finish(); &#125; &#125; &#125; static final class FinishSpanCallback implements ResponseCallback &#123; final Span span; FinishSpanCallback(Span span) &#123; this.span = span; &#125; @Override public void done(Object response) &#123; span.finish(); &#125; @Override public void caught(Throwable exception) &#123; onError(exception, span); span.finish(); &#125; &#125;&#125; 在resource目录增加/META-INF/dubbo/org.apache.dubbo.rpc.Filter文件org.apache.dubbo.rpc.Filter文件的内容如下:tracing=com.youxia.userinfo.config.TracingFilter 配置zipkin客户端ZipkinConfig内容如下：package com.youxia.userinfo.config;import brave.context.slf4j.MDCScopeDecorator;import brave.propagation.CurrentTraceContext;import brave.spring.beans.CurrentTraceContextFactoryBean;import brave.spring.beans.TracingFactoryBean;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import zipkin2.codec.SpanBytesEncoder;import zipkin2.reporter.AsyncReporter;import zipkin2.reporter.Sender;import zipkin2.reporter.beans.AsyncReporterFactoryBean;import zipkin2.reporter.okhttp3.OkHttpSender;import java.util.Arrays;import java.util.concurrent.TimeUnit;@Configurationpublic class ZipkinConfig &#123; @Value(&quot;$&#123;zipkin.server.url&#125;&quot;) private String ZipkinServerUrl; @Bean(&quot;okHttpSender&quot;) public Sender okHttpSender() &#123; Sender sender= OkHttpSender.create(ZipkinServerUrl); return sender; &#125; @Bean(&quot;reporter&quot;) public AsyncReporter getAsyncReporter()&#123; AsyncReporter asyncReporter=AsyncReporter.builder(okHttpSender()).closeTimeout(50000, TimeUnit.MILLISECONDS).build(SpanBytesEncoder.JSON_V2); return asyncReporter; &#125; @Bean public AsyncReporterFactoryBean reporter(@Qualifier(&quot;okHttpSender&quot;)OkHttpSender sender)&#123; AsyncReporterFactoryBean asyncReporterFactoryBean = new AsyncReporterFactoryBean(); asyncReporterFactoryBean.setSender(sender); asyncReporterFactoryBean.setCloseTimeout(3000); return asyncReporterFactoryBean; &#125; @Bean public TracingFactoryBean getTracingBean(@Qualifier(&quot;reporter&quot;) AsyncReporter reporter)&#123; TracingFactoryBean tracingFactoryBean=new TracingFactoryBean(); tracingFactoryBean.setLocalServiceName(&quot;userinfo-service&quot;); CurrentTraceContextFactoryBean currentTraceContextFactoryBean = new CurrentTraceContextFactoryBean(); CurrentTraceContext.ScopeDecorator scopeDecorator = MDCScopeDecorator.create(); currentTraceContextFactoryBean.setScopeDecorators(Arrays.asList(scopeDecorator)); tracingFactoryBean.setCurrentTraceContext(currentTraceContextFactoryBean.getObject()); tracingFactoryBean.setSpanReporter(reporter); return tracingFactoryBean; &#125;&#125; dubbo注解文件添加filter业务实现类添加filter：package com.youxia.userinfo.service.impl;import com.alibaba.dubbo.config.annotation.Service;import com.youxia.userinfo.domain.User;import com.youxia.userinfo.service.UserService;@Service(filter = &#123;&quot;tracing&quot;&#125;)public class UserServiceImpl implements UserService &#123; @Override public User saveUser(User user) &#123; user.setUserId(1); user.setUserName(user.getUserName()); System.out.println(user.toString()); return user; &#125;&#125; provider配置文件完整的application.properties:spring.application.name=UserInfoServiceserver.address=10.3.20.57server.port=28081dubbo.registry.address=redis://192.168.172.4:6380dubbo.protocol.name=dubbodubbo.protocol.port=28080dubbo.scan.base-packages=com.youxia.userinfo.servicedubbo.application.qos.enable=truezipkin.server.url=http://192.168.172.6:9411/api/v2/spans consumer工程代码业务逻辑处理类代码package com.youxia.service.user;import com.youxia.userinfo.domain.User;import com.youxia.userinfo.service.UserService;import org.apache.dubbo.config.annotation.Reference;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Service;@Servicepublic class UserServiceImpl &#123; private static final Logger LOGGER = LoggerFactory.getLogger(UserServiceImpl.class); @Reference private UserService userService; public User sayHello(User user) &#123; return userService.saveUser(user); &#125;&#125; application.properties文件内容server.port=28082server.address=10.3.20.57dubbo.registry.protocol=redisdubbo.registry.address=redis://192.168.172.4:6380dubbo.application.name=dubbo-demo-servicedubbo.scan.base-packages=com.youxia.service.user 最终效果 完整代码地址https://github.com/youxia999/spring_boot_service/apache_dubbo_zipkin_project.git","categories":[],"tags":[{"name":"链路跟踪技术","slug":"链路跟踪技术","permalink":"https://youxia999.github.io/tags/链路跟踪技术/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【转载】jira7.13版本破解与restful api","slug":"cicd-jira-crack-restfulapi","date":"2019-06-30T11:00:00.000Z","updated":"2020-02-06T04:15:12.843Z","comments":true,"path":"2019/06/30/cicd-jira-crack-restfulapi/","link":"","permalink":"https://youxia999.github.io/2019/06/30/cicd-jira-crack-restfulapi/","excerpt":"","text":"jira的restful api官方参考文档https://docs.atlassian.com/software/jira/docs/api/REST/7.13.0/ jira的其他参考资料https://blog.csdn.net/five3/article/details/7181655https://blog.csdn.net/liumiaocn/article/details/81301550 jira7.13破解","categories":[],"tags":[{"name":"持续集成技术","slug":"持续集成技术","permalink":"https://youxia999.github.io/tags/持续集成技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【转载】TCP通信转UDP通信并加速","slug":"centos7-share-a-photo","date":"2019-06-01T13:46:40.000Z","updated":"2020-02-06T04:15:12.803Z","comments":true,"path":"2019/06/01/centos7-share-a-photo/","link":"","permalink":"https://youxia999.github.io/2019/06/01/centos7-share-a-photo/","excerpt":"","text":"附原文","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"redis5.0的sentinel模式体验","slug":"redis-sentinel-install","date":"2019-05-19T02:43:17.000Z","updated":"2020-02-06T04:16:21.884Z","comments":true,"path":"2019/05/19/redis-sentinel-install/","link":"","permalink":"https://youxia999.github.io/2019/05/19/redis-sentinel-install/","excerpt":"","text":"参考资料https://www.cnblogs.com/ibethfy/p/9965902.htmlhttp://redisdoc.com/topic/sentinel.html sentinel架构和整体规划sentinel模式的整体架构 部署整体规划redis-server节点一主两从(生产环境不要部署在一台机器上，从概率学的角度，风险很大):master: 192.168.172.4:7001slave1: 192.168.172.4:7002slave2: 192.168.172.4:7003 redis-sentinel节点三个节点(生产环境不要部署在一台机器上，从概率学的角度，风险很大):node1: 192.168.172.7:27001node2: 192.168.172.7:27002mode3: 192.168.172.7:27003 编译、安装、配置和启动编译安装编译安装参考单节点安装redis 5.0集群。 配置master节点配置cat /data/redis/conf/redis7001.conf#redis监听的本地IP地址bind 192.168.172.4#监听端口port 7001#开启后台运行，no表示运行在前台daemonize yes#pid文件，另一个节点改为7002pidfile /var/run/redis_7001.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7001.aof&quot;#开启集群，把注释去掉cluster-enabled nodbfilename dump7001.rdb slave1节点配置cat /data/redis/conf/redis7002.conf#redis监听的本地IP地址bind 192.168.172.4#监听端口port 7002#开启后台运行，no表示运行在前台daemonize yes#pid文件，另一个节点改为7002pidfile /var/run/redis_7002.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7002.aof&quot;#开启集群，把注释去掉cluster-enabled nodbfilename dump7002.rdbreplicaof 192.168.172.4 7001 slave2节点配置cat /data/redis/conf/redis7003.conf#redis监听的本地IP地址bind 192.168.172.4#监听端口port 7003#开启后台运行，no表示运行在前台daemonize yes#pid文件，另一个节点改为7002pidfile /var/run/redis_7003.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7003.aof&quot;#开启集群，把注释去掉cluster-enabled nodbfilename dump7003.rdbreplicaof 192.168.172.4 7001 sentinel服务节点1配置cat conf/sentinel27001.confprotected-mode noport 27001daemonize yespidfile &quot;/data/redis/sentinel27001/sentinel.pid&quot;logfile &quot;/data/redis/sentinel27001/27001.log&quot;dir &quot;/data/redis/sentinel27001&quot;#sentinel auth-pass master7000 ibethfysentinel monitor master7001 192.168.172.4 7001 2# Generated by CONFIG REWRITEsentinel down-after-milliseconds master7001 5000sentinel failover-timeout master7001 30000 sentinel服务节点2配置cat conf/sentinel27002.confprotected-mode noport 27002daemonize yespidfile &quot;/data/redis/sentinel27002/sentinel.pid&quot;logfile &quot;/data/redis/sentinel27002/27002.log&quot;dir &quot;/data/redis/sentinel27002&quot;#sentinel auth-pass master7000 ibethfysentinel monitor master7001 192.168.172.4 7001 2# Generated by CONFIG REWRITEsentinel down-after-milliseconds master7001 5000sentinel failover-timeout master7001 30000 sentinel服务节点3配置cat conf/sentinel27003.confprotected-mode noport 27003daemonize yespidfile &quot;/data/redis/sentinel27003/sentinel.pid&quot;logfile &quot;/data/redis/sentinel27003/27003.log&quot;dir &quot;/data/redis/sentinel27003&quot;#sentinel auth-pass master7001 ibethfysentinel monitor master7001 192.168.172.4 7001 2# Generated by CONFIG REWRITEsentinel down-after-milliseconds master7001 5000sentinel failover-timeout master7001 30000 疑惑在没有相互配置地址的时候，三个sentinel节点之间怎么相互发现呢。从官网看下说明： 启动服务启动master节点：redis-server /data/redis/conf/redis7001.conf 启动slave1节点：redis-server /data/redis/conf/redis7002.conf 启动slave2节点：redis-server /data/redis/conf/redis7003.conf 启动sentinel节点1：redis-sentinel /data/redis/conf/sentinel27001.conf 启动sentinel节点2：redis-sentinel /data/redis/conf/sentinel27002.conf 启动sentinel节点3：redis-sentinel /data/redis/conf/sentinel27003.conf 体验redis主从情况 sentinel服务情况 sentinel之间怎么相互发现的","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://youxia999.github.io/tags/redis/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"关于java学习的阶段性思考","slug":"spring-architecture","date":"2019-05-12T13:46:07.000Z","updated":"2019-12-27T11:59:45.645Z","comments":true,"path":"2019/05/12/spring-architecture/","link":"","permalink":"https://youxia999.github.io/2019/05/12/spring-architecture/","excerpt":"","text":"最近有学PHP的朋友想转到java（说公司要求用spring boot做项目），问我有没有什么捷径或者经验，给他一个换语言的学习指引。看得出来，他还没有搭建自己的知识体系，我就趁机给他灌输一个观念：先要有完整的知识体系结构（或者通用技术架构），然后编程语言只是工具，是可以被替换的。当然，这不是我的原创，是一些前辈告诉我的（我相信很多高手都会觉得:语言都是工具,架构模式才是关键。用java写的，用go语言、Python也可以，只是效果、代码管理、维护性的差异）。当然，计算机是讲究动手的学科，不能空谈理论，得输出点东西才可信，然后基于本人最近的一些思考（看了几张别人spring全家桶教程的ppt），画了一张整体架构图给朋友。并且告知：java是可以被换成go、python语言实现的，就看技术决策者熟悉哪门语言、以及要实现什么样的系统（其实是废话：单讲语言生态，目前java确实是生态最好的语言，尤其是在spring的助攻下。呵呵）","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"RHEL8发布，坐等centos8","slug":"centos8-base-rhel8-release","date":"2019-05-12T05:24:41.000Z","updated":"2020-02-06T04:15:12.824Z","comments":true,"path":"2019/05/12/centos8-base-rhel8-release/","link":"","permalink":"https://youxia999.github.io/2019/05/12/centos8-base-rhel8-release/","excerpt":"","text":"背景和参考资料参考资料https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/8.0_release_notes/indexhttps://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/8/html-single/8.0_release_notes/index(机器翻译版)https://blog.51cto.com/vanehsuan/2392207https://www.itzgeek.com/how-tos/linux/centos-how-tos/how-to-install-mysql-8-0-on-rhel-8.html 背景正文上面的参考资料所说，5月7号，红帽子公司发布了REDHAT ENTERPRISE LINUX 8(简称RHEL8)。于是乎，想先体验一把新特性和原有功能，为后面的centos8做技术储备。 rhel8的release note 虚拟机安装RHEL8下载RHEL8镜像注册一个redhat的账号，从官网下载RHEL8 虚拟机安装RHEL8参考https://blog.51cto.com/vanehsuan/2392207做下版本的选择，磁盘的选择，整个过程还是比较顺畅。 #安装后的初步体验 内核版本采用的之前beta版本用的4.18版本（内核社区好像没有提供本版本的长期支持版） 注册账号订阅仓库subscription-manager register --username 红帽子账号 --password 密码 --auto-attachsubscription-manager repos --list-enabled 安装mysql8.0简单安装dnf clean packagesyum -y install @mysqlsystemctl start mysqld 查看日志2019-05-12T12:45:20.890020Z 0 [System] [MY-013169] [Server] /usr/libexec/mysqld (mysqld 8.0.13) initializing of server in progress as process 401132019-05-12T12:45:23.643786Z 5 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.2019-05-12T12:45:26.199807Z 0 [System] [MY-013170] [Server] /usr/libexec/mysqld (mysqld 8.0.13) initializing of server has completed2019-05-12T12:45:28.821217Z 0 [System] [MY-010116] [Server] /usr/libexec/mysqld (mysqld 8.0.13) starting as process 401592019-05-12T12:45:29.877569Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.2019-05-12T12:45:29.987015Z 0 [System] [MY-010931] [Server] /usr/libexec/mysqld: ready for connections. Version: &apos;8.0.13&apos; socket: &apos;/var/lib/mysql/mysql.sock&apos; port: 3306 Source distribution.2019-05-12T12:45:30.206112Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: &apos;/var/lib/mysql/mysqlx.sock&apos; bind-address: &apos;::&apos; port: 33060 知道mysql8的root账号密码为空，所以需要登录后，初始化密码。alter user &apos;root&apos;@&apos;localhost&apos; identified with mysql_native_password by &apos;123456&apos;;update user set host=&apos;%&apos; where user=&apos;root&apos;;flush privileges; 初步结论 软件安装这块，除了增加appstream的概念，已经源的控制，和centos7没有太大的差别。","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"单节点安装redis 5.0集群","slug":"redis-5-0-install","date":"2019-04-19T08:46:12.000Z","updated":"2020-02-06T04:16:21.839Z","comments":true,"path":"2019/04/19/redis-5-0-install/","link":"","permalink":"https://youxia999.github.io/2019/04/19/redis-5-0-install/","excerpt":"","text":"参考资料主要参考资料https://blog.51cto.com/andyxu/2319767https://www.oschina.net/news/100931/redis-5-0-released redis 5.0主要的特性最吸引我的可能就是第二点。要知道基于ruby的集群方案，真的比较让人酸爽。 安装过程下载、解压、编译yum -y install make gcc gcc-c++ wgetwget http://download.redis.io/releases/redis-5.0.0.tar.gztar zxvf redis-5.0.0.tar.gzcd redis-5.0.0/make &amp;&amp; make install PREFIX=/data/redisecho &quot;export PATH=$PATH:/data/redis/bin&quot; &gt;&gt; /etc/profilesource /etc/profile redis客户端命令 redis集群配置配置文件7001端口#redis监听的本地IP地址bind 192.168.128.54#监听端口port 7001#开启后台运行，no表示运行在前台daemonize yes#pid文件，另一个节点改为7002pidfile /var/run/redis_7001.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7001.aof&quot;#开启集群，把注释去掉cluster-enabled yesdbfilename dump7001.rdb#集群的配置文件，首次启动会自动创建cluster-config-file nodes-7001.conf #集群节点连接超时时间，15秒cluster-node-timeout 15000 7002端口#redis监听的本地IP地址bind 192.168.128.54#监听端口port 7002#开启后台运行，no表示运行在前台daemonize yes#pid文件pidfile /var/run/redis_7002.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7002.aof&quot;dbfilename dump7002.rdb#开启集群，把注释#去掉cluster-enabled yes#集群的配置文件，首次启动会自动创建cluster-config-file nodes-7002.conf#集群节点连接超时时间，15秒cluster-node-timeout 15000 7003端口#redis监听的本地IP地址bind 192.168.128.54#监听端口port 7003#开启后台运行，no表示运行在前台daemonize yes#pid文件，另一个节点改为7002pidfile /var/run/redis_7003.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7003.aof&quot;dbfilename dump7003.rdb#开启集群，把注释#去掉cluster-enabled yes#集群的配置文件，首次启动会自动创建cluster-config-file nodes-7003.conf#集群节点连接超时时间，15秒cluster-node-timeout 15000 7004端口#redis监听的本地IP地址bind 192.168.128.54#监听端口port 7004#开启后台运行，no表示运行在前台daemonize yes#pid文件，另一个节点改为7002pidfile /var/run/redis_7004.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7004.aof&quot;dbfilename dump7004.rdb#开启集群，把注释#去掉cluster-enabled yes#集群的配置文件，首次启动会自动创建cluster-config-file nodes-7004.conf#集群节点连接超时时间，15秒cluster-node-timeout 15000 7005端口#redis监听的本地IP地址bind 192.168.128.54#监听端口port 7005#开启后台运行，no表示运行在前台daemonize yes#pid文件pidfile /var/run/redis_7005.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7005.aof&quot;dbfilename dump7005.rdb#开启集群，把注释#去掉cluster-enabled yes#集群的配置文件，首次启动会自动创建cluster-config-file nodes-7005.conf#集群节点连接超时时间，15秒cluster-node-timeout 15000 7006端口#redis监听的本地IP地址bind 192.168.128.54#监听端口port 7006#开启后台运行,no表示运行在前台daemonize yes#pid文件pidfile /var/run/redis_7006.pid#开启aof日志，每次写操作都会记录一条日志appendonly yesappendfilename &quot;appendonly7006.aof&quot;dbfilename dump7006.rdb#开启集群，把注释去掉cluster-enabled yes #集群的配置文件，首次启动会自动创建cluster-config-file nodes-7006.conf#集群节点连接超时时间，15cluster-node-timeout 15000 节点启动脚本redis-all.sh#!/bin/bash/data/redis/bin/redis-server /data/redis/conf/redis7001.conf/data/redis/bin/redis-server /data/redis/conf/redis7002.conf/data/redis/bin/redis-server /data/redis/conf/redis7003.conf/data/redis/bin/redis-server /data/redis/conf/redis7004.conf/data/redis/bin/redis-server /data/redis/conf/redis7005.conf/data/redis/bin/redis-server /data/redis/conf/redis7006.conf 集群创建脚本redis-cluster.sh#!/bin/bash/data/redis/bin/redis-cli --cluster create 192.168.128.54:7001 192.168.128.54:7002 192.168.128.54:7003 192.168.128.54:7004 192.168.128.54:7005 192.168.128.54:7006 --cluster-replicas 1 启动后的文件信息 体验redis集群","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://youxia999.github.io/tags/redis/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"percona-xtrabackup 8.0安装和初步使用","slug":"percona-xtrabackup-for-mysql-8-0","date":"2019-04-13T04:58:32.000Z","updated":"2020-02-06T04:16:21.774Z","comments":true,"path":"2019/04/13/percona-xtrabackup-for-mysql-8-0/","link":"","permalink":"https://youxia999.github.io/2019/04/13/percona-xtrabackup-for-mysql-8-0/","excerpt":"","text":"参考参考资料https://blog.csdn.net/wfs1994/article/details/80396604 官方文档https://www.percona.com/doc/percona-xtrabackup/8.0/index.htmlhttps://www.percona.com/doc/percona-xtrabackup/8.0/installation.html#installing-percona-xtrabackup-from-repositorieshttps://www.percona.com/doc/percona-xtrabackup/8.0/installation/yum_repo.htmlhttps://www.percona.com/doc/percona-xtrabackup/8.0/using_xtrabackup/privileges.htmlhttps://www.percona.com/doc/percona-xtrabackup/8.0/backup_scenarios/full_backup.html xtrabackup简介与安装xtrabackup简介 安装wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-8.0.4/binary/redhat/7/x86_64/percona-xtrabackup-80-8.0.4-1.el7.x86_64.rpmyum localinstall percona-xtrabackup-80-8.0.4-1.el7.x86_64.rpm 授权和备份操作授权操作CREATE USER &apos;backupuser&apos;@&apos;%&apos; IDENTIFIED with mysql_native_password BY &apos;123456&apos;;GRANT BACKUP_ADMIN, PROCESS, RELOAD, LOCK TABLES, REPLICATION CLIENT ON *.* TO &apos;backupuser&apos;@&apos;%&apos;;GRANT SELECT ON performance_schema.log_status TO &apos;backupuser&apos;@&apos;%&apos;;FLUSH PRIVILEGES; 备份操作xtrabackup --host=192.168.128.54 --port=3306 --user=backupuser --password=123456 --backup --target-dir=/data/backup/percona8.0 查看备份结果 适用场景percona xtrabackup 8.0适合mysql/percona 8.0版本，如果是mysql 5.6/5.7版本，请参考官方的xtrabackup 2.4版本。 未完待续。","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://youxia999.github.io/tags/mysql/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"centos7内核升级","slug":"centos7-kernel-upgrade","date":"2019-04-01T13:12:52.000Z","updated":"2020-02-06T04:15:12.762Z","comments":true,"path":"2019/04/01/centos7-kernel-upgrade/","link":"","permalink":"https://youxia999.github.io/2019/04/01/centos7-kernel-upgrade/","excerpt":"","text":"参考资料https://linux.cn/article-8310-1.html 背景基于以下三点，让我有种升级内核的冲动：1.docker容器近期暴露的漏洞，问了”专家”说是都是内核的问题。2.centos7还是用的3.10，centos8（RHEL用的是4.18版本）发布时间表还不知道。3.linux的内核稳定版本已经到5.0.X。 升级步骤检查已安装版本 修改下hostname先。 升级内核 然后reboot。 结果 建议在新操作系统安装好以后，就先yum update、然后升级内核。再安装其他的数据库、中间件、(java、go、python、ruby、erlang)语言runtime。","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"rhel8 beta尝鲜","slug":"rhel8-beta-experience","date":"2019-03-31T14:54:07.000Z","updated":"2020-02-06T04:15:13.151Z","comments":true,"path":"2019/03/31/rhel8-beta-experience/","link":"","permalink":"https://youxia999.github.io/2019/03/31/rhel8-beta-experience/","excerpt":"","text":"参考资料https://zocodev.com/red-hat-enterprise-linux-8-beta.htmlhttps://zhuanlan.zhihu.com/p/56892392http://www.sohu.com/a/294442178_194621https://ywnz.com/linuxjc/3705.htmlhttps://www.kclouder.cn/rhel8-beta-experience/ 安装过程安装过程概述 下载iso文件，准备安装 安装VMware 15.0 pro，并修改配置文件 初步安装完成后体验，发现网络没有配置，要配置网络 初步安装完成后体验，发现没有界面，要安装Workstation，而要安装Workstation，就需要先有注册帐号 下载iso文件，准备安装本人从 https://access.cdn.redhat.com/content/origin/files/sha256/06/06bec9e7de3ebfcdb879804be8c452b69ba3e046daedac3731e1ccd169cfd316/rhel-8.0-beta-1-x86_64-dvd.iso?_auth_=1556668800_f465a37d228dfb88281c7cf7e9f8446f 下载的iso文件。 安装VMware15.0，并修改配置文件安装VMware15.0，并加载iso文件，安装rhel8beta。会发现找不到本地磁盘，需要参考 https://ywnz.com/linuxjc/3705.html 修改配置文件，并继续安装。 根据虚拟机的nat配置，配置网络安装完成后，进入虚拟机体验，会发现ifconfig用不了，只能用ip a。而网卡下没有ip。也就是在vmware之外，无法连接rhel服务，进行操作。这时候只能参考知乎专栏进行相应的操作。ip段可以看vmware的nat网络设置。nmcli c add type ethernet con-name ethX ifname ethX ipv4.addr 192.168.197.128 ipv4.gateway 192.168.197.1 ipv4.method manualnmcli c up ethX 这时候，可以用mobaxterm等终端进行远程操作了。 下载yum源看了/etc/yum.repo.d/下面，基本就是空的，所以得想办法找一个yum源。谷歌一番，发现，只能用redhat官方的yum源。只能参考 http://www.sohu.com/a/294442178_194621 去清华大学的镜像服务器上下载并上传到对应的目录。（注意不需要修改里面的内容） 执行一个yum命令，就会提示订阅 执行命令订阅查找一番，只能去redhat官网注册帐号，并订阅。 subscription-manager register --username 帐号 --password &apos;密码&apos; --auto-attach 安装图形界面yum groupinstall Workstationsystemctl set-default graphical#默认启用 体验感受 yum被替换成dnf ifconfig被替换成了nmcli，有点小难受","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"gin源码分析第一版","slug":"golang-gin-source-code-v1","date":"2019-03-24T04:48:30.000Z","updated":"2019-12-27T11:59:45.497Z","comments":true,"path":"2019/03/24/golang-gin-source-code-v1/","link":"","permalink":"https://youxia999.github.io/2019/03/24/golang-gin-source-code-v1/","excerpt":"","text":"demo源码package mainimport ( &quot;encoding/json&quot; &quot;fmt&quot; &quot;github.com/gin-gonic/gin&quot;)type User struct &#123; Id int Name string Age int&#125;func main() &#123; defaultServer:=gin.New(); defaultServer.RouterGroup.GET(&quot;/&quot;,defaultHandeler) v1 := defaultServer.Group(&quot;/v1&quot;) v1.GET(&quot;/ping&quot;,v1pingHandeler) defaultServer.Run(&quot;:8083&quot;)&#125;func defaultHandeler(ctx *gin.Context) &#123; ctx.JSON(200,gin.H&#123; &quot;status&quot;:200, &quot;success&quot;:true, &quot;data&quot;:&quot;&#123;&#125;&quot;, &#125;)&#125;func v1pingHandeler(context *gin.Context) &#123; user:=User&#123;1,&quot;golang&quot;,20&#125; data,_:=json.Marshal(user) fmt.Println(user,data) context.JSON(200,gin.H&#123; &quot;status&quot;:200, &quot;success&quot;:true, &quot;data&quot;:string(data), &#125;) //context.Redirect(http.StatusMovedPermanently,&quot;https://github.com/gin-goinc/gin&quot;)&#125; 时序图","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"golang","slug":"golang","permalink":"https://youxia999.github.io/tags/golang/"}]},{"title":"Percona Server8.0安装和初步使用","slug":"percona-server-for-mysql-8-0-install","date":"2019-03-04T02:43:32.000Z","updated":"2020-02-06T04:16:21.763Z","comments":true,"path":"2019/03/04/percona-server-for-mysql-8-0-install/","link":"","permalink":"https://youxia999.github.io/2019/03/04/percona-server-for-mysql-8-0-install/","excerpt":"","text":"本文参考资料https://blog.csdn.net/vkingnew/article/details/85220187https://www.mayi888.com/archives/59253https://www.percona.com/doc/percona-server/LATEST/installation/yum_repo.htmlhttps://blog.csdn.net/myNameIssls/article/details/84031426 安装过程参考 https://youxia999.github.io/2016/12/21/mysql5-7-install/ ，安装软件。 安装需要的软件wget https://repo.percona.com/release/7Server/RPMS/x86_64/jemalloc-3.6.0-1.el7.x86_64.rpmrpm -ivh jemalloc-3.6.0-1.el7.x86_64.rpm 删除废弃的软件包rpm -e --nodeps mariadb-libs 按照需要的软件：yum -y install net-tools libaio perl openssl openssl-devel perl-Data-Dumper perl-JSONwget https://www.percona.com/downloads/Percona-Server-8.0/Percona-Server-8.0.13-3/binary/redhat/7/x86_64/Percona-Server-8.0.13-3-ra920dd6-el7-x86_64-bundle.tar 解压，查看软件包列表：ll percona*.rpm 安装过程：rpm -ivh percona-*.rpm 创建目录mkdir -p /data/mysql/&#123;log,binlogs,run,data&#125;touch /data/mysql/run/mysqld.pidchown -R mysql:mysql /data/mysql 初始化mysqld --initialize-insecure --user=mysql --basedir=/data/mysql --datadir=/data/mysql/data 配置文件/etc/my.cnf# Percona Server template configuration## For advice on how to change settings please see# http://dev.mysql.com/doc/refman/8.0/en/server-configuration-defaults.html[mysqld]## Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M## Remove the leading &quot;# &quot; to disable binary logging# Binary logging captures changes between backups and is enabled by# default. It&apos;s default setting is log_bin=binlog# disable_log_bin## Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2M## Remove leading # to revert to previous value for default_authentication_plugin,# this will increase compatibility with older clients. For background, see:# https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_default_authentication_plugin# default-authentication-plugin=mysql_native_passwordport = 3306#basedir = /data/mysql这里注释掉，如果不注释掉，percona会报错：datadir = /data/mysql/datasocket = /data/mysql/mysql.socklog-error = /data/mysql/log/mysqld.logpid-file = /data/mysql/run/mysqld.pidlog-bin = /data/mysql/binlogs/mysql-binslow_query_log = 1slow_query_log_file = /data/mysql/log/mysql_slow_query.loglong_query_time = 5symbolic-links=0character_set_server=utf8mb4collation_server=utf8mb4_unicode_ciskip-character-set-client-handshakeinnodb_undo_log_truncate=off#允许时间字段为&quot;0000-00-00 00:00:00&quot;sql_mode=&apos;STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&apos;#打开函数功能log_bin_trust_function_creators=1# generaltable_open_cache = 200000table_open_cache_instances=64back_log=3500max_connections=110000# filesinnodb_file_per_tableinnodb_log_file_size=1Ginnodb_log_files_in_group=2innodb_open_files=4000# buffersinnodb_buffer_pool_size= 2Ginnodb_buffer_pool_instances=8innodb_log_buffer_size=256M# tuneinnodb_doublewrite= 1innodb_thread_concurrency=0innodb_flush_log_at_trx_commit= 0innodb_flush_method=O_DIRECT_NO_FSYNCinnodb_max_dirty_pages_pct=90innodb_max_dirty_pages_pct_lwm=10innodb_lru_scan_depth=2048innodb_page_cleaners=4join_buffer_size=256Ksort_buffer_size=256Kinnodb_use_native_aio=1innodb_stats_persistent = 1innodb_spin_wait_delay=96innodb_adaptive_flushing = 1innodb_flush_neighbors = 0innodb_read_io_threads = 16innodb_write_io_threads = 16innodb_io_capacity=1500innodb_io_capacity_max=2500innodb_purge_threads=4innodb_adaptive_hash_index=0max_prepared_stmt_count=1000000innodb_monitor_enable = ‘%’performance_schema = ON 启动systemctl start mysqld.servicesystemctl status mysqld.service 初始化密码和权限mysql -h 127.0.0.1 -u rootalter user &apos;root&apos;@&apos;localhost&apos; identified with mysql_native_password by &apos;123456&apos;; --查询了资料，percona 8.0的密码插件已变成caching_sha2_passwordUPDATE user SET Host = &apos;%&apos; WHERE User = &apos;root&apos;;flush privileges; 体验","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://youxia999.github.io/tags/mysql/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"golang 1.12初步体验","slug":"golang-1-12-experience","date":"2019-02-26T11:10:08.000Z","updated":"2019-12-27T11:59:45.484Z","comments":true,"path":"2019/02/26/golang-1-12-experience/","link":"","permalink":"https://youxia999.github.io/2019/02/26/golang-1-12-experience/","excerpt":"","text":"背景golang 1.12于2月25号发布了，迫不及待的体验了一把。除了下载golang1.12，goland也更下2018.3.4。 体验把之前的项目（1.11版本开发）的mod更新了，设置了把代理（不设置代理，就要用replace）。 然后体验了一把go list -m -json all。贼快了。有点激动哈。重温下go mod相关的命令。","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"golang","slug":"golang","permalink":"https://youxia999.github.io/tags/golang/"}]},{"title":"spring框架系列之spring cloud sleuth","slug":"spring-cloud-2x-cloud-sleuth","date":"2019-02-09T09:15:36.000Z","updated":"2020-02-06T04:14:27.035Z","comments":true,"path":"2019/02/09/spring-cloud-2x-cloud-sleuth/","link":"","permalink":"https://youxia999.github.io/2019/02/09/spring-cloud-2x-cloud-sleuth/","excerpt":"","text":"关于spring cloud 2.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service、prometheus client，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client。","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"docker 1.17安装和初步使用","slug":"docker-1-17-install","date":"2019-02-07T02:51:23.000Z","updated":"2020-02-06T04:15:12.932Z","comments":true,"path":"2019/02/07/docker-1-17-install/","link":"","permalink":"https://youxia999.github.io/2019/02/07/docker-1-17-install/","excerpt":"","text":"参考资料1.docker-ce 1.17的relase noteshttps://docs.docker.com/v17.12/release-notes/docker-ce/2.docker engine apihttps://docs.docker.com/engine/api/v1.19/#21-containers 安装过程yum install -y yum-utils device-mapper-persistent-data lvm2yum install -y http://mirror.centos.org/centos/7/os/x86_64/Packages/libseccomp-2.3.1-3.el7.x86_64.rpmyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum-config-manager --enable docker-ce-edgeyum-config-manager --enable docker-ce-testyum makecache fastyum list docker-ce --showduplicates|sort -ryum-config-manager --disable docker-ce-testyum-config-manager --disable docker-ce-edge 配置过程docker1.13.1之后的版本，和之前有很大的不同，有些参数都丢弃了,可以通过dockerd COMMAND –help docker.service配置如下：[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerd --storage-driver=devicemapper --exec-opt native.cgroupdriver=systemd -D --pidfile=/var/run/docker.pidExecReload=/bin/kill -s HUP $MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process# restart the docker process if it exits prematurelyRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target daemon.json&#123; &quot;data-root&quot;:&quot;/data/docker&quot;, &quot;registry-mirror&quot;: [ &quot;https://csokz3oi.mirror.aliyuncs.com&quot; ], &quot;hosts&quot;: [&quot;unix:///var/run/docker.sock&quot;,&quot;tcp://10.2.1.31:2375&quot;]&#125; 启动和相关操作启动命令：systemctl daemon-reloadsystemctl restart docker.service api操作：","categories":[],"tags":[{"name":"restful api","slug":"restful-api","permalink":"https://youxia999.github.io/tags/restful-api/"},{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"spring cloud解决方案之spring-cloud-stream","slug":"spring-cloud-2x-cloud-stream","date":"2019-02-02T09:44:15.000Z","updated":"2020-02-06T04:14:33.514Z","comments":true,"path":"2019/02/02/spring-cloud-2x-cloud-stream/","link":"","permalink":"https://youxia999.github.io/2019/02/02/spring-cloud-2x-cloud-stream/","excerpt":"","text":"关于spring cloud 2.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring dat redis client、spring web service、prometheus client，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client。 个人研判此module不具备商用要求。","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"prometheus实现动态监控","slug":"prometheus-confd-etcd","date":"2019-01-30T11:45:49.000Z","updated":"2019-12-28T13:06:03.622Z","comments":true,"path":"2019/01/30/prometheus-confd-etcd/","link":"","permalink":"https://youxia999.github.io/2019/01/30/prometheus-confd-etcd/","excerpt":"","text":"参考资料http://www.net-add.com/a/dockerzhuanti/fuwuzhucefaxian/2017/0421/42.html 下载、安装etcd及其可视化工具下载安装etcdetcd的下载安装参考：https://youxia999.github.io/2018/12/05/container-etcd-install/ 下载安装etcdkeeperetcdkeeper的安装参考：https://github.com/evildecay/etcdkeeper 下载、启动2.x版本的Prometheusdocker run –name prometheus -p 9090:9090 -e TZ=”Asia/Shanghai” –link alertmanager -v /etc/prometheus/:/etc/prometheus/ -d prom/prometheus –web.enable-lifecycle –config.file=/etc/prometheus/prometheus.yml 下载、配置confd从github下载confd0.16.0https://github-production-release-asset-2e65be.s3.amazonaws.com/13234395/625c633c-4fef-11e8-88cf-9ef24f2771cc?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191226%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20191226T020617Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=031584e37ca8ed73a665e94df2e7ebfc8ff13292c71915176e3e06d8f7366a74&amp;X-Amz-SignedHeaders=host&amp;actor_id=17592332&amp;response-content-disposition=attachment%3B%20filename%3Dconfd-0.16.0-linux-amd64&amp;response-content-type=application%2Foctet-stream confd的配置文件/etc/confd/confd.tomlbackend = &quot;etcdv3&quot;confdir = &quot;/etc/confd&quot;log-level = &quot;debug&quot;interval = 5nodes = [&quot;http://192.168.1.5:2379&quot;,&quot;http://192.168.1.6:2379&quot;,&quot;http://192.168.1.7:2379&quot;]scheme = &quot;http&quot;watch = true 配置prometheus.conf.toml[template]prefix = &quot;/prometheus&quot;src = &quot;prometheus.conf.tmpl&quot;dest = &quot;/etc/prometheus/prometheus.yml&quot;keys = [ &quot;/job&quot;,]#check_cmd = &quot;/usr/sbin/nginx -t -c &#123;&#123;.src&#125;&#125;&quot;reload_cmd = &quot; curl -XPOST &apos;http://192.168.172.7:9090/-/reload&apos;&quot; 配置/etc/confd/template/prometheus.conf.tmplglobal: scrape_interval: 300s # By default, scrape targets every 15 seconds. evaluation_interval: 300s # By default, scrape targets every 15 seconds.# scrape_timeout is set to the global default (10s). # # Load and evaluate rules in this file every &apos;evaluation_interval&apos; seconds.rule_files: - &quot;/etc/prometheus/alert.rules&quot;scrape_configs: - job_name: &apos;prometheus&apos; scrape_interval: 5s static_configs: - targets: [&apos;localhost:9090&apos;] - job_name: &apos;alertmanager&apos; scrape_interval: 5s static_configs: - targets: [&apos;192.168.172.7:9093&apos;] labels: instance: container&#123;&#123;range $label := ls &quot;/job&quot; &#125;&#125; - job_name: &apos;&#123;&#123;base $label&#125;&#125;&apos; scrape_interval: 5s static_configs:&#123;&#123;$job_name := printf &quot;/job/%s/*&quot; $label&#125;&#125;&#123;&#123;range gets $job_name&#125;&#125; - targets: [&apos;&#123;&#123;.Value&#125;&#125;&apos;] labels: instance: &apos;&#123;&#123;base .Key&#125;&#125;&apos;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125; 启动confd./confd –config-file /etc/confd/confd.toml &amp; 具体效果用etcdkeeper模拟服务注册和服务推出在prometheus中查看效果","categories":[],"tags":[{"name":"技术平台实现","slug":"技术平台实现","permalink":"https://youxia999.github.io/tags/技术平台实现/"}]},{"title":"2018年度的技术总结","slug":"jishuzongjie-2018","date":"2019-01-01T02:35:47.000Z","updated":"2019-12-13T15:42:39.227Z","comments":true,"path":"2019/01/01/jishuzongjie-2018/","link":"","permalink":"https://youxia999.github.io/2019/01/01/jishuzongjie-2018/","excerpt":"","text":"背景新年了，按惯例，要盘盘肚子里面的“存货”了. 2018年度已掌握的技术学习已初步掌握的技术的进展，即具体技术流水账： java代码写的有点少，spring boot2.0、spring cloud2.X没有过多的深入和落地。 go语言没有很深入的学，就学了gin、go-kit的皮毛、了解协程的调度。 中间件技术mysql还是有点云里雾里，还是没有彻底搞清楚线程模型，线程调度 rabbitmq还是有点云里雾里，还是没有彻底掌握 redis新学了一个codis的方案，这个方案有点重，有些命令用不了，比如keys *（没有从group做聚合） zookeeper还是保留在会用的阶段。 elasticsearch还是保留在会用的阶段。 elastic-job-liteelastic-job-lite整个流程，看了一遍源码。 keepalived高可用技术这一年没有什么进展。 系统工具-centos技术这一年没有什么进展。 中间件技术注册中心和配置中心还是eureka，配置中心还是用的携程的apollo，至于阿里云的nacos，继续跟进吧。 docker和kubernetes这一年，开始捣鼓portainer和kubernetes。下半年的时候，花了一些时间捣鼓了一下kubernetes,并结合之前的技术方案，捣鼓了一套基于apollo+kubernetes方案（后面发现社区有公司也是采用类似的方案）:见本文附录。总的感觉，kubernetes是个好东西。如果要推动，需要自上而下的推动。现在这个产品驱动开发的公司，技术老大的重心由偏向于产品层面，所以，只能在线下玩一玩。如果要推动，需要自上而下的推动。现在这个产品驱动开发的公司，技术老大的重心由偏向于产品层面，所以，只能在线下玩一玩。运维做的有点多了。 监控跟踪技术容器监控技术还是比较熟悉prometheus。也了解了cadvisord技术和influxdb技术。 zipkin技术链路监控还是比较熟悉zipkin。 应用监控和中间件监控还是比较熟悉prometheus。 技术架构方面的收获架构方面的书籍本人工作伊始，到现在一直都有接触各种“架构”模式：SOA、分层、服务化、微服务、云原生。相关的词汇陈出不穷。所以，今年花了比较多的时间在理解云原生与微服务的区别（后面发现其实没啥本质区别，就是升级版）。并读了一本质量还行的书《云原生架构下的微服务最佳实践》。 综合评价先说综合评价，这本书还是有点货的，里面的很多技术是可以引入到项目的技术体系。如果让我推荐，我会推荐读。然后，根据组织结构，去各个技术的官方站点，看官方文档，仔细的撸一遍代码和命令。 这本书的组织架构从这本书的组织结构，可以看出，这是位有“货”的架构师。从传统架构谈到了: 架构模式（云原生、微服务） 公有或者私有云(一笔带过) 云上技术基础设施（数据库、缓存、消息中间件、任务调度中间件、自动化、持续集成、监控、框架、分布式ID） 分布式系统面临的一致性 分布式系统面临的可用性 分布式系统面临的扩展性 分布式系统面临的性能 未来值得关注的方向 研发流程、组织结构、研发文化 这本书的槽点下面本人将按组织结构的顺序进行点评。 架构模式本书花了6页的篇幅，探讨云原生的概念。个人认为还是比较浅，还可以深入点。后面又花了40多页的篇幅探讨为什么要引入微服务架构、微服务设计原则、实施的先决条件（很重要的一个点）、服务划分模式、接口设计、接口可视化（对于团队开发很重要）。基本算是合格。 公有/私有云有点遗憾，这块基本上是一笔带过。其实还是可以提一下GCP、AWS、IBM、Azure（毕竟公有云现在如日中天）、基于openstack或者kvm组成的私有云，以及基于这些云上的容器云技术kubernetes。 云上技术基础设施云上技术基础设施：数据库、缓存、消息中间件、任务调度中间件、自动化、持续集成、监控、框架、分布式ID。这些技术分成了第二章的后半章、第三章整章来说，个人认为，从篇幅上是很不够的。应该是每个技术安排一章。不过也理解，纸质书籍有其局限性，还要出版社方面的考虑。因为纸质书籍的局限性，给了博客一些机会，本人的博客，也细化了这些技术。 分布式系统面临的可用性谈到了几点：可用性的几板斧-重试、流控、熔断、容量预估、隔离。至于服务端入口处的高可用，没有谈论到。 分布式系统面临的可扩展性谈到了一点：AKF。没有记错的话，架构即未来也谈到了这个方法论。 分布式系统面临的性能性能这块，其实就几板斧：同步变异步、缓存、数据库分库分表、消息队列。 分布式系统面临的一致性本书从传统的定义引入分布式系统的一致性，然后做了分类（数据为中心的一致性、用户为中心的一致性）。然后讲了两阶段提交的局限性、三阶段提交的局限性。以及业界常用的几种解决方案：重试机制、本地记录日志、可靠事件机制、TCC事务模型。另外还提到了分布式锁。其实分布式锁、分布式id、可以单独起一章讨论。 后续值得关注的技术、组织文化、研发流程至于这块，当小说看了。尤其是组织、研发流程变化，最好是自上而下的推动，而不是自下而上。 形成自己的通用技术架构图形成自己的业务平台通用技术架构图:","categories":[],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://youxia999.github.io/tags/杂项/"}]},{"title":"玩转kubernetes之kubernetes阶段性成果","slug":"container-kubernetes-jieduanxing-zongjie","date":"2018-12-30T02:38:44.000Z","updated":"2020-02-06T04:15:12.872Z","comments":true,"path":"2018/12/30/container-kubernetes-jieduanxing-zongjie/","link":"","permalink":"https://youxia999.github.io/2018/12/30/container-kubernetes-jieduanxing-zongjie/","excerpt":"","text":"这两天到搭建好的kubernetes集群上，把公司的一个项目放到了集群上。最终的架构如下。 未在线上环境校验的方案","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"玩转kubernetes之kubernetes1.8安装","slug":"container-kubernetes-1-8-install","date":"2018-12-28T06:04:49.000Z","updated":"2020-02-06T04:15:12.862Z","comments":true,"path":"2018/12/28/container-kubernetes-1-8-install/","link":"","permalink":"https://youxia999.github.io/2018/12/28/container-kubernetes-1-8-install/","excerpt":"","text":"背景资料本文主要参考资料(建议收藏)https://jimmysong.io/kubernetes-handbook/https://anjia0532.github.io/2017/11/15/gcr-io-image-mirror/ 本文的收获实践“纸上读来终觉浅，绝知此事要躬行”，上手操作第5章 安装之前三件事熟悉kubernetes基本架构和工作原理建议好好看《Kubernetes Handbook》1-4章或者《kubernetes in action》(中文版已经出版).了解整体架构、以及基本概念模型： cluster与node rabc： pod：简单的理解就是一组容器，基础容器是 deployment(pod升级版)： service： configmap： 本文使用版本centos:CentOS Linux release 7.3.1611 (Core) kubernetes:1.8.5flannel:0.7.1docker:1.12.6harbor:1.5.4(依赖的docker为1.12.6、docker-compose为1.12.0)etcd:3.3.5cfssl_linux-amd64cfssljson_linux-amd64cfssl-certinfo_linux-amd64 ip和服务部署规划因为测试环境资源有限，尽量的合理利用资源。10.2.1.30(centos7): harbor0.5(docker1.10.3、 docker-compose1.10)、etcd3.3.510.2.1.33(centos7): etcd3.3.510.2.1.31(centos7): etcd3.3.510.2.1.37(centos7): docker1.12.6、flanneld0.7.1、kubernetes master(kube-apiserver 1.8.5、kube-scheduler1.8.5、kube-controller-manager1.8.5)、kubernetes node(kubelet1.8.5、kube-proxy1.8.5)10.2.1.38(centos7): docker1.12.6、flanneld0.7.1、kubernetes node(kubelet1.8.5、kube-proxy1.8.5)10.2.1.39(centos7): docker1.12.6、flanneld0.7.1、kubernetes node(kubelet1.8.5、kube-proxy1.8.5) 安装步骤安装etcd集群可以安装一个etcd集群，也可以用一个已有集群。本文安装的etcd没有采用Kubernetes Handbook的证书，觉得麻烦并创建key-value。 启动harbor服务安装和启动harbor服务。并将一些镜像从anjia0532上pull下来，push到私有仓库。供后续步骤使用k8s-dns-kube-dns-amd64:1.14.1k8s-dns-dnsmasq-nanny-amd64:1.14.1k8s-dns-sidecar-amd64:1.14.1heapster-amd64:v1.3.0heapster-grafana-amd64:v4.0.2heapster-influxdb-amd64:v1.1.1fluentd-elasticsearch:1.22kibana:4.6.1kubernetes-dashboard-amd64:v1.6.3 安装docker1.12.6可以参考之前的文章，不再赘述，记得要配置私有仓库，否则，很多功能不好实现。 安装flanneld0.7.1参考 https://jimmysong.io/kubernetes-handbook/practice/flannel-installation.html 安装flanneld。 注意：/etc/sysconfig/flanneld配置文件# Flanneld configuration options # etcd url location. Point this to the server where etcd runsFLANNEL_ETCD_ENDPOINTS=&quot;http://10.2.1.30:2379,http://10.2.1.31:2379,http://10.2.1.33:2379&quot;# etcd config key. This is the configuration key that flannel queries# For address range assignmentFLANNEL_ETCD_PREFIX=&quot;/kube-centos/network&quot;# Any additional options that you want to passFLANNEL_OPTIONS=&quot;--iface=ens192&quot; 并在etcd创建kubernetes集群的flanneld网络config 准备证书安装CSL文件创建CA证书参考 https://jimmysong.io/kubernetes-handbook/practice/create-tls-and-secret-key.html 创建证书，并分发到所有kubernetes node。 安装kubernetes master准备/etc/kubernetes/config配置文件该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用。#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=0&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot;# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=&quot;--master=http://10.2.1.37:8080&quot; 准备/etc/kubernetes/apiserver文件##### kubernetes system config#### The following values are used to configure the kube-apiserver## The address on the local server to listen to.#KUBE_API_ADDRESS=&quot;--address=bc.authorization.k8s.io/v10.2.1.30&quot;KUBE_API_ADDRESS=&quot;--advertise-address=10.2.1.37 --bind-address=10.2.1.37 --insecure-bind-address=10.2.1.37&quot;## The port on the local server to listen on.KUBE_API_PORT=&quot;--port=8080&quot;## Port minions listen onKUBELET_PORT=&quot;--kubelet-port=10250&quot;## Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=&quot;--etcd-servers=http://10.2.1.30:2379,http://10.2.1.31:2379,http://10.2.1.33:2379&quot;## Address range to use for services#KUBE_SERVICE_ADDREKUBELET_POD_INFRA_CONTAINERSSES=&quot;--service-cluster-ip-range=172.17.0.0/16&quot;#KUBE_ANONYMOUS_AUTH=&quot;--anonymous-auth=false&quot;KUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=172.19.0.0/16&quot;## default admission control policies#KUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;KUBE_ADMISSION_CONTROL=&quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&quot;KUBE_API_ARGS=&quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&quot; 配置kube-apiserver的systemd[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target#After=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/bin/kube-apiserver \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_ETCD_SERVERS \\ $KUBE_API_ADDRESS \\ $KUBE_API_PORT \\ $KUBELET_PORT \\ $KUBE_ALLOW_PRIV \\ $KUBE_SERVICE_ADDRESSES \\ $KUBE_ADMISSION_CONTROL \\ $KUBE_ANONYMOUS_AUTH \\ $KUBE_API_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target 启动kube-apiserversystemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserver 启动kube-controller-manager启动kube-scheduler安装kubernetes node启动kube-proxy启动kubelet安装K8S插件最终的效果dashboard效果 etcd数据库中的数据","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"docker1.12管理技术之portainer","slug":"docker-1-12-management","date":"2018-12-27T04:51:23.000Z","updated":"2020-02-06T04:15:12.915Z","comments":true,"path":"2018/12/27/docker-1-12-management/","link":"","permalink":"https://youxia999.github.io/2018/12/27/docker-1-12-management/","excerpt":"","text":"本文背景参考资料http://www.manongjc.com/article/11900.html 背景现在docker部署的越来越多，但是还没有上百台，所以kubernete技术用不到(投入和产出不匹配，有时间写写kubernetes）。于是乎，portainer进入我们的视野。 文章更新历史2019-03-06 portainer更新至1.20.2版本 实操过程docker Daemon配置&#123; &quot;insecure-registries&quot;: [ &quot;10.2.1.30:5000&quot; ], &quot;registry-mirror&quot;: [ &quot;https://csokz3oi.mirror.aliyuncs.com&quot; ], &quot;graph&quot;:&quot;/data/docker&quot;, &quot;hosts&quot;: [&quot;unix:///var/run/docker.sock&quot;,&quot;tcp://192.168.172.1:2375&quot;]&#125; portainer配置因为所有服务器的docker daemon进程都开启了2375端口，所以只用在页面配置，即可进行相应管理。截图如下：","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【转载】jenkins restful api","slug":"jenkins-restful-api","date":"2018-12-23T11:00:00.000Z","updated":"2020-02-06T04:15:13.078Z","comments":true,"path":"2018/12/23/jenkins-restful-api/","link":"","permalink":"https://youxia999.github.io/2018/12/23/jenkins-restful-api/","excerpt":"","text":"原文地址https://www.cnblogs.com/zjsupermanblog/p/7238422.html 备注有api就可以进行二次开发和集成了。 截图","categories":[],"tags":[{"name":"持续集成技术","slug":"持续集成技术","permalink":"https://youxia999.github.io/tags/持续集成技术/"},{"name":"restful api","slug":"restful-api","permalink":"https://youxia999.github.io/tags/restful-api/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"spring框架系列之清谈spring-cloud 2.1版本的候选版本","slug":"spring-cloud-2x-talk-about-greenwich","date":"2018-12-23T05:35:44.000Z","updated":"2020-02-06T04:14:40.670Z","comments":true,"path":"2018/12/23/spring-cloud-2x-talk-about-greenwich/","link":"","permalink":"https://youxia999.github.io/2018/12/23/spring-cloud-2x-talk-about-greenwich/","excerpt":"","text":"概述背景最近出来的spring cloud Greenwich版本(2.1版本)候选版本出了一个Spring Cloud Kubernetes 1.0.0。这也说明:spring cloud也全面拥抱kubernetes(私有云选择kubernetes作为容器云平台是个不错的选择)。 关于spring cloud 2.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring dat redis client、spring web service、prometheus client，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client。 看下官方的release note 猜想不禁在想，还有哪个云厂商会加入到spring cloud大家庭，这已经涉及技术后面的商业布局了。呵呵。 附录","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"centos7下yum安装jenkins","slug":"yum-install-jenkins","date":"2018-12-22T02:41:23.000Z","updated":"2019-11-14T02:17:01.182Z","comments":true,"path":"2018/12/22/yum-install-jenkins/","link":"","permalink":"https://youxia999.github.io/2018/12/22/yum-install-jenkins/","excerpt":"","text":"安装规划目录jdk版本:oracle jdk8jenkins目录: /home/jenkins 前置条件已经安装好jdk版本。 安装jenkinswget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.reporpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.keyyum install jenkins 编辑配置文件vim /etc/sysconfig/jenkins## Path: Development/Jenkins## Description: Jenkins Automation Server## Type: string## Default: &quot;/var/lib/jenkins&quot;## ServiceRestart: jenkins## Directory where Jenkins store its configuration and working# files (checkouts, build reports, artifacts, ...).#JENKINS_HOME=&quot;/home/jenkins&quot;## Type: string## Default: &quot;&quot;## ServiceRestart: jenkins## Java executable to run Jenkins# When left empty, we&apos;ll try to find the suitable Java.#JENKINS_JAVA_CMD=&quot;&quot;## Type: string## Default: &quot;jenkins&quot;## ServiceRestart: jenkins## Unix user account that runs the Jenkins daemon# Be careful when you change this, as you need to update# permissions of $JENKINS_HOME and /var/log/jenkins.#JENKINS_USER=&quot;jenkins&quot;## Type: string## Default: &quot;false&quot;## ServiceRestart: jenkins## Whether to skip potentially long-running chown at the# $JENKINS_HOME location. Do not enable this, &quot;true&quot;, unless# you know what you&apos;re doing. See JENKINS-23273.##JENKINS_INSTALL_SKIP_CHOWN=&quot;false&quot;## Type: string## Default: &quot;-Djava.awt.headless=true&quot;## ServiceRestart: jenkins## Options to pass to java when running Jenkins.#JENKINS_JAVA_OPTIONS=&quot;-Djava.awt.headless=true&quot;## Type: integer(0:65535)## Default: 8080## ServiceRestart: jenkins## Port Jenkins is listening on.# Set to -1 to disable#JENKINS_PORT=&quot;48080&quot; 编辑启动文件vim /etc/rc.d/init.d/jenkins···# Search usable Java as /usr/bin/java might not point to minimal version required by Jenkins.# see http://www.nabble.com/guinea-pigs-wanted-----Hudson-RPM-for-RedHat-Linux-td25673707.html# 修改java路径candidates=&quot;/data/jdk1.8.0_161/bin/java&quot;···# 根据磁盘的情况，修改安装路径JAVA_CMD=&quot;$JENKINS_JAVA_CMD $JENKINS_JAVA_OPTIONS -DJENKINS_HOME=/home/jenkins -jar $JENKINS_WAR&quot;PARAMS=&quot;--logfile=/home/jenkins/log/jenkins.log --webroot=/home/jenkins/war --daemon&quot;··· 重新加载systemctl daemon-reload 创建目录和授权cd home/mkdir jenkinschown -R jenkins:jenkins jenkins/llcd jenkins/mv /data/jenkins/jenkins.war .mkdir logmkdir warchown -R jenkins:jenkins jenkins/cd jenkins/ 启动jenkins不同于传统的systemctl，这里用service start jenkins","categories":[],"tags":[{"name":"持续集成技术","slug":"持续集成技术","permalink":"https://youxia999.github.io/tags/持续集成技术/"}]},{"title":"docker1.12版本下swarm操作","slug":"docker-1-12-swarm-ops","date":"2018-12-15T04:51:23.000Z","updated":"2020-02-06T04:15:12.923Z","comments":true,"path":"2018/12/15/docker-1-12-swarm-ops/","link":"","permalink":"https://youxia999.github.io/2018/12/15/docker-1-12-swarm-ops/","excerpt":"","text":"本文参考资料https://www.cnblogs.com/bigberg/p/8779302.html docker swarm操作docker swarm集群初始化如果docker1.12的swarm机制是inactive。想启动docker swarm功能，则需要初始化swarmdocker swarm init --advertise-addr 10.2.1.37 其他的节点，加入集群，则需要输入：docker swarm join --token SWMTKN-1-1o6pbdwzd4fj2ewym1npno6qub1imgg85gp5x311cc5d8gsi74-3lq395n1h8hrdwrgta17iil9c 10.2.1.37:2377","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【微服务架构java解决方案】spring security","slug":"spring-boot-2x-boot-security","date":"2018-12-09T11:55:51.000Z","updated":"2019-12-14T12:44:20.168Z","comments":true,"path":"2018/12/09/spring-boot-2x-boot-security/","link":"","permalink":"https://youxia999.github.io/2018/12/09/spring-boot-2x-boot-security/","excerpt":"","text":"参考资料https://blog.csdn.net/yuanlaijike/article/details/80249235https://blog.csdn.net/yuanlaijike/article/details/80249869https://blog.csdn.net/yuanlaijike/article/details/80250389https://blog.csdn.net/yuanlaijike/article/details/80253922https://blog.csdn.net/yuanlaijike/article/details/80327880https://blog.csdn.net/yuanlaijike/article/details/84638745https://blog.csdn.net/yuanlaijike/article/details/84703690https://segmentfault.com/a/1190000012465134https://www.cnkirito.moe/spring-security-7/ 关于spring boot/cloud 2.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring dat redis client、spring web service、prometheus client(在spring boot 2.X集成)，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client。","categories":[],"tags":[{"name":"云原生架构","slug":"云原生架构","permalink":"https://youxia999.github.io/tags/云原生架构/"},{"name":"微服务架构java解决方案","slug":"微服务架构java解决方案","permalink":"https://youxia999.github.io/tags/微服务架构java解决方案/"}]},{"title":"etcd3安装和初步操作","slug":"etcd-install","date":"2018-12-05T13:58:54.000Z","updated":"2020-02-06T04:15:13.032Z","comments":true,"path":"2018/12/05/etcd-install/","link":"","permalink":"https://youxia999.github.io/2018/12/05/etcd-install/","excerpt":"","text":"下载安装下载和安装下载tar.gz文件，解压并cp到/usr/bin/目录 创建用户和目录useradd etcdmkdir -p /data/etcd/etcd02mkdir -p /etc/etcd/chown etcd:etcd /data/etcd/etcd02chown etcd:etcd /etc/etcd/ 配置systemd新建文件/usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/data/etcd/etcd02EnvironmentFile=-/etc/etcd/etcd.confUser=etcd# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\\&quot;$&#123;ETCD_NAME&#125;\\&quot; --data-dir=\\&quot;$&#123;ETCD_DATA_DIR&#125;\\&quot; --listen-client-urls=\\&quot;$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\\&quot;&quot;Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target etcd.confetcd配置文件如下# 节点名称ETCD_NAME=etcd02# 数据存放位置ETCD_DATA_DIR=&quot;/data/etcd/etcd02&quot;# 监听其他 Etcd 实例的地址ETCD_LISTEN_PEER_URLS=&quot;http://10.2.1.31:2380&quot;# 监听客户端地址ETCD_LISTEN_CLIENT_URLS=&quot;http://10.2.1.31:2379,http://10.2.1.31:4001&quot;# 通知其他 Etcd 实例地址ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://10.2.1.31:2380&quot;# 初始化集群内节点地址ETCD_INITIAL_CLUSTER=&quot;etcd01=http://10.2.1.30:2380,etcd02=http://10.2.1.31:2380,etcd03=http://10.2.1.33:2380&quot;# 初始化集群状态，new 表示新建ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;# 初始化集群 tokenETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;# 通知 客户端地址ETCD_ADVERTISE_CLIENT_URLS=&quot;http://10.2.1.31:2379,http://10.2.1.31:4001&quot; 启动etcdsystemctl daemon-reloadsystemctl start etcd.service etcd操作etcdctl api2操作指南 etcdctl api2健康检查","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"spring boot的一种监控技术","slug":"spring-boot-2x-boot-admin-server","date":"2018-12-02T09:15:05.000Z","updated":"2020-02-06T04:15:13.194Z","comments":true,"path":"2018/12/02/spring-boot-2x-boot-admin-server/","link":"","permalink":"https://youxia999.github.io/2018/12/02/spring-boot-2x-boot-admin-server/","excerpt":"","text":"概述本文参考资料http://codecentric.github.io/spring-boot-admin/2.1.0/ (官方文档)https://my.oschina.net/u/3877886/blog/1825253 温馨提示如果能看到英文版的官方文档，就不建议继续往下阅读浪费时间，谢谢。 关于spring boot admin 2.0spring boot admin 为Spring Boot应用提供了可视化的监控视图。它基于spring boot actuator模块，因为spring boot actuator 模块为监控Spring Boot 应用程序暴露的大量的管理端点(Endpoints)。可结合eureka、consul、zookeeper等服务注册及发现组件来监控并展示微服务集群中服务的状态及各项指标信息。下面进入正题。","categories":[],"tags":[{"name":"应用监控技术","slug":"应用监控技术","permalink":"https://youxia999.github.io/tags/应用监控技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"spring框架系列之spring boot micrometer监控","slug":"spring-boot-2x-boot-micrometer","date":"2018-11-18T09:15:59.000Z","updated":"2019-12-14T12:44:44.665Z","comments":true,"path":"2018/11/18/spring-boot-2x-boot-micrometer/","link":"","permalink":"https://youxia999.github.io/2018/11/18/spring-boot-2x-boot-micrometer/","excerpt":"","text":"关于spring boot 2.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service、prometheus client，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client。","categories":[],"tags":[{"name":"云原生架构","slug":"云原生架构","permalink":"https://youxia999.github.io/tags/云原生架构/"},{"name":"微服务架构java解决方案","slug":"微服务架构java解决方案","permalink":"https://youxia999.github.io/tags/微服务架构java解决方案/"}]},{"title":"镜像仓库harbor1.5.4版本安装和使用","slug":"docker-registry-harbor-154-install","date":"2018-10-31T14:49:09.000Z","updated":"2020-02-06T04:15:13.006Z","comments":true,"path":"2018/10/31/docker-registry-harbor-154-install/","link":"","permalink":"https://youxia999.github.io/2018/10/31/docker-registry-harbor-154-install/","excerpt":"","text":"本文背景本文参考资料https://www.cnblogs.com/huangjc/p/6266564.html 下载、配置和安装下载harbor1.5.4版本离线版https://storage.googleapis.com/harbor-releases/harbor-offline-installer-v1.5.4.tgz 下载docker-copose 1.12.0版本下载docker-copose 1.12.0版本的二进制文件curl -L https://github.com/docker/compose/releases/download/1.12.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 修改harbor配置harbor.cfg## Configuration file of Harbor#This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY!_version = 1.5.0#The IP address or hostname to access admin UI and registry service.#DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.hostname = 192.168.172.10#The protocol for accessing the UI and token/notification service, by default it is http.#It can be set to https if ssl is enabled on nginx.ui_url_protocol = http#Maximum number of job workers in job servicemax_job_workers = 50#Determine whether or not to generate certificate for the registry&apos;s token.#If the value is on, the prepare script creates new root cert and private key#for generating token to access the registry. If the value is off the default key/cert will be used.#This flag also controls the creation of the notary signer&apos;s cert.customize_crt = on#The path of cert and key files for nginx, they are applied only the protocol is set to https#ssl_cert = /data/cert/server.crt#ssl_cert_key = /data/cert/server.key#The path of secretkey storagesecretkey_path = /data/harbor-adminserver#Admiral&apos;s url, comment this attribute, or set its value to NA when Harbor is standaloneadmiral_url = NA#Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.log_rotate_count = 50#Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.#If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G#are all valid.log_rotate_size = 200M#Config http proxy for Clair, e.g. http://my.proxy.com:3128#Clair doesn&apos;t need to connect to harbor ui container via http proxy.http_proxy =https_proxy =no_proxy = 127.0.0.1,localhost,ui#NOTES: The properties between BEGIN INITIAL PROPERTIES and END INITIAL PROPERTIES#only take effect in the first boot, the subsequent changes of these properties#should be performed on web ui#************************BEGIN INITIAL PROPERTIES************************#Email account settings for sending out password resetting emails.#Email server uses the given username and password to authenticate on TLS connections to host and act as identity.#Identity left blank to act as username.email_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = falseemail_insecure = false##The initial password of Harbor admin, only works for the first time when Harbor starts.#It has no effect after the first launch of Harbor.#Change the admin password from UI after launching Harbor.harbor_admin_password = Harbor12345##By default the auth mode is db_auth, i.e. the credentials are stored in a local database.#Set it to ldap_auth if you want to verify a user&apos;s credentials against an LDAP server.auth_mode = db_auth#The url for an ldap endpoint.ldap_url = ldaps://ldap.mydomain.com#A user&apos;s DN who has the permission to search the LDAP/AD server.#If your LDAP/AD server does not support anonymous search, you should configure this DN and ldap_search_pwd.#ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com#the password of the ldap_searchdn#ldap_search_pwd = password#The base DN from which to look up a user in LDAP/ADldap_basedn = ou=people,dc=mydomain,dc=com#Search filter for LDAP/AD, make sure the syntax of the filter is correct.#ldap_filter = (objectClass=person)# The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP/ADldap_uid = uid#the scope to search for users, 0-LDAP_SCOPE_BASE, 1-LDAP_SCOPE_ONELEVEL, 2-LDAP_SCOPE_SUBTREEldap_scope = 2#Timeout (in seconds) when connecting to an LDAP Server. The default value (and most reasonable) is 5 seconds.ldap_timeout = 5#Verify certificate from LDAP serverldap_verify_cert = true#The base dn from which to lookup a group in LDAP/ADldap_group_basedn = ou=group,dc=mydomain,dc=com#filter to search LDAP/AD groupldap_group_filter = objectclass=group#The attribute used to name a LDAP/AD group, it could be cn, nameldap_group_gid = cn#The scope to search for ldap groups. 0-LDAP_SCOPE_BASE, 1-LDAP_SCOPE_ONELEVEL, 2-LDAP_SCOPE_SUBTREEldap_group_scope = 2#Turn on or off the self-registration featureself_registration = on#The expiration time (in minute) of token created by token service, default is 30 minutestoken_expiration = 30#The flag to control what users have permission to create projects#The default value &quot;everyone&quot; allows everyone to creates a project.#Set to &quot;adminonly&quot; so that only admin user can create project.project_creation_restriction = everyone#************************END INITIAL PROPERTIES************************#######Harbor DB configuration section########The address of the Harbor database. Only need to change when using external db.db_host = 192.168.172.10#The password for the root user of Harbor DB. Change this before any production use.db_password = root123456#The port of Harbor database hostdb_port = 3306#The user name of Harbor databasedb_user = root##### End of Harbor DB configuration########The redis server address. Only needed in HA installation.#address:port[,weight,password,db_index]redis_url = redis:6379##########Clair DB configuration#############Clair DB host address. Only change it when using an exteral DB.clair_db_host = postgres#The password of the Clair&apos;s postgres database. Only effective when Harbor is deployed with Clair.#Please update it before deployment. Subsequent update will cause Clair&apos;s API server and Harbor unable to access Clair&apos;s database.clair_db_password = password#Clair DB connect portclair_db_port = 5432#Clair DB usernameclair_db_username = postgres#Clair default databaseclair_db = postgres##########End of Clair DB configuration#############The following attributes only need to be set when auth mode is uaa_authuaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pem### Docker Registry setting ####registry_storage_provider can be: filesystem, s3, gcs, azure, etc.registry_storage_provider_name = filesystem#registry_storage_provider_config is a comma separated &quot;key: value&quot; pairs, e.g. &quot;key1: value, key2: value2&quot;.#Refer to https://docs.docker.com/registry/configuration/#storage for all available configuration.registry_storage_provider_config =#If reload_config=true, all settings which present in harbor.cfg take effect after prepare and restart harbor, it overwrites exsiting settings.#reload_config=true#Regular expression to match skipped environment variables#skip_reload_env_pattern=(^EMAIL.*)|(^LDAP.*) 修改docker-compose.yml文件暴露registry端口，否则有时会报连接被拒绝version: &apos;2&apos;services: log: image: vmware/harbor-log:v1.5.4 container_name: harbor-log restart: always volumes: - /data/harbor-log/logs/:/var/log/docker/:z - ./common/config/log/:/etc/logrotate.d/:z ports: - 127.0.0.1:1514:10514 networks: - harbor dns_search: . registry: image: vmware/registry-photon:v2.6.2-v1.5.4 container_name: harbor-registry restart: always volumes: - /data/harbor-registry:/storage:z - ./common/config/registry/:/etc/registry/:z networks: - harbor ports: - 5000:5000 dns_search: . environment: - GODEBUG=netdns=cgo command: [&quot;serve&quot;, &quot;/etc/registry/config.yml&quot;] depends_on: - log logging: driver: &quot;json-file&quot; #options: #syslog-address: &quot;tcp://127.0.0.1:1514&quot; #tag: &quot;registry&quot; mysql: image: vmware/harbor-db:v1.5.4 container_name: harbor-db restart: always volumes: - /data/harbor-mysql:/var/lib/mysql:z networks: - harbor ports: - 3306:3306 dns_search: . env_file: - ./common/config/db/env depends_on: - log logging: driver: &quot;json-file&quot; #options: #syslog-address: &quot;tcp://127.0.0.1:1514&quot; #tag: &quot;mysql&quot; adminserver: image: vmware/harbor-adminserver:v1.5.4 container_name: harbor-adminserver env_file: - ./common/config/adminserver/env restart: always volumes: - /data/harbor-adminserver/config/:/etc/adminserver/config/:z - /data/harbor-adminserver/secretkey:/etc/adminserver/key:z - /data/harbor-adminserver/data/:/data/:z networks: - harbor dns_search: . depends_on: - log logging: driver: &quot;json-file&quot; #options: #syslog-address: &quot;tcp://127.0.0.1:1514&quot; #tag: &quot;adminserver&quot; ui: image: vmware/harbor-ui:v1.5.4 container_name: harbor-ui env_file: - ./common/config/ui/env restart: always volumes: - ./common/config/ui/app.conf:/etc/ui/app.conf:z - ./common/config/ui/private_key.pem:/etc/ui/private_key.pem:z - ./common/config/ui/certificates/:/etc/ui/certificates/:z - /data/harbor-ui/secretkey:/etc/ui/key:z - /data/harbor-ui/ca_download/:/etc/ui/ca/:z - /data/harbor-ui/psc/:/etc/ui/token/:z networks: - harbor dns_search: . depends_on: - log - adminserver - registry logging: driver: &quot;json-file&quot; #options: #syslog-address: &quot;tcp://127.0.0.1:1514&quot; #tag: &quot;ui&quot; jobservice: image: vmware/harbor-jobservice:v1.5.4 container_name: harbor-jobservice env_file: - ./common/config/jobservice/env restart: always volumes: - /data/harbor-job/logs:/var/log/jobs:z - ./common/config/jobservice/config.yml:/etc/jobservice/config.yml:z networks: - harbor dns_search: . depends_on: - redis - ui - adminserver logging: driver: &quot;json-file&quot; #options: #syslog-address: &quot;tcp://127.0.0.1:1514&quot; #tag: &quot;jobservice&quot; redis: image: vmware/redis-photon:v1.5.4 container_name: harbor-redis restart: always volumes: - /data/harbor-redis:/data networks: - harbor dns_search: . depends_on: - log logging: driver: &quot;json-file&quot; #options: #syslog-address: &quot;tcp://127.0.0.1:1514&quot; #tag: &quot;redis&quot; proxy: image: vmware/nginx-photon:v1.5.4 container_name: harbor-nginx restart: always volumes: - ./common/config/nginx:/etc/nginx:z networks: - harbor dns_search: . ports: - 80:80 - 443:443 - 4443:4443 depends_on: - mysql - registry - ui - log logging: driver: &quot;json-file&quot; #options: #syslog-address: &quot;tcp://127.0.0.1:1514&quot; #tag: &quot;proxy&quot;networks: harbor: external: false 执行./prepare命令执行./prepare后，然后执行cp /data/harbor-adminserver/secretkey /data/harbor-ui/secretkey 执行./install.sh脚本 登录查看效果 docker登录和打标签docker login 192.168.172.9:5000docker tag vmware/harbor-migrator:v1.5.0 192.168.172.9:5000/harbor/harbor-migrator:v1.5.0docker push 192.168.172.9:5000/harbor/harbor-migrator:v1.5.0 复制效果 restful api效果","categories":[],"tags":[{"name":"持续集成技术","slug":"持续集成技术","permalink":"https://youxia999.github.io/tags/持续集成技术/"},{"name":"restful api","slug":"restful-api","permalink":"https://youxia999.github.io/tags/restful-api/"},{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"docker扩展iptables实现网络隔离与通信","slug":"docker-network-basic","date":"2018-10-09T13:26:11.000Z","updated":"2020-02-06T04:15:12.976Z","comments":true,"path":"2018/10/09/docker-network-basic/","link":"","permalink":"https://youxia999.github.io/2018/10/09/docker-network-basic/","excerpt":"","text":"本文参考https://www.cnblogs.com/foxgab/p/6896957.html iptables的概念iptables基础概念 iptables表的分类 iptable链的走向 docker对于iptables的应用docker对iptables filter表的扩展 docker对iptables nat表的扩展 docker之后的iptables链的走向","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"docker1.12安装和初步使用","slug":"docker-1-12-install","date":"2018-10-07T02:51:23.000Z","updated":"2020-02-06T04:15:12.906Z","comments":true,"path":"2018/10/07/docker-1-12-install/","link":"","permalink":"https://youxia999.github.io/2018/10/07/docker-1-12-install/","excerpt":"","text":"本文背景背景最近闲着无事，准备玩一把k8s，作为核心，要升级一波docker。这次直接安装docker1.12。查了资料，docker1.12的最显著特性就是内置了docker自身的swarm调度。 安装过程参考去年的docker 1.10安装心得,整一次1.12。 下载rpm包并安装yum install -y --nogpgcheck http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/Packages/docker-engine-selinux-1.12.6-1.el7.centos.noarch.rpmyum install -y --nogpgcheck http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/Packages/docker-engine-1.12.6-1.el7.centos.x86_64.rpm 修改docker的systemd文件[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerEnvironmentFile=-/run/flannel/dockerExecStart=/usr/bin/dockerd --storage-driver=devicemapper --exec-opt native.cgroupdriver=systemd -D -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --pidfile=/var/run/docker.pid ExecReload=/bin/kill -s HUP $MAINPIDExecReload=/bin/kill -s HUP $MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process[Install]WantedBy=multi-user.target 这里暴露2375是为了其他的docker client调用。 修改配置文件发现/etc/sysconfig/docker不顶用了。谷歌了一番，发现配置文件目录变了/data/docker/daemon.json&#123; &quot;insecure-registries&quot;: [ &quot;10.2.1.30:5000&quot; ], &quot;registry-mirror&quot;: [ &quot;https://csokz3oi.mirror.aliyuncs.com&quot; ], &quot;graph&quot;:&quot;/data/docker&quot;&#125; 启动dockersystemctl restart docker.service docker操作docker1.12与1.10的区别先看docker –help对比 果然是多了三个关键字:docker nodedocker swarmdocker service docker1.12 实操先看docker help和docker.hub.com。 docker login命令作用:登录私有镜像仓库docker login 10.2.1.30:5000 输入账号和密码，即可提示登录成功。 docker tag命令作用:对docker镜像打标签命令使用:docker tag anjia0532/google-containers.kubernetes-dashboard-amd64:v1.6.3 10.2.1.30:5000/anjia0532/kubernetes-dashboard-amd64:v1.6.3 docker pull这个命令不需要多说了，从中央仓库或者私有仓库拉取镜像 docker push命令作用：向仓库推送镜像文件命令使用:docker push 10.2.1.30:5000/anjia0532/kubernetes-dashboard-amd64:v1.6.3 至于docker swarm模式，有时间再了解。","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"基于elasticsearch的日志监控方案(改进版)","slug":"elasticsearch-log-metrics","date":"2018-10-01T06:29:46.000Z","updated":"2020-02-06T04:15:13.022Z","comments":true,"path":"2018/10/01/elasticsearch-log-metrics/","link":"","permalink":"https://youxia999.github.io/2018/10/01/elasticsearch-log-metrics/","excerpt":"","text":"","categories":[],"tags":[{"name":"日志监控技术","slug":"日志监控技术","permalink":"https://youxia999.github.io/tags/日志监控技术/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://youxia999.github.io/tags/elasticsearch/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"go语言反射学习","slug":"golang-reflect-learn","date":"2018-09-11T10:06:47.000Z","updated":"2019-12-27T11:59:45.504Z","comments":true,"path":"2018/09/11/golang-reflect-learn/","link":"","permalink":"https://youxia999.github.io/2018/09/11/golang-reflect-learn/","excerpt":"","text":"源码package mainimport ( &quot;flag&quot; &quot;fmt&quot; &quot;net/http&quot; &quot;reflect&quot;)type GoService struct &#123;&#125;func (G *GoService) HttpServer(port string)&#123; http.HandleFunc(&quot;/&quot;, func(responseWriter http.ResponseWriter, request *http.Request) &#123; responseWriter.Write([]byte(&quot;hello word&quot;)) &#125;) http.ListenAndServe(&quot;:&quot;+port,nil)&#125;func main()&#123; cmd:=flag.String(&quot;cmd&quot;,&quot;&quot;,&quot;cmd is a string indecated what server&quot;) arg:=flag.String(&quot;args&quot;,&quot;&quot;,&quot;p is a string ,parameter for service&quot;) flag.Parse() if len(*cmd)==0 &#123; flag.Usage() return &#125; in :=make([]reflect.Value,0) if len(*arg)!=0 &#123; in=append(in,reflect.ValueOf(*arg)) &#125; //指定类 service:=&amp;GoService&#123;&#125; f:=reflect.ValueOf(service).MethodByName(*cmd) if !f.IsValid() &#123; return &#125; f.Call(in) //u:=User&#123;1,&quot;golang&quot;,12&#125; //Info(u)&#125; 编译go build -o bin/runt.exe main.go 执行cd binrunt.exe -cmd=HttpServer -args=8091","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"golang","slug":"golang","permalink":"https://youxia999.github.io/tags/golang/"}]},{"title":"单节点安装redis 3.2版本集群","slug":"redis-cluster-install","date":"2018-08-25T06:09:47.000Z","updated":"2020-02-06T04:16:21.849Z","comments":true,"path":"2018/08/25/redis-cluster-install/","link":"","permalink":"https://youxia999.github.io/2018/08/25/redis-cluster-install/","excerpt":"","text":"本文参考资料https://blog.csdn.net/qq_37595946/article/details/77800147https://blog.csdn.net/ahzxj2012/article/details/73468119https://www.jianshu.com/p/854d702e6153 背景交代公司有些没权限的机器又抽风，于是想把部分服务放到有权限的机器上。比如redis cluster。 安装规划采用docker安装redis node。由redis-trib.rb构建集群。 安装过程docker安装redisnode准备配置文件port 6381bind 10.2.1.31cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes 拉取镜像并启动docker pull redis:3.2.0docker run --net=host -m 100m --name=redis1 -v `pwd`/redis1.conf:/usr/local/etc/redis/redis.conf -d redis:3.2.0 redis-server /usr/local/etc/redis/redis.confdocker run --net=host -m 100m --name=redis2 -v `pwd`/redis2.conf:/usr/local/etc/redis/redis.conf -d redis:3.2.0 redis-server /usr/local/etc/redis/redis.confdocker run --net=host -m 100m --name=redis3 -v `pwd`/redis3.conf:/usr/local/etc/redis/redis.conf -d redis:3.2.0 redis-server /usr/local/etc/redis/redis.confdocker run --net=host -m 100m --name=redis4 -v `pwd`/redis4.conf:/usr/local/etc/redis/redis.conf -d redis:3.2.0 redis-server /usr/local/etc/redis/redis.confdocker run --net=host -m 100m --name=redis5 -v `pwd`/redis5.conf:/usr/local/etc/redis/redis.conf -d redis:3.2.0 redis-server /usr/local/etc/redis/redis.confdocker run --net=host -m 100m --name=redis6 -v `pwd`/redis6.conf:/usr/local/etc/redis/redis.conf -d redis:3.2.0 redis-server /usr/local/etc/redis/redis.conf 下载redis源码并解压执行下面的命令前，请确保ruby版本已经升级到2.2.0。ps:yum install ruby时，centos只会安装ruby2.0版本。 wget http://download.redis.io/releases/redis-3.2.1.tar.gztar xvf redis-3.2.1.tar.gzmv redis-3.2.1 redisruby redis/src/redis-trib.rb create --replicas 1 10.2.1.31:6381 10.2.1.31:6382 10.2.1.31:6383 10.2.1.31:6384 10.2.1.31:6385 10.2.1.31:6386 顺利的话，能看到如下信息：&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:10.2.1.31:638110.2.1.31:638210.2.1.31:6383Adding replica 10.2.1.31:6384 to 10.2.1.31:6381Adding replica 10.2.1.31:6385 to 10.2.1.31:6382Adding replica 10.2.1.31:6386 to 10.2.1.31:6383M: cb4258f0a219065492d519ebf4417833c397718a 10.2.1.31:6381 slots:0-5460 (5461 slots) masterM: 5e1c70538ec4ddd28b0dbc79bce491ac1d25e452 10.2.1.31:6382 slots:5461-10922 (5462 slots) masterM: 31c913e81bbb994e843d8874d8063debd2e82363 10.2.1.31:6383 slots:10923-16383 (5461 slots) masterS: 34750039ae8d7010a06a7691b95ae81f48e44cb9 10.2.1.31:6384 replicates cb4258f0a219065492d519ebf4417833c397718aS: 48f5de18c92ebdc277c4eaee9d73ea6f1cc91c8e 10.2.1.31:6385 replicates 5e1c70538ec4ddd28b0dbc79bce491ac1d25e452S: 1a349016ed6f326bb4118ec94951301eb3ca7815 10.2.1.31:6386 replicates 31c913e81bbb994e843d8874d8063debd2e82363Can I set the above configuration? (type &apos;yes&apos; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 10.2.1.31:6381)M: cb4258f0a219065492d519ebf4417833c397718a 10.2.1.31:6381 slots:0-5460 (5461 slots) masterM: 5e1c70538ec4ddd28b0dbc79bce491ac1d25e452 10.2.1.31:6382 slots:5461-10922 (5462 slots) masterM: 31c913e81bbb994e843d8874d8063debd2e82363 10.2.1.31:6383 slots:10923-16383 (5461 slots) masterM: 34750039ae8d7010a06a7691b95ae81f48e44cb9 10.2.1.31:6384 slots: (0 slots) master replicates cb4258f0a219065492d519ebf4417833c397718aM: 48f5de18c92ebdc277c4eaee9d73ea6f1cc91c8e 10.2.1.31:6385 slots: (0 slots) master replicates 5e1c70538ec4ddd28b0dbc79bce491ac1d25e452M: 1a349016ed6f326bb4118ec94951301eb3ca7815 10.2.1.31:6386 slots: (0 slots) master replicates 31c913e81bbb994e843d8874d8063debd2e82363[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 但是一般没有那么顺利。 常见错误以及处理出现”kernel_require.rb:55:in `require’: cannot load suc”解决办法：gem install redis 出现”报错redis requires Ruby version -= 2.2.2”解决办法：升级ruby版本。curl -L get.rvm.io | bash -s stablesource /usr/local/rvm/scripts/rvmrvm list knownrvm install 2.4.0rvm remove 2.0.0ruby --versiongem install redis 然后执行:ruby redis/src/redis-trib.rb create –replicas 1 10.2.1.31:6381 10.2.1.31:6382 10.2.1.31:6383 10.2.1.31:6384 10.2.1.31:6385 10.2.1.31:6386就可以看到上面的提示信息了。 ruby版本失效解决办法：rvm use ruby-2.4.0 体验","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://youxia999.github.io/tags/redis/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【转载】打造高逼格、可视化的Docker容器监控系统平台","slug":"docker-monitor-tech","date":"2018-08-14T12:03:07.000Z","updated":"2020-02-06T04:15:12.961Z","comments":true,"path":"2018/08/14/docker-monitor-tech/","link":"","permalink":"https://youxia999.github.io/2018/08/14/docker-monitor-tech/","excerpt":"","text":"原文地址https://mp.weixin.qq.com/s?__biz=MzI0MDQ4MTM5NQ==&amp;mid=2247486129&amp;idx=1&amp;sn=986d170f115071cbe676d211a0458008&amp;chksm=e91b6fadde6ce6bb271dedda23acef2c031ee3bdd7d9e2034ceab9a9e2c65f2caa98932e9491&amp;scene=38#wechat_redirect 笔记版","categories":[],"tags":[{"name":"容器监控技术","slug":"容器监控技术","permalink":"https://youxia999.github.io/tags/容器监控技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"安装nodejs并初步使用","slug":"centos7-install-nodejs","date":"2018-08-12T08:37:48.000Z","updated":"2020-02-06T04:15:12.741Z","comments":true,"path":"2018/08/12/centos7-install-nodejs/","link":"","permalink":"https://youxia999.github.io/2018/08/12/centos7-install-nodejs/","excerpt":"","text":"node与npm介绍 centos下安装nodejs下载执行脚本curl -sL https://rpm.nodesource.com/setup_10.x | bash - 安装依赖yum install gcc-c++ make 安装nodejsyum install -y nodejs 初步使用node帮助命令 npm帮助命令 npm安装命令 第一个nodejs程序var http=require(&apos;http&apos;)http.createServer(function(req,res)&#123; res.writeHead(200,&#123;&apos;Content-type&apos;:&apos;test/plain&apos;&#125;); res.end(&apos;hello,wordl\\n&apos;);&#125;).listen(1337,&apos;127.0.0.1&apos;);console.log(&apos;server running at http://127.0.0.1:1337&apos;);","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"2018年下半年学习落地计划","slug":"2019_learn_plan","date":"2018-08-06T02:35:47.000Z","updated":"2019-12-14T14:30:03.612Z","comments":true,"path":"2018/08/06/2019_learn_plan/","link":"","permalink":"https://youxia999.github.io/2018/08/06/2019_learn_plan/","excerpt":"","text":"2019年度展望和学习计划在上述通用技术架构中已掌握的5大块进行加强: 服务化所需的中间件 openresty、keepalived、mysql、redis、zookeeper、rabbitmq/kafka、注册中心、配置中心等中间件技术。 研发管理平台 项目管理、持续集成、持续交付、静态代码扫描、wiki知识管理。 容器云平台技术 云原生中间件-docker、etcd、kubernetes等容器技术。 监控平台技术 zipkin调用连监控技术、prometheus监控技术、容器监控技术。 编程框架和中间件客户端java语言的应用开发脚手架主要是spring boot2.X，微服务集成框架则是spring cloud 2.X版本。go语言的应用开发脚手架主要是gin,主要是go-micro框架。 杂项 比如os系统，比如centos 6、centos 7、centos8(待发布，RHEL8已经出了beta版本)。 最后来张图。 画饼下面这张图是来自于普元EA：基于paas/caas平台的技术架构。不禁感叹：有一个私有容器云平台，真的挺好。2019年度，看能不能在业务平台的技术架构、kubernetes两点上有所收获、积累更多的落地经验，加油。","categories":[],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://youxia999.github.io/tags/杂项/"}]},{"title":"elastic-job-lite使用和源码初步分析","slug":"elastic-job-basic-use","date":"2018-07-29T14:20:03.000Z","updated":"2020-02-06T04:16:21.643Z","comments":true,"path":"2018/07/29/elastic-job-basic-use/","link":"","permalink":"https://youxia999.github.io/2018/07/29/elastic-job-basic-use/","excerpt":"","text":"elastic-job使用elasticjob概念模型elastic-job的概念模型主要分为5类： 注册中心bean(ZookeeperRegistryCenter) job配置bean 业务逻辑job(需要实现simpleJob/DataJob) job事件配置(需要写入到存储中去) job调度器bean开发者显示调用jobscheduler的init方法（或者采用spring注解，初始化init方法），进入到quartz调度。 elastic-job使用ZookeeperRegistryCenter配置LiteJobConfiguration配置elastic-job源码分析ZookeeperRegistryCenter封装cutor，调用zookeeper。 LiteJobConfiguration执行逻辑的元数据：类名、分片数量。 JobEventConfiguration数据源封装，结果最终会记录入库 JobScheduler这是核心类，作用：任务调度 SpringJobScheduler简单job的任务调度器。 LiteJob继承quartz的job接口，实现execute方法。并调用任务执行工厂类，实例化任务执行器。 AbstractJobExecutor任务执行器。执行job。比如典型的实现类simplejobExecutor。","categories":[],"tags":[{"name":"elastic job","slug":"elastic-job","permalink":"https://youxia999.github.io/tags/elastic-job/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"2016-2018年技术感悟总结(草稿)","slug":"jishuzongjie-2018_beta","date":"2018-07-01T02:35:41.000Z","updated":"2019-12-03T03:13:00.508Z","comments":true,"path":"2018/07/01/jishuzongjie-2018_beta/","link":"","permalink":"https://youxia999.github.io/2018/07/01/jishuzongjie-2018_beta/","excerpt":"","text":"背景背景新年了，要盘盘肚子里面的“存货”了。 2016-2018年度年技术总结在五大块进行加强: java语言和框架主要是spring boot2.X。 go语言和框架主要是go-micro框架。 系统工具-centos centos 6、centos 7、centos8(待发布，RHEL8已经出了beta版本)。 研发管理平台 项目管理、持续集成、持续交付、静态代码扫描、知识管理 服务化所需的中间件 openresty、keepalived、mysql、redis、zookeeper、rabbitmq/kafka等中间件技术、监控技术、prometheus监控技术。一图胜千言 2019年度的技术主题因为微服务很火、云原生也渐渐在大厂落地，所以，大家都在研究和投入，所以，可供学习和思考的点就很多。","categories":[],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://youxia999.github.io/tags/杂项/"}]},{"title":"对oauth2技术的简单理解","slug":"oauth2-technologies-understanding","date":"2018-06-21T12:55:27.000Z","updated":"2020-02-06T04:13:09.275Z","comments":true,"path":"2018/06/21/oauth2-technologies-understanding/","link":"","permalink":"https://youxia999.github.io/2018/06/21/oauth2-technologies-understanding/","excerpt":"","text":"","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【转载】oauth2介绍","slug":"oauth2-introduce","date":"2018-06-16T07:02:30.000Z","updated":"2020-02-06T04:12:59.939Z","comments":true,"path":"2018/06/16/oauth2-introduce/","link":"","permalink":"https://youxia999.github.io/2018/06/16/oauth2-introduce/","excerpt":"","text":"原文地址:https://zhuanlan.zhihu.com/p/30720675","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"zipkin与thrift结合进行接口监控","slug":"zipkin_rpc_example","date":"2018-06-09T03:02:34.000Z","updated":"2020-02-06T04:14:56.647Z","comments":true,"path":"2018/06/09/zipkin_rpc_example/","link":"","permalink":"https://youxia999.github.io/2018/06/09/zipkin_rpc_example/","excerpt":"","text":"#参考资料http://blog.mozhu.org/2017/11/11/zipkin/zipkin-2.html 进入正题添加maven依赖&lt;dependency&gt; &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt; &lt;artifactId&gt;brave&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.reporter2&lt;/groupId&gt; &lt;artifactId&gt;zipkin-reporter&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/io..reporter2/-sender-okhttp3 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.reporter2&lt;/groupId&gt; &lt;artifactId&gt;zipkin-sender-okhttp3&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt; 配置文件(用于初始化)package com.xiaogang.apigateway.config;import brave.Tracer;import brave.Tracing;import brave.propagation.B3Propagation;import brave.propagation.ExtraFieldPropagation;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Component;import zipkin2.codec.SpanBytesEncoder;import zipkin2.reporter.AsyncReporter;import zipkin2.reporter.Sender;import zipkin2.reporter.okhttp3.OkHttpSender;import java.util.concurrent.TimeUnit;@Componentpublic class TracerConfig implements InitializingBean &#123; private static Tracer tracer; public static Tracer getTracer()&#123; return tracer; &#125; @Override public void afterPropertiesSet() throws Exception &#123; Sender sender= OkHttpSender.create(&quot;http://192.168.172.6:9411/api/v2/spans&quot;); AsyncReporter asyncReporter=AsyncReporter.builder(sender).closeTimeout(500, TimeUnit.MILLISECONDS).build(SpanBytesEncoder.JSON_V2); Tracing tracing= Tracing.newBuilder().localServiceName(&quot;xiaogang_api&quot;).spanReporter(asyncReporter) .propagationFactory(ExtraFieldPropagation.newFactory(B3Propagation.FACTORY,&quot;user-name&quot;)).build(); tracer=tracing.tracer(); &#125;&#125; 嵌入到使用的rpc框架入口（多服务共有一个端口）package com.xiaogang.apigateway.thrift;import brave.Span;import com.xiaogang.apigateway.config.TracerConfig;import org.apache.thrift.TException;import org.apache.thrift.TProcessor;import org.apache.thrift.protocol.TMessage;import org.apache.thrift.protocol.TProtocol;import org.apache.thrift.protocol.TProtocolDecorator;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.Map;public class MyTMutiplexedProcessor implements TProcessor &#123; private static final Logger logger = LoggerFactory.getLogger(ThriftPoolServerStartThread.class); private final Map&lt;String, TProcessor&gt; SERVICE_PROCESSOR_MAP = new HashMap(); public void registerProcessor(String serviceName, TProcessor processor) &#123; this.SERVICE_PROCESSOR_MAP.put(serviceName, processor); &#125; @Override public boolean process(TProtocol inProtocol, TProtocol outProtocol1) throws TException &#123; TMessage message = inProtocol.readMessageBegin(); logger.info(&quot;service name &#123;&#125;&quot;,message.name); if (message.type != 1 &amp;&amp; message.type != 4) &#123; throw new TException(&quot;This should not have happened!?&quot;); &#125; else &#123; int index = message.name.indexOf(&quot;:&quot;); if (index &lt; 0) &#123; throw new TException(&quot;Service name not found in message name: &quot; + message.name + &quot;. Did you &quot; + &quot;forget to use a TMultiplexProtocol in your client?&quot;); &#125; else &#123; String serviceName = message.name.substring(0, index); TProcessor actualProcessor = (TProcessor)this.SERVICE_PROCESSOR_MAP.get(serviceName); if (actualProcessor == null) &#123; throw new TException(&quot;Service name not found: &quot; + serviceName + &quot;. Did you forget &quot; + &quot;to call registerProcessor()?&quot;); &#125; else &#123; logger.info(&quot;rpc start&quot;); TMessage standardMessage = new TMessage(message.name.substring(serviceName.length() + &quot;:&quot;.length()), message.type, message.seqid); Span span=TracerConfig.getTracer().newTrace().name(message.name.substring(serviceName.length() + &quot;:&quot;.length())).start(); boolean result=actualProcessor.process(new MyTMutiplexedProcessor.StoredMessageProtocol(inProtocol, standardMessage), outProtocol1); span.finish(); logger.info(&quot;rpc end&quot;); return result; &#125; &#125; &#125; &#125; private static class StoredMessageProtocol extends TProtocolDecorator &#123; TMessage messageBegin; public StoredMessageProtocol(TProtocol protocol, TMessage messageBegin) &#123; super(protocol); this.messageBegin = messageBegin; &#125; public TMessage readMessageBegin() throws TException &#123; return this.messageBegin; &#125; &#125;&#125; 效果 题外话以前的技术选型做的有点草率，thrift这个框架可定制化的地方不是很多。","categories":[],"tags":[{"name":"链路跟踪技术","slug":"链路跟踪技术","permalink":"https://youxia999.github.io/tags/链路跟踪技术/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【微服务架构spring cloud解决方案】spring-cloud之netfix-ribbon","slug":"spring-cloud-1x-cloud-netfix-ribbon","date":"2018-06-03T09:14:29.000Z","updated":"2020-02-06T04:14:12.802Z","comments":true,"path":"2018/06/03/spring-cloud-1x-cloud-netfix-ribbon/","link":"","permalink":"https://youxia999.github.io/2018/06/03/spring-cloud-1x-cloud-netfix-ribbon/","excerpt":"","text":"关于spring boot/cloud 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【微服务架构java解决方案】服务注册中心eureka的初步使用","slug":"spring-cloud-1x-cloud-netfix-eureka","date":"2018-05-27T09:13:39.000Z","updated":"2020-02-06T04:14:04.126Z","comments":true,"path":"2018/05/27/spring-cloud-1x-cloud-netfix-eureka/","link":"","permalink":"https://youxia999.github.io/2018/05/27/spring-cloud-1x-cloud-netfix-eureka/","excerpt":"","text":"概述本文参考资料prometheus监控: https://github.com/prometheus/client_java/releases/tag/parent-0.5.0spring官方文档： https://cloud.spring.io/spring-cloud-static/Edgware.SR5/single/spring-cloud.htmlhttps://github.com/Netflix/eureka/wiki/eureka-REST-operations 关于spring boot/cloud 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。 eureka server配置pom配置&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;skipCheck&gt;true&lt;/skipCheck&gt; &lt;!--指定版本号，防止有些环境是jdk7--&gt; &lt;jetty.version&gt;9.2.19.v20160908&lt;/jetty.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;1.4.0.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- The prometheus client --&gt; &lt;dependency&gt; &lt;groupId&gt;io.prometheus&lt;/groupId&gt; &lt;artifactId&gt;simpleclient&lt;/artifactId&gt; &lt;version&gt;0.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hotspot JVM metrics--&gt; &lt;dependency&gt; &lt;groupId&gt;io.prometheus&lt;/groupId&gt; &lt;artifactId&gt;simpleclient_hotspot&lt;/artifactId&gt; &lt;version&gt;0.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.prometheus&lt;/groupId&gt; &lt;artifactId&gt;simpleclient_servlet&lt;/artifactId&gt; &lt;version&gt;0.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Exposition HTTPServer--&gt; &lt;dependency&gt; &lt;groupId&gt;io.prometheus&lt;/groupId&gt; &lt;artifactId&gt;simpleclient_httpserver&lt;/artifactId&gt; &lt;version&gt;0.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; application.yml配置server: port: 8084eureka: instance: hostname: ip地址 client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://ip地址:8084/eureka/ 启动eureka和相应restful api操作metrics入口(可被监控) 获取所有实例 手工注册实例 手工下架实例","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"restful api","slug":"restful-api","permalink":"https://youxia999.github.io/tags/restful-api/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"zipkin的基本概念和初步体验","slug":"zipkin-install","date":"2018-05-26T03:02:34.000Z","updated":"2020-02-06T04:14:51.260Z","comments":true,"path":"2018/05/26/zipkin-install/","link":"","permalink":"https://youxia999.github.io/2018/05/26/zipkin-install/","excerpt":"","text":"参考文档https://blog.csdn.net/apei830/article/details/78722168 概念与协议协议概念英文版(spring官网) 概念中文版 Span基本工作单元，一次链路调用（可以是RPC，DB等没有特定的限制）创建一个span，通过一个64位ID标识它，span通过还有其他的数据，例如描述信息，时间戳，key-value对的（Annotation）tag信息，parent-id等，其中parent-id可以表示span调用链路来源，通俗的理解span就是一次请求信息。 Trace类似于树结构的Span集合，表示一条调用链路（一次端到端的完整请求），存在唯一标识，即TraceId。 Annotationspan的注解，用来记录请求特定事件相关信息（例如时间），通常包含四个注解信息：cs:Client Start，表示客户端发起请求sr:Server Receive，表示服务端收到请求ss:Server Send，表示服务端完成处理，并将结果发送给客户端cr:Client Received，表示客户端获取到服务端返回信息 BinaryAnnotation：提供一些额外信息，一般以key-value对出现 传输协议http传输 传输的数据（V2版本）存储到es的document的_source字段&quot;_source&quot;: &#123; &quot;traceId&quot;: &quot;388c003cbf4d978e&quot;, &quot;duration&quot;: 50735, &quot;remoteEndpoint&quot;: &#123; &quot;ipv4&quot;: &quot;192.168.128.57&quot;, &quot;port&quot;: 12497 &#125;, &quot;shared&quot;: true, &quot;localEndpoint&quot;: &#123; &quot;serviceName&quot;: &quot;serviceb&quot;, &quot;ipv4&quot;: &quot;192.168.128.57&quot; &#125;, &quot;timestamp_millis&quot;: 1527308495000, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /call/&#123;id&#125;&quot;, &quot;id&quot;: &quot;c7e7ea03cfaf7108&quot;, &quot;parentId&quot;: &quot;388c003cbf4d978e&quot;, &quot;timestamp&quot;: 1527308495000715, &quot;tags&quot;: &#123; &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/call/abcd&quot;, &quot;mvc.controller.class&quot;: &quot;UserController&quot;, &quot;mvc.controller.method&quot;: &quot;callHome&quot; &#125;&#125; 架构图 主要包括四个模块 Collector接收或收集各应用传输的数据 Storage 存储接受或收集过来的数据，当前支持Memory，MySQL，Cassandra，ElasticSearch等，默认存储在内存中。 API（Query） 负责查询Storage中存储的数据，提供简单的JSON API获取数据，主要提供给web UI使用 Web提供简单的web界面 服务端安装配置mysql存储mysql脚本CREATE TABLE IF NOT EXISTS _spans ( trace_id_high BIGINT NOT NULL DEFAULT 0 COMMENT ‘If non zero, this means the trace uses 128 bit traceIds instead of 64 bit’, trace_id BIGINT NOT NULL, id BIGINT NOT NULL, name VARCHAR(255) NOT NULL, remote_service_name VARCHAR(255), parent_id BIGINT, debug BIT(1), start_ts BIGINT COMMENT ‘Span.timestamp(): epoch micros used for endTs query and to implement TTL’, duration BIGINT COMMENT ‘Span.duration(): micros used for minDuration and maxDuration query’, PRIMARY KEY (trace_id_high, trace_id, id)) ENGINE=InnoDB ROW_FORMAT=COMPRESSED CHARACTER SET=utf8 COLLATE utf8_general_ci; ALTER TABLE _spans ADD INDEX(trace_id_high, trace_id) COMMENT ‘for getTracesByIds’;ALTER TABLE _spans ADD INDEX(name) COMMENT ‘for getTraces and getSpanNames’;ALTER TABLE _spans ADD INDEX(remote_service_name) COMMENT ‘for getTraces and getRemoteServiceNames’;ALTER TABLE _spans ADD INDEX(start_ts) COMMENT ‘for getTraces ordering and range’; CREATE TABLE IF NOT EXISTS _annotations ( trace_id_high BIGINT NOT NULL DEFAULT 0 COMMENT ‘If non zero, this means the trace uses 128 bit traceIds instead of 64 bit’, trace_id BIGINT NOT NULL COMMENT ‘coincides with _spans.trace_id’, span_id BIGINT NOT NULL COMMENT ‘coincides with _spans.id’, a_key VARCHAR(255) NOT NULL COMMENT ‘BinaryAnnotation.key or Annotation.value if type == -1’, a_value BLOB COMMENT ‘BinaryAnnotation.value(), which must be smaller than 64KB’, a_type INT NOT NULL COMMENT ‘BinaryAnnotation.type() or -1 if Annotation’, a_timestamp BIGINT COMMENT ‘Used to implement TTL; Annotation.timestamp or _spans.timestamp’, endpoint_ipv4 INT COMMENT ‘Null when Binary/Annotation.endpoint is null’, endpoint_ipv6 BINARY(16) COMMENT ‘Null when Binary/Annotation.endpoint is null, or no IPv6 address’, endpoint_port SMALLINT COMMENT ‘Null when Binary/Annotation.endpoint is null’, endpoint_service_name VARCHAR(255) COMMENT ‘Null when Binary/Annotation.endpoint is null’) ENGINE=InnoDB ROW_FORMAT=COMPRESSED CHARACTER SET=utf8 COLLATE utf8_general_ci; ALTER TABLE _annotations ADD UNIQUE KEY(trace_id_high, trace_id, span_id, a_key, a_timestamp) COMMENT ‘Ignore insert on duplicate’;ALTER TABLE _annotations ADD INDEX(trace_id_high, trace_id, span_id) COMMENT ‘for joining with _spans’;ALTER TABLE _annotations ADD INDEX(trace_id_high, trace_id) COMMENT ‘for getTraces/ByIds’;ALTER TABLE _annotations ADD INDEX(endpoint_service_name) COMMENT ‘for getTraces and getServiceNames’;ALTER TABLE _annotations ADD INDEX(a_type) COMMENT ‘for getTraces and autocomplete values’;ALTER TABLE _annotations ADD INDEX(a_key) COMMENT ‘for getTraces and autocomplete values’;ALTER TABLE _annotations ADD INDEX(trace_id, span_id, a_key) COMMENT ‘for dependencies job’; CREATE TABLE IF NOT EXISTS _dependencies ( day DATE NOT NULL, parent VARCHAR(255) NOT NULL, child VARCHAR(255) NOT NULL, call_count BIGINT, error_count BIGINT, PRIMARY KEY (day, parent, child)) ENGINE=InnoDB ROW_FORMAT=COMPRESSED CHARACTER SET=utf8 COLLATE utf8_general_ci; 启动脚本RABBIT_URI=amqp://admin:admin@192.168.172.3:5672/sleuth STORAGE_TYPE=mysql MYSQL_DB= MYSQL_USER=root MYSQL_PASS=123456 MYSQL_TCP_PORT=3306 MYSQL_HOST=192.168.172.2 MYSQL_DB= MYSQL_USE_SSL=false java -jar .jar elaticsearch存储启动脚本RABBIT_URI=amqp://admin:admin@192.168.172.3:5672/sleuth STORAGE_TYPE=elasticsearch ES_HOSTS=http://192.168.172.8:9200 ES_INDEX_REPLICAS=0 java -jar .jar –logging.level.=debug –logging.level.2=debug &amp; &gt; .log","categories":[],"tags":[{"name":"链路跟踪技术","slug":"链路跟踪技术","permalink":"https://youxia999.github.io/tags/链路跟踪技术/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"玩转kubernetes之为什么要学习kubernetes","slug":"container-why-learn-kubernetes","date":"2018-04-29T14:22:20.000Z","updated":"2020-02-06T04:15:12.883Z","comments":true,"path":"2018/04/29/container-why-learn-kubernetes/","link":"","permalink":"https://youxia999.github.io/2018/04/29/container-why-learn-kubernetes/","excerpt":"","text":"以下内容属于阅读/转载。 kubernetes与微服务架构 图片来源博客园，版权属163云刘超 附录为什么kubernetes天然适合微服务:http://www.cnblogs.com/163yun/p/8855360.html","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【转载】利用Prometheus和Grafana可视化mysql性能","slug":"prometheus-monitor-mysql","date":"2018-04-14T13:39:46.000Z","updated":"2020-02-06T04:13:31.663Z","comments":true,"path":"2018/04/14/prometheus-monitor-mysql/","link":"","permalink":"https://youxia999.github.io/2018/04/14/prometheus-monitor-mysql/","excerpt":"","text":"原文地址https://www.percona.com/blog/2016/02/29/graphing-mysql-performance-with-prometheus-and-grafana/ 截图版","categories":[],"tags":[{"name":"监控技术","slug":"监控技术","permalink":"https://youxia999.github.io/tags/监控技术/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"go语言http编程","slug":"golang-build-web-application","date":"2018-03-10T11:23:20.000Z","updated":"2019-12-27T11:59:45.490Z","comments":true,"path":"2018/03/10/golang-build-web-application/","link":"","permalink":"https://youxia999.github.io/2018/03/10/golang-build-web-application/","excerpt":"","text":"参考资料https://time-track.cn/go-http-server-learning.html http编程基础类http包下的ListenAndServe、Server、conn、serverHandler、Handler接口 工作时序图","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"golang","slug":"golang","permalink":"https://youxia999.github.io/tags/golang/"}]},{"title":"apollo配置中心wrapper启动","slug":"apollo-config-wrapper-install","date":"2018-02-05T11:54:55.000Z","updated":"2020-02-06T04:11:40.201Z","comments":true,"path":"2018/02/05/apollo-config-wrapper-install/","link":"","permalink":"https://youxia999.github.io/2018/02/05/apollo-config-wrapper-install/","excerpt":"","text":"非官方参考资料https://286.iteye.com/blog/1915478 本文背景几个业务平台的几套环境的配置参数管理很混乱，让有些新进入的小伙伴很混乱，所以，引入配置中心迫在眉睫。 配置中心的部署规划图部署ip规划configservice+eurekaserver的ip端口：10.2.1.30:8761adminservice的ip端口：10.2.1.30:8762portald的ip端口：10.2.1.30:8763 1.2 账户和路径规划由root创建apolloconfig账号.并创建/data/server/apolloconfig目录、/data/server/logs/apolloconfig目录,并授权给apolloconfig用户。 1.3 配置中心的最终使用使用入口：http://10.2.1.30:8763/注册中心：http://10.2.1.30:8761/ 下载github源码打包clone源码目前来说，apollo最高版本的v1.1.0，但还是snapshot版本的，因此不建议使用，而源码master分支是对应v1.1版本，这时先切换到v1.0.0的tag. 将项目中的scripts/sql导入脚本到mysql数据库1.打开项目中的scripts/sql文件夹，将apolloconfigdb，apolloportaldb的sql,改成自己想要的数据库名com_apolloconfig_db、com_apolloportal_db。 2.修改com_apolloconfig_db的serverconfig表的eureka地址为10.2.1.30:8761/eureka 3.修改com_apolloportal_db的serverconfig表的apollo.portal.envs为fat 修改com_apolloportal_db的serverconfig表的organizations为[{&quot;orgId&quot;:&quot;XXJYZ&quot;,&quot;orgName&quot;:&quot;**交易组&quot;},{&quot;orgId&quot;:&quot;YUN&quot;,&quot;orgName&quot;:&quot;**云平台&quot;}] 执行mvn install -Dmaven.test.skip打包 执行mvn install -Dmaven.test.skip 分别把target中的apollo-configservice-1.0.0-github.zip，apollo-adminservice-1.0.0-github.zip，apollo-portal-1.0.0-github.zip取出， 这就是我们即将使用的部署包。 基于wrapper工程完整制作过程创建三个wrapper工程adminservice、configservice、portalservice 将apollo-adminservice/target/apollo-adminservice-1.0.0-github.zip下的apollo-adminservice-1.0.0.jar放到adminservice的app目录下。 将apollo-configservice/target/apollo-configservice-1.0.0-github.zip下的apollo-configservice-1.0.0.jar放到configservice的app目录下。 将apollo-portal/target/apollo-portal-1.0.0-github.zip下的apollo-portal-1.0.0.jar放到portalservice的app目录下。 配置configservice复制源码工程下的配置文件到wrapper工程的conf目录 ①复制apollo-configservice\\src\\main\\config的三个文件到configservice的wrapper工程的conf目录 ②apollo-configservice\\src\\main\\resources的4个文件到configservice的wrapper工程的conf目录 修改apolloconfigdb的数据库表serverconfig 打开步骤2中创建的com_apolloconfig_db数据库，将serverconfig表中的eureka.service.url配置项改成http://10.2.1.30:8897/eureka/ 修改configservice配置文件 修改bootstrap.yml文件。 修改application-github.properties。将application-github.properties修改为application-fat.properties。并将数据库信息配置成步骤2中建立的com_apolloconfig_db 修改application.yml 修改configservice.properties 修改apollo-configservice.conf文件 修改logback文件 增加wrapper文件 配置adminservice复制源码工程下的配置文件到wrapper工程的conf目录 复制apollo-configservice\\src\\main\\config的三个文件到adminservice的wrapper工程的conf目录 复制apollo-configservice\\src\\main\\resources的4个文件到adminservice的wrapper工程的conf目录 修改adminservice配置文件 修改bootstrap.yml文件 修改application.yml 修改apollo-configservice.conf文件 修改application-github.properties 修改为application-fat.properties 修改logback.xml文件 增加wrapper相关文件 配置portalservice复制源码工程下的配置文件到wrapper工程的conf目录 复制apollo-portal\\src\\main\\config的三个文件到portalservice的wrapper工程的conf目录 复制apollo-portal\\src\\main\\resources的4个文件到portalservice的wrapper工程的conf目录 修改portalservice配置文件 修改apollo-env.properties 修改application.yml apollo-portal.conf 修改application-github.properties。application-github.properties为application-fat.properties,并修改内容 logback文件 新增wrapper相关文件","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"镜像仓库harbor0.5.0版本安装和使用","slug":"docker-registry-harbor-05-install","date":"2018-01-14T14:49:09.000Z","updated":"2020-02-06T04:15:12.997Z","comments":true,"path":"2018/01/14/docker-registry-harbor-05-install/","link":"","permalink":"https://youxia999.github.io/2018/01/14/docker-registry-harbor-05-install/","excerpt":"","text":"本文背景背景玩docker也差不多一年有余了，但是一直依赖没有一个私有仓库(docker registry没有可视化的界面，一直被我嫌弃)，一直依赖都是在docker hub上面找。没有形成自己的积淀 本文参考资料https://www.cnblogs.com/huangjc/p/6266564.html 下载、配置和安装下载harbor0.5.0版本离线版wget https://github.com/vmware/harbor/releases/download/0.5.0/harbor-offline-installer-0.5.0.tgz 下载docker-copose 1.10版本下载docker-copose 1.10版本的二进制文件curl -L https://github.com/docker/compose/releases/download/1.10.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 修改harbor配置vim harbor.cfg#修改iphostname = 10.2.1.30 修改docker-compose.yml文件#暴露registry端口，否则有时会报连接被拒绝 执行./install.sh脚本登录查看效果","categories":[],"tags":[{"name":"持续集成技术","slug":"持续集成技术","permalink":"https://youxia999.github.io/tags/持续集成技术/"},{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【java应用开发脚手架系列】spring boot与dubbo 2.5.8整合体验","slug":"spring_boot_dubbo_2.5.8_experience","date":"2017-12-28T06:04:49.000Z","updated":"2019-12-27T11:59:45.735Z","comments":true,"path":"2017/12/28/spring_boot_dubbo_2.5.8_experience/","link":"","permalink":"https://youxia999.github.io/2017/12/28/spring_boot_dubbo_2.5.8_experience/","excerpt":"","text":"本文参考资料https://www.oschina.net/news/91565/dubbo-2-5-8-released","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"centos7下rabbitmq3.7.2集群配置(续)","slug":"rabbitmq-cluster-install_2","date":"2017-12-23T13:26:52.000Z","updated":"2020-02-06T04:16:21.802Z","comments":true,"path":"2017/12/23/rabbitmq-cluster-install_2/","link":"","permalink":"https://youxia999.github.io/2017/12/23/rabbitmq-cluster-install_2/","excerpt":"","text":"参考资料https://www.cnblogs.com/me-sa/p/erlang-epmd.html erlang虚拟机查询官网后得知，rabbitmq是一个基于erlang语言的应用程序。在 Erlang中有两个概念：节点(node)和应用程序(application)。 节点(node)node为运行Erlang虚拟机的一个实例。 应用程序(application)多个Erlang应用程序(application)可以运行在同一个节点(node)之上。 erlang节点特点节点(node)之间可进行通信（无论是否运行在同一台服务器)。简单来说：1.一个运行在节点A上的应用程序（application）可以调用节点B上应用程序的方法，就好像调用本地函数一样。2.如果要关闭整个RabbitMQ节点可以使用stop参数，它会和本地节点通信并指示其干净得关闭：/sbin/rabbitmqctl stop。比如：默认node名称是rabbit@server，如果主机名是server.example.com，那么node名称就是 rabbit@server.example.com/sbin/rabbitmqctl -n rabbit@server.example.com stop 3.rabbitmqctl关闭RabbitMQ应用程序，如果只想关闭应用程序，同时保持Erlang节点运行则可以使用 stop_app。rabbitmqctl也可以指定关闭不同得节点（包括远程节点），只需传入参数也可以指定关闭不同的节点（包括远程节点），且只需传入参数 -n node(依赖于下面要提到的epmd)。/sbin/rabbitmqctl -n rabbit@server.example.com stop_app empd rabbitmq顶层架构集群整体架构 单点整体架构 启动参数解析上面提到的整体架构可能有点抽象，看下启动参数/usr/lib64/erlang/erts-9.3.3.6/bin/beam.smp -W w -A 64 -P 1048576 -t 5000000 -stbt db -zdbbl 128000 -K true -- -root /usr/lib64/erlang #erlang虚拟机目录-progname erl -- -home /var/lib/rabbitmq #rabbitmq家目录-- -pa /usr/lib/rabbitmq/lib/rabbitmq_server-3.7.2/ebin -noshell-noinput -s rabbit boot -sname rabbit@lxg53 -boot start_sasl -conf /etc/rabbitmq/rabbitmq #rabbitmq配置文件-conf_dir /var/lib/rabbitmq/config #rabbitmq配置文件目录-conf_script_dir /usr/lib/rabbitmq/bin -conf_schema_dir /var/lib/rabbitmq/schema -kernel inet_default_connect_options [&#123;nodelay,true&#125;] #kerner程序的配置-sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root &quot;/data/rabbitmq/log&quot; #rabbitmq程序的日志配置-rabbit lager_default_file &quot;/data/rabbitmq/log/rabbit@lxg53.log&quot; #rabbitmq程序的日志配置-rabbit lager_upgrade_file &quot;/data/rabbitmq/log/rabbit@lxg53_upgrade.log&quot; #rabbitmq程序的日志配置-rabbit enabled_plugins_file &quot;/etc/rabbitmq/enabled_plugins&quot; #rabbitmq程序的插件配置-rabbit plugins_dir &quot;/usr/lib/rabbitmq/plugins:/usr/lib/rabbitmq/lib/rabbitmq_server-3.7.2/plugins&quot; #rabbitmq程序的插件目录 -rabbit plugins_expand_dir &quot;/data/rabbitmq/mnesia/rabbit@lxg53-plugins-expand&quot; -os_mon start_cpu_sup false #rabbitmq程序的配置-os_mon start_disksup false -os_mon start_memsup false -mnesia dir &quot;/data/rabbitmq/mnesia/rabbit@lxg53&quot; -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672 rabbitmq相关应用程序第二种手段：查询日志。结合启动参数、日志后可以看出，rabbitmq相关的erlang程序包括： rabbitmq_management amqp_client rabbitmq_web_dispatch cowboy #http server cowlib rabbitmq_management_agent #基于cowboy的插件 rabbit mnesia rabbit_common os_mon2017-12-23 13:21:40.187 [info] &lt;0.18635.0&gt; RabbitMQ is asked to stop...2017-12-23 13:21:40.410 [info] &lt;0.18635.0&gt; Stopping RabbitMQ applications and their dependencies in the following order: rabbitmq_management amqp_client rabbitmq_web_dispatch cowboy cowlib rabbitmq_management_agent rabbit mnesia rabbit_common os_mon2017-12-23 13:21:40.410 [info] &lt;0.18635.0&gt; Stopping application &apos;rabbitmq_management&apos;2017-12-23 13:21:40.414 [info] &lt;0.33.0&gt; Application rabbitmq_management exited with reason: stopped2017-12-23 13:21:40.414 [info] &lt;0.18635.0&gt; Stopping application &apos;amqp_client&apos;2017-12-23 13:21:40.416 [info] &lt;0.33.0&gt; Application amqp_client exited with reason: stopped2017-12-23 13:21:40.416 [info] &lt;0.18635.0&gt; Stopping application &apos;rabbitmq_web_dispatch&apos;2017-12-23 13:21:40.419 [info] &lt;0.33.0&gt; Application rabbitmq_web_dispatch exited with reason: stopped2017-12-23 13:21:40.419 [info] &lt;0.18635.0&gt; Stopping application &apos;cowboy&apos;2017-12-23 13:21:40.421 [info] &lt;0.33.0&gt; Application cowboy exited with reason: stopped2017-12-23 13:21:40.421 [info] &lt;0.18635.0&gt; Stopping application &apos;cowlib&apos;2017-12-23 13:21:40.421 [info] &lt;0.33.0&gt; Application cowlib exited with reason: stopped2017-12-23 13:21:40.421 [info] &lt;0.18635.0&gt; Stopping application &apos;rabbitmq_management_agent&apos;2017-12-23 13:21:40.424 [info] &lt;0.33.0&gt; Application rabbitmq_management_agent exited with reason: stopped2017-12-23 13:21:40.424 [info] &lt;0.18635.0&gt; Stopping application &apos;rabbit&apos;2017-12-23 13:21:40.424 [info] &lt;0.944.0&gt; Peer discovery backend rabbit_peer_discovery_classic_config does not support registration, skipping unregistration.2017-12-23 13:21:40.424 [info] &lt;0.1300.0&gt; stopped TCP Listener on [::]:56722017-12-23 13:21:40.425 [info] &lt;0.1039.0&gt; Closing all connections in vhost &apos;bbs&apos; on node &apos;rabbit@lxg54&apos; because the vhost is stopping2017-12-23 13:21:40.426 [info] &lt;0.1039.0&gt; Closing all connections in vhost &apos;fsp_metrics&apos; on node &apos;rabbit@lxg54&apos; because the vhost is stopping2017-12-23 13:21:40.426 [info] &lt;0.1236.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/2EO163IWD87C0LGDYBMCOU9AZ/msg_store_persistent&apos;2017-12-23 13:21:40.426 [info] &lt;0.1183.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/CJ961M2R0VKA2HG0XCI215CBA/msg_store_persistent&apos;2017-12-23 13:21:40.426 [info] &lt;0.1130.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L/msg_store_persistent&apos;2017-12-23 13:21:40.426 [info] &lt;0.1077.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/BILT8EMIAEYFVG7TLIG4764T5/msg_store_persistent&apos;2017-12-23 13:21:40.426 [info] &lt;0.1039.0&gt; Closing all connections in vhost &apos;/&apos; on node &apos;rabbit@lxg54&apos; because the vhost is stopping2017-12-23 13:21:40.426 [info] &lt;0.1039.0&gt; Closing all connections in vhost &apos;fsp-asynctask-service&apos; on node &apos;rabbit@lxg54&apos; because the vhost is stopping2017-12-23 13:21:40.482 [info] &lt;0.1130.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L/msg_store_persistent&apos; is stopped2017-12-23 13:21:40.482 [info] &lt;0.1127.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L/msg_store_transient&apos;2017-12-23 13:21:40.483 [info] &lt;0.1236.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/2EO163IWD87C0LGDYBMCOU9AZ/msg_store_persistent&apos; is stopped2017-12-23 13:21:40.483 [info] &lt;0.1077.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/BILT8EMIAEYFVG7TLIG4764T5/msg_store_persistent&apos; is stopped2017-12-23 13:21:40.483 [info] &lt;0.1183.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/CJ961M2R0VKA2HG0XCI215CBA/msg_store_persistent&apos; is stopped2017-12-23 13:21:40.483 [info] &lt;0.1074.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/BILT8EMIAEYFVG7TLIG4764T5/msg_store_transient&apos;2017-12-23 13:21:40.483 [info] &lt;0.1233.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/2EO163IWD87C0LGDYBMCOU9AZ/msg_store_transient&apos;2017-12-23 13:21:40.483 [info] &lt;0.1180.0&gt; Stopping message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/CJ961M2R0VKA2HG0XCI215CBA/msg_store_transient&apos;2017-12-23 13:21:40.626 [info] &lt;0.1127.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L/msg_store_transient&apos; is stopped2017-12-23 13:21:40.626 [info] &lt;0.1233.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/2EO163IWD87C0LGDYBMCOU9AZ/msg_store_transient&apos; is stopped2017-12-23 13:21:40.626 [info] &lt;0.1180.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/CJ961M2R0VKA2HG0XCI215CBA/msg_store_transient&apos; is stopped2017-12-23 13:21:40.627 [info] &lt;0.1074.0&gt; Message store for directory &apos;/var/lib/rabbitmq/mnesia/rabbit@lxg54/msg_stores/vhosts/BILT8EMIAEYFVG7TLIG4764T5/msg_store_transient&apos; is stopped2017-12-23 13:21:40.631 [info] &lt;0.18635.0&gt; Stopping application &apos;mnesia&apos;2017-12-23 13:21:40.631 [info] &lt;0.33.0&gt; Application rabbit exited with reason: stopped2017-12-23 13:21:40.634 [info] &lt;0.33.0&gt; Application mnesia exited with reason: stopped2017-12-23 13:21:40.634 [info] &lt;0.18635.0&gt; Stopping application &apos;rabbit_common&apos;2017-12-23 13:21:40.634 [info] &lt;0.33.0&gt; Application rabbit_common exited with reason: stopped2017-12-23 13:21:40.634 [info] &lt;0.18635.0&gt; Stopping application &apos;os_mon&apos;2017-12-23 13:21:40.636 [info] &lt;0.33.0&gt; Application os_mon exited with reason: stopped2017-12-23 13:21:40.636 [info] &lt;0.18635.0&gt; Successfully stopped RabbitMQ and its dependencies rabbitmq核心概念Broker简单来说就是消息队列服务器的实体，类似于 JMS 规范中的 JMS provider。它用于接收和分发消息，有时候也称为 Message Broker 或者更直白的称为 RabbitMQ Server。 cluster一群broker组成的集群，其中一个是disc节点，其他的为ram节点。 Virtual Host和 Web 服务器中的虚拟主机（Virtual Host）是类似的概念，出于多租户和安全因素设计的，可以将 RabbitMQ Server 划分成多个独立的空间，彼此之间互相独立，这样就可以将一个 RabbitMQ Server 同时提供给多个用户使用，每个用户在自己的空间内创建 Exchange 和 Queue。 Exchange交换机用于接收消息，这是消息到达 Broker 的第一站，然后根据交换机的类型和路由规则（Routing Key），将消息分发到特定的队列中去。常用的交换机类型有：direct (point-to-point)、topic (publish-subscribe) 和 fanout (multicast)。 Queue生产者发送的消息就是存储在这里，在 JMS 规范里，没有 Exchange 的概念，消息是直接发送到 Queue，而在 AMQP 中，消息会经过 Exchange，由 Exchange 来将消息分发到各个队列中。消费者可以直接从这里取走消息。 Binding绑定的作用就是把 Exchange 和 Queue 按照路由规则绑定起来，路由规则可由下面的 Routing Key 指定。 Routing Key路由关键字，Exchange 根据这个关键字进行消息投递。 Producer/Publisher消息生产者或发布者，产生消息的程序。 Consumer/Subscriber消息消费者或订阅者，接收消息的程序。 Connection生产者和消费者和 Broker 之间的连接，一个 Connection 实际上就对应着一条 TCP 连接。 Channel由于 TCP 连接的创建和关闭开销非常大，如果每次访问 Broker 都建立一个 Connection，在消息量大的时候效率会非常低。Channel 是在 Connection 内部建立的逻辑连接，相当于一次会话，如果应用程序支持多线程，通常每个线程都会创建一个单独的 Channel 进行通讯，各个 Channel 之间完全隔离，但这些 Channel 可以公用一个 Connection。","categories":[],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"https://youxia999.github.io/tags/rabbitmq/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"centos7下rabbitmq3.7.2集群配置","slug":"rabbitmq-cluster-install","date":"2017-12-23T11:30:54.000Z","updated":"2020-02-06T04:16:21.791Z","comments":true,"path":"2017/12/23/rabbitmq-cluster-install/","link":"","permalink":"https://youxia999.github.io/2017/12/23/rabbitmq-cluster-install/","excerpt":"","text":"参考资料和更新历史本文参考资料http://www.cnblogs.com/luxiaoxun/p/3918054.htmlhttps://www.jianshu.com/p/518a504fe2cchttps://www.rabbitmq.com/changelog.htmlhttp://blog.didispace.com/spring-boot-rabbitmq/https://blog.csdn.net/zyz511919766/article/details/41896747https://www.rabbitmq.com/configure.htmlhttps://www.jianshu.com/p/b6bdd6f56f9c 文章更新历史2017-12-23 19:30:54 初稿2019-04-29 20:30:54 因为erlang的rpm包被移走了，需要更新repo文件 部署规划集群所有机器增加hosts10.2.1.53 lxg5310.2.1.54 lxg5410.2.1.55 lxg55 目录规划配置文件目录:/etc/rabbitmq/主要工作目录:/var/lib/rabbitmq日志文件目录:/data/rabbitmq/log数据库目录：/data/rabbitmq/mnesia/rabbit@lxg53 ps:真的很讨厌程序的工作目录放在/var/lib目录下（这是系统盘下，如果系统盘符小，分分钟撑爆系统盘符）。但是目前也没有找到有效的办法，修改工作目录。 安装erlang与rabbit-server集群更新curl程序yum update nss curl 更新erlang的repovim /etc/yum.repos.d/rabbitmq-erlang.repo[rabbitmq-erlang]name=rabbitmq-erlangbaseurl=https://dl.bintray.com/rabbitmq-erlang/rpm/erlang/20/el/7gpgcheck=1gpgkey=https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascrepo_gpgcheck=0enabled=1 安装erlangyum install erlang 安装rabbitmq-serverwget https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.2/rabbitmq-server-3.7.2-1.el7.noarch.rpmrpm --import https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascyum install rabbitmq-server-3.7.2-1.el7.noarch.rpm 修改rabbitmq配置配置文件位置 rabbit3.7之后，采用了rabbitmq.conf、rabbitmq-env.conf。（网上很多照抄的说，从3.7开始命名叫rabbitmq.config，看着蛋疼） rabbitmq.conf rabbitmq-env.conf 启动rabbitmq主节点和插件创建目录及启动rabbit-serverchown -R rabbitmq:rabbitmq /var/lib/rabbitmq/mkdir -p data/rabbitmq/logmkdir -p /data/rabbitmq/mnesiachown -R rabbitmq:rabbitmq /data/rabbitmq/systemctl start rabbitmq-server 启用插件授权登录rabbitmq-plugins enable rabbitmq_managementrabbitmqctl add_user admin adminrabbitmqctl set_user_tags admin administratorrabbitmqctl set_permissions -p / admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;rabbitmqctl status 其他的插件rabbitmq-plugins enable rabbitmq_mqttrabbitmq-plugins enable rabbitmq_stomprabbitmq-plugins enable rabbitmq_web_mqttrabbitmq-plugins enable rabbitmq_web_stomp 同步cookie和启动从节点同步cookie到从节点cat /var/lib/rabbitmq/.erlang.cookieDSXSMAIWGQYIUAALJXQH nc到从节点的/var/lib/rabbitmq/目录。 启动从节点systemctl start rabbitmq-serverrabbitmq-plugins enable rabbitmq_managementrabbitmqctl add_user admin adminrabbitmqctl set_user_tags admin administratorrabbitmqctl set_permissions -p / admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;rabbitmqctl stop_apprabbitmqctl reset 从节点加入到集群rabbitmqctl join_cluster --ram rabbit@lxg53rabbitmqctl start_app 某个从节点重启systemctl start rabbitmq-serverrabbitmqctl stop_apprabbitmqctl start_app 镜像队列负载均衡负载均衡#---------------------------------------------------------------------# Example configuration for a possible web application. See the# full configuration options online.## http://haproxy.1wt.eu/download/1.4/doc/configuration.txt##---------------------------------------------------------------------#---------------------------------------------------------------------# Global settings#---------------------------------------------------------------------global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local0 info chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will# use if not designated in their block#---------------------------------------------------------------------defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000listen status bind 10.2.1.30:18080 #监听的地址和端口，默认端口1080 mode http #模式 stats enable #启用状态监控 stats hide-version #隐藏软件版本号 stats auth admin:admin #登陆用户名和密码 stats realm HAproxy\\ stats #提示信息，空格之前加\\ stats admin if TRUE #当通过认证才可管理 stats uri /stats #访问路径，在域名后面添加/stats可以查看haproxy监控状态,默认为/haproxy?stats stats refresh 5 #页面自动刷新间隔,每隔5s刷新frontend rabbitmq_broker bind 10.2.1.30:5672 default_backend rabbitmq_broker_backendbackend rabbitmq_broker_backend balance roundrobin server lxg53 10.2.1.53:5672 check server lxg54 10.2.1.54:5672 check server lxg55 10.2.1.55:5672 checkfrontend rabbitmq_management bind 10.2.1.30:15672 default_backend rabbitmq_management_backendbackend rabbitmq_management_backend balance roundrobin server lxg53 10.2.1.53:15672 check server lxg54 10.2.1.54:15672 check server lxg55 10.2.1.55:15672 check #---------------------------------------------------------------------# main frontend which proxys to the backends#---------------------------------------------------------------------#frontend main *:5000# acl url_static path_beg -i /static /images /javascript /stylesheets# acl url_static path_end -i .jpg .gif .png .css .js# use_backend static if url_static# default_backend app#---------------------------------------------------------------------# static backend for serving up images, stylesheets and such#---------------------------------------------------------------------#backend static# balance roundrobin# server static 127.0.0.1:4331 check#---------------------------------------------------------------------# round robin balancing between the various backends#---------------------------------------------------------------------#backend app# balance roundrobin# server app1 127.0.0.1:5001 check# server app2 127.0.0.1:5002 check# server app3 127.0.0.1:5003 check# server app4 127.0.0.1:5004 check 最终效果","categories":[],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"https://youxia999.github.io/tags/rabbitmq/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【转载】rabbitmq手册之rabbitmqctl","slug":"rabbitmqctl-handbook","date":"2017-12-22T11:52:16.000Z","updated":"2020-02-06T04:16:21.829Z","comments":true,"path":"2017/12/22/rabbitmqctl-handbook/","link":"","permalink":"https://youxia999.github.io/2017/12/22/rabbitmqctl-handbook/","excerpt":"","text":"原文地址https://www.jianshu.com/p/61a90fba1d2a application相关命令(集群管理的基础)rabbitmqctl stop [{pid_file}]rabbitmqctl stop 或者 rabbitmqctl stop[{pid_file}]表示stop 在RabbitMQ服务器上运行的一个Erlang节点，可以指定某一个 *pid_file*，表示会等待这个指定的程序结束 rabbitmqctl shutdown表示终止RabbitMQ 服务器上的Erlang进程，如果终止失败，会返回非零数字 rabbitmqctl stop_app表示终止RabbitMQ的应用，但是Erlang节点还在运行。该命令典型的运行在一些需要RabbitMQ应用被停止的管理行为之前，例如 reset rabbitmqctl start_app表示启动RabbitMQ的应用。该命令典型的运行在一些需要RabbitMQ应用被停止的管理行为之后，例如 reset rabbitmqctl wait {pid_file}表示等待RabbitMQ应用启动。该命令会等待指定的pid file被创建，也就是启动的进程对应的pid保存在这个文件中，然后RabbitMQ应用在这个进程中启动。如果该进程终止，没有启动RabbitMQ应用，就会返回错误。合适的pid file是有rabbitmq-server 脚本创建的，默认保存在 Mnesia 目录下，可以通过修改 RABBITMQ_PID_FILE 环境变量来修改例如 rabbitmqctl wait /var/run/rabbitmq/pid rabbitmqctl reset表示设置RabbitMQ节点为原始状态。会从该节点所属的cluster中都删除，从管理数据库中删除所有数据，例如配置的用户和vhost，还会删除所有的持久消息。要想reset和force_reset操作执行成功，RabbitMQ应用需要处于停止状态，即执行过 stop_app rabbitmqctl force_reset表示强制性地设置RabbitMQ节点为原始状态。它和reset的区别在于，可以忽略目前管理数据库的状态和cluster的配置，无条件的reset。该方法的使用，应当用在当数据库或者cluster配置损坏的情况下作为最后的方法。 rabbitmqctl rotate_logs {suffix}表示将日志文件的内容追加到新的日志文件中去，这个新的日志文件的文件名是原有的日志文件名加上命令中的 suffix，并且恢复日志到原来位置的新文件中。注意：如果新文件原先不存在，那么会新建一个；如果suffix为空，那么不会发生日志转移，只是重新打开了一次日志文件而已。 rabbitmqctl hipe_compile {directory}表示在指定的目录下执行HiPE编译和缓存结果文件 .beam-files如果需要父目录会被创建。并且在编译之前，该目录下的所有 .beam-files会被自动删除。使用预编译的文件，你应该设置 RABBITMQ_SERVER_CODE_PATH 环境变量为 hipe_compile 调用指定的目录。 cluster management(集群管理)rabbitmqctl join_cluster {clusternode} [–ram]表示结合到指定的集群，如果有参数 --ram 表示作为RAM节点结合到该集群中。该命令指令本节结合到指定的集群中，在结合之前，该节点需要reset，所以在使用时，需要格外注意。为了成功运行本命令，必须要停止RabbitMQ应用，例如 stop_app集群节点有两种类型: disc 和 RAM。disc类型，复制数据在RAM和disc上，在节点失效的情况下，提供了冗余保证，也能从一些全局事件中恢复，例如所有节点失效。RAM类型，只复制数据在RAM上，主要表现在伸缩性上，特别是在管理资源（例如：增加删除队列，交换器，或者绑定）上表现突出。一个集群必须至少含有一个disc节点，当通常都多余一个。通过该命令时，默认是设置为disc节点，如果需创建RAM节点，需要指定参数 --ram执行此命令之后，在该节点上启动的RabbitMQ应用，在该节点挂掉之后，会尝试连接节点所在集群中的其他节点。为了离开集群，可以 reset 该节点，也可以使用命令 forget_cluster_node 远程删除节点 rabbitmqctl cluster_status表示显示通过节点类型聚合在一起的集群中的所有节点，还有目前正在运行的节点 rabbitmqctl change_cluster_node_type {disc|ram}表示改变集群节点的类型。该操作的正确执行，必定会停止该节点。并且在调整一个node为ram类型时，该节点不能为该集群的唯一node rabbitmqctl forget_cluster_node [–offline]表示远程移除一个集群节点。要删除的节点必须脱机，如果没有脱机，需要使用 --offline 参数。当使用 --offline 参数时，rabbitmqctl不会去连接节点，而是暂时变成节点，以便进行变更。这在节点不能正常启动时非常有用。在这种情况下，节点会成为集群元数据的规范来源（例如哪些队列存在）。因此如果可以的话，应该使用此命令在最新的节点上关闭。--offline 参数使节点从脱机节点上移除。使用场景主要是在所有节点脱机，且最后一个节点无法联机时，从而防止整个集群启动。在其他情况不应该使用，否则会导致不一致。例如 rabbitmqctl -n hare@mcnulty forget_cluster_node rabbit@stringer 上述命令将从节点 hare@mcnulty 中移除节点 rabbit@stringer rabbitmqctl rename_cluster_node {oldnode1} {newnode1} [oldnode2] [newnode2…]表示在本地数据库上修改集群节点名称。该命令让rabbitmqctl暂时成为一个节点来做出做变更。因此，本地的待修改的集群节点一定要完全停止，其他节点可以是online或者offline rabbitmqctl update_cluster_nodes {clusternode}表示指示已经集群的节点在唤醒时联系 &#123;clusternode&#125; 进行集群。这与 join_cluster 命令不同，因为它不加入任何集群，它是检查节点是否已经在具有 &#123;clusternode&#125; 的集群中。该命令的需求，是在当一个节点offline时，修改了集群节点的情形下。例如：节点A和B聚群，节点A offline了，节点C和B集群，并且B节点离开了该集群，那么当节点A起来的时候，A会尝试连接B，但是由于B节点已经不在该集群中，所以会失败。通过 update_cluster_nodes -n A C 将会解决上述问题。 rabbitmqctl force_boot表示强制确保节点启动，即使该节点并不是最后down的。一般情况下，当你同时shut down了RabbitMQ集群时，第一个重启的节点应该是最后一个down掉的，因为它可能已经看到了其他节点发生的事情。但是有时候这并不可能：例如当整个集群lose power，那么该集群的所有节点会认为他们不是最后一个关闭的。如果最后down的节点永久的lost，那么应该优先使用 rabbitmqctl forget_cluster_node --offline ，因为这将确保在丢失节点上的镜像队列得到优先处理。 rabbitmqctl sync_queue [-p vhost] {queue}&#123;queue&#125; 表示待同步的队列名称指引含有异步slaves的镜像队列去同步自身。当队列执行同步化时，其将会被锁定（指所有publishers发送出去的和consumers获取到的队列都会被锁定）。为了成功执行此命令，队列必须要被镜像。注意，排除消息的异步队列将最终被同步化，此命令主要运用于未被排除完全消息的队列。 rabbitmqctl cancel_sync_queue [-p vhost] {queue}指引一个正在同步的镜像队列停止此操作。 rabbitmqctl purge_queue [-p vhost] {queue}&#123;queue&#125; 表示待清空消息的队列名称该命令表示清空队列（即删除队列中的所有消息） rabbitmqctl set_cluster_name {name}设置集群的名称。在连接中，集群的名称被声明在客户端上，被同盟和插件用来记录一个消息所在的位置。集群的名称默认来自于集群中第一个节点的主机名，但是可以被修改。 User management(用户和角色管理)用户角色rabbitmq用户角色（role）分为五类： 超级管理员（administrator）administrator可登录管理控制台（启用management plugin的情况下），查看所有的信息，并且可以对用户、策略（policy）进行操作。 监控者（monitor）monitoring可登录管理控制台（启用management plugin的情况下），同时可以查看rabbitmq节点的相关信息（进程数、内存使用情况，磁盘使用情况等）。 决策制定者（policymaker）policymaker可以登录管理控制台（启用management plugin的情况下），同时可以对策略（policy）进行操作。 普通管理者（management）management 仅可登录管理控制台（启用management plugin的情况下），无法看到节点信息，也无法对策略进行管理。 其他无法登录管理控制台，通常就是普通的生产者和消费者。注意，rabbitmqctl 管理 RabbitMQ 的内部用户数据库，所有其他后台需要认证的用户对于rabbitmqctl将不可见。 rabbitmqctl add_user {username} {password}&#123;username&#125; 表示用户名； ｛password｝表示用户密码该命令将创建一个 non-administrative 用户 rabbitmqctl delete_user {username}表示删除一个用户，该命令将指示RabbitMQ broker去删除指定的用户 rabbitmqctl change_password {username} {newpassword}表示修改指定的用户的密码 rabbitmqctl clear_password {username}表示清除指定用户的密码执行此操作后的用户，将不能用密码登录，但是可能通过已经配置的SASL EXTERNAL的方式登录。 rabbitmqctl authenticate_user {username} {password}表示指引RabbitMQ broker认证该用户和密码 rabbitmqctl set_user_tags {username} {tag …}表示设置用户的角色，｛tag｝可以是零个，一个，或者是多个。并且已经存在的tag也将会被移除。rabbitmqctl set_user_tags tonyg administrator 该命令表示指示RabbitMQ broker确保用户tonyg为一个管理员角色。上述命令在用户通过AMQP方式登录时，不会有任何影响；但是如果通过其他方式，例如管理插件方式登录时，就可以去管理用户、vhost 和权限。 rabbitmqctl list_users作用:查看用户信息结果如下Listing users ...guest [administrator]...done. rabbitmqctl add_user {username} {password}创建新用户例子:rabbitmqctl add_user jshan 123456结果如下，表示创建成功Creating user &quot;jshan&quot; ......done. 再次查询结果如下：Listing users ...guest [administrator]jshan []...done. 上述结果中，第一列表示用户名，第二列表示用户角色 rabbitmqctl set_user_tags {username} {role}作用:为用户设置用户角色例子:rabbitmqctl set_user_tags jshan monitoring结果如下，表示设置成功Setting tags for user &quot;jshan&quot; to [monitoring] ......done. 说明一下，执行该命令之后，会先删除该用户已有的角色，然后添加新的角色，可以填写多个角色，如果想删除某个用户的所有角色，可以设置如下：rabbitmqctl set_user_tags {username} rabbitmqctl change_password {username} {newpassword}作用:修改用户密码例子:rabbitmqctl change_password jshan 123结果如下，表示修改成功Changing password for user &quot;jshan&quot; ......done. rabbitmqctl clear_password {username}作用:清除用户密码例子:rabbitmqctl clear_password jshan结果如下，表示清除成功Clearing password for user &quot;jshan&quot; ......done.说明一下，执行该命令之后，用户无法对该用户使用密码登录 rabbitmq delete_user jshan删除用户 rabbitmqctl delete_user &#123;username&#125;结果如下，表示删除成功Deleting user &quot;jshan&quot; ......done. Access control(访问控制)注意，rabbitmqctl 管理 RabbitMQ 的内部用户数据库，所有其他后台需要认证的用户的权限对于rabbitmqctl将不可见。 rabbitmqctl add_vhost {vhost}｛vhost｝ 表示待创建的虚拟主机项的名称 rabbitmqctl delete_vhost {vhost}表示删除一个vhost。删除一个vhost将会删除该vhost的所有exchange、queue、binding、用户权限、参数和策略。 rabbitmqctl list_vhosts {vhostinfoitem …}表示列出所有的vhost。其中 &#123;vhostinfoitem&#125; 表示要展示的vhost的字段信息，展示的结果将按照 &#123;vhostinfoitem&#125; 指定的字段顺序展示。这些字段包括： name（名称） 和 tracing （是否为此vhost启动跟踪）。如果没有指定具体的字段项，那么将展示vhost的名称。 rabbitmqctl set_permissions [-p vhost] {user} {conf} {write} {read}表示设置用户权限。 &#123;vhost&#125; 表示待授权用户访问的vhost名称，默认为 &quot;/&quot;； &#123;user&#125; 表示待授权反问特定vhost的用户名称； &#123;conf&#125;表示待授权用户的配置权限，是一个匹配资源名称的正则表达式； &#123;write&#125; 表示待授权用户的写权限，是一个匹配资源名称的正则表达式； &#123;read&#125;表示待授权用户的读权限，是一个资源名称的正则表达式。rabbitmqctl set_permissions -p myvhost tonyg &quot;^tonyg-.*&quot; &quot;.*&quot; &quot;.*&quot;例如上面例子，表示授权给用户 &quot;tonyg&quot; 在vhost为 `myvhost` 下有资源名称以 &quot;tonyg-&quot; 开头的 配置权限；所有资源的写权限和读权限。 rabbitmqctl clear_permissions [-p vhost] {username}表示设置用户拒绝访问指定指定的vhost，vhost默认值为 &quot;/&quot; rabbitmqctl list_permissions [-p vhost]表示列出具有权限访问指定vhost的所有用户、对vhost中的资源具有的操作权限。默认vhost为 &quot;/&quot;。注意，空字符串表示没有任何权限。 rabbitmqctl list_user_permissions {username}表示列出指定用户的权限vhost，和在该vhost上的资源可操作权限。 Parameter Management(参数管理)RabbitMQ的一些特性（例如联合插件）是被动态的、集群范围内的参数控制。有两类参数：属于vhost的参数和全局参数。一个属于vhost的参数由三部分组成： 组件名称，参数名称和值。其中组件名称和名称是字符串，值是一个Erlang项。一个全局参数由两部分组成： 参数名称和值。其中名称是字符串，值是一个Erlang项。参数可以被设置，删除，列出。参数的具体设置方法如下： rabbitmqctl set_parameter [-p vhost] {component_name} {name} {value}设置参数，｛component_name｝表示待设置参数的组件名称，&#123;name&#125; 表示待设置的参数名称，｛value｝表示待设置的参数值，是一个JSON项，在多数shell中，你很有可能要应用该值 rabbitmqctl set_parameter federation local_username ‘“guest”‘上述例子，表示设置默认vhost即 &quot;/&quot; 的 federation 组件的参数 local_username 的值设置为JSON项 &quot;guest&quot; rabbitmqctl clear_parameter [-p vhost] {component_name} {key}表示清理一个参数，｛component_name｝表示待清理的组件名称，｛key｝表示待清理的参数名称rabbitmqctl clear_parameter federation local_username上述例子表示清理默认vhost上的组件 federation 的参数 local_username rabbitmqctl list_parameters [-p vhost]表示列举出指定的vhost上的所有参数 rabbitmqctl set_global_parameter {name} {value}设置一个全局运行时的变量，有些类似于 set_parameter ，但是此 key-value 对并不绑定于vhost。 rabbitmqctl set_global_parameter mqtt_default_vhosts &apos;&#123;&quot;O=client,CN=guest&quot;:&quot;/&quot;&#125;&apos;上述例子，设置一个全局运行时的参数 mqtt_default_vhosts 的值为一个JSON项， &#123;&quot;O=client,CN=guest&quot;:&quot;/&quot;&#125; rabbitmqctl clear_global_parameter {name}清除一个全局运行时参数，类似于 clear_parameter，但是此 key-value 对并不绑定于vhost。rabbitmqctl clear_global_parameter mqtt_default_vhosts上述例子，清除一个全局运行时参数 mqtt_default_vhosts rabbitmqctl list_global_parameters列出所有的全局运行时参数，类似于 list_parameters，但是该命令不绑定于任何vhost Policy Management(策略管理)rabbitmqctl set_policy [-p vhost] [–priority priority] [–apply-to apply-to] {name} {pattern} {definition}&#123;name&#125; 表示策略名称；&#123;pattern&#125; 表示当匹配到给定资源的正则表达式，使的该策略得以应用； &#123;definition&#125; 表示策略的定义，作为一个JSON项，在多数shell中，你很可能需要去应用它&#123;priority&#125; 表示策略的优先级的整数，数据越大表示优先级越高，默认值为0&#123;apply_to&#125; 表示策略应该应用的类型： queues/exchange/all，默认值是 all rabbitmqctl clear_policy [-p vhost] {name}表示清理一个策略。 &#123;name&#125; 表示待清理的策略名称 rabbitmqctl list_policies [-p vhost]表示列举出给定的vhost的所有策略信息 Server Status(服务状态)rabbitmqctl list_queues [-p vhost] [[–offline] | [–online] | [–local]] [queueinfoitem …]返回队列的详细信息。如果 &quot;-p&quot; 标志不存在，那么将返回默认虚拟主机的队列详细信息。&quot;-p&quot; 可以用来覆盖默认vhost。可以使用一下互斥选项之一，通过其状态或者位置过滤显示的队列。[--offline] 表示仅仅列出当前不可用的持久队列（更具体地说，他们的主节点不是）[--online] 表示列出当前可用的队列（他们的主节点是）[--local] 表示仅仅列出那些主程序在当前节点上的队列queueinfoitem参数用于指示要包括在结果中的哪些队列信息项。结果中的列顺序将与参数的顺序相匹配。queueinfoitem可以从以下列表中获取任何值：name 表示队列的名称durable 表示服务器重启之后，队列是否存活auto_delete 表示不再使用的队列是否自动被删除arguments 表示队列的参数policy 表示应用在队列中的策略名称pid 表示和队列相关联的Erlang进程的IDowner_pid 表示作为队列的排他所有者的连接的Erlang进程的ID，如果队列是非排他，则为空exclusive 表示队列是否是排他的，有 owner_pid 返回 True，否则返回 Falseexclusive_consumer_pid 表示排他消费者订阅该队列的频道的Erlang进程的ID，如果没有独家消费者，则为空exclusive_consumer_tag 表示订阅该队列的排他消费者的消费tag。如果没有排他消费者，则为空messages_ready 表示准备被发送到客户端的消息数量messages_unacknowledged 表示已经被发送到客户端但是还没有被确认的消息数量messages 表示准备发送和没有被确认的消息数量总和（队列深度）messages_ready_ram 表示驻留在 ram 里的 messages_ready 的消息数量messages_unacknowledged_ram 表示驻留在 ram 里的 messages_unacknowledged 的消息数量messages_ram 表示驻留在 ram 里的消息总数messages_persistent 表示队列中持久消息的总数（对于临时队列，总是为0）message_bytes 表示在队列中所有消息body的大小，这并不包括消息属性（包括header）或者任何开销message_bytes_ready 表示类似于 messge_bytes 但仅仅计算那些将发送到客户端的消息message_bytes_unacknowledged 表示类似于 message_bytes 但仅仅计算那些已经发送到客户还没有确认的消息message_bytes_ram 表示类似于 message_bytes 但仅仅计算那些驻留在ram中的消息message_bytes_persistent 表示类似于 message_bytes 但仅仅计算那些持久消息head_message_timestamp 表示队列中第一个消息的时间戳属性（如果存在）。只有处在 paged-in 状态的消息才存在时间戳。disk_reads 表示该队列自start起，从磁盘读取消息的次数总和disk_writes 表示该队列自start起，被写入磁盘消息的次数总和consumers 表示consumer的数量consumer_utilisation 表示队列能够立即将消息传递给消费者的时间分数（0.0 ~ 1.0之间），如果消费者受到网络拥塞或者预取计数的限制，该值可能小于1.0memory 表示和该队列相关联的Erlang进程消耗的内存字节数，包括stack/heap/内部数据结构slave_pids 表示该队列目前的slave的ID号（如果该队列被镜像的话）synchronised_slave_pids 表示如果队列被镜像，给出与主队列同步的当前slave的ID号，即可以从主队列接管而不丢失消息的slave的IDstate 表示队列的状态，一般是 &quot;running&quot;； 如果队列正在同步，也可能是 &quot;&#123;syncing, MsgCount&#125;&quot;； 如果队列所处的节点当前down了，队列显示的状态为 &quot;down&quot;如果没有指定queueinfoitem，那么将显示队列的名称（name）和深度（messages） rabbitmqctl list_exchanges [-p vhost] [exchangeinfoitem …]返回交换器的详细信息。如果 &quot;-p&quot; 标志不存在，那么将返回默认虚拟主机的交换器详细信息。&quot;-p&quot; 可以用来覆盖默认vhost。exchangeinfoitem参数用于指示要包括在结果中的哪些交换器信息项。结果中的列顺序将与参数的顺序相匹配。exchangeinfoitem可以从以下列表中获取任何值：name 表示交换器的名称type 表示交换器类型（例如： direct/topic/fanout/headers）durable 表示服务器重启之后，交换器是否存活auto_delete 表示交换器不再使用时，是否被自动删除internal 表示交换器是否是内部的，例如不能被客户端直接发布arguments 表示交换器的参数policy 表示引用在该交换器上的策略名称如果没有指定任何 exchangeinfoitem，那么该命令将显示交换器的名称（name）和类型（type） rabbitmqctl list_bindings [-p vhost] [bindinginfoitem …]返回绑定的详细信息。如果 &quot;-p&quot; 标志不存在，那么将返回默认虚拟主机的绑定详细信息。&quot;-p&quot; 可以用来覆盖默认vhost。bindinginfoitem参数用于指示要包括在结果中的哪些绑定信息项。结果中的列顺序将与参数的顺序相匹配。bindinginfoitem可以从以下列表中获取任何值：source_name 表示绑定附加到的消息源的名称source_kind 表示绑定附加到的消息源的类型，目前通常交换器destination_name 表示附加绑定到的消息目的地的名称destination_kind 表示附加绑定到的消息目的地的类型routing_key 表示绑定的routing keyarguments 表示绑定的参数如果没有指定任何的 bindinginfoitem ，那么将展示上述所有的参数rabbitmqctl list_bindings -p /myvhost exchange_name queue_name上述命令，表示展示在 /myvhost 虚拟主机中的绑定的exchange名称和queue名称 rabbitmqctl list_connections [connectioninfoitem …]返回TCP/IP连接统计信息connectioninfoitem 参数用于指示要包括在结果中的哪些连接信息项，结果中的列顺序将与参数的顺序相匹配。connectioninfoitem可以从以下列表中获取任何值：pid 表示与该connection相关联的Erlang进程的id号name 表示该连接的可读性名称port 表示服务端口host 表示通过反向DNS获取的服务器主机名，如果反向DNS失败或未启用，则为其IP地址peer_port 表示对等端口peer_host 表示通过反向DNS获取的对等主机名，如果反向DNS失败或未启用，则为其IP地址ssl 表示该连接是否使用SSL保护的bool值ssl_protocal 表示SSL协议(例如： tlsv1)ssl_key_exchange 表示SSL关键交换器算法（例如： rsa）ssl_cipher 表示SSL密码算法（例如： aes_256_cbc）ssl_hash 表示SSL哈希函数（例如： sha）peer_cert_issuer 表示对等体的SSL证书的颁发者，以RFC4514形式出现peer_cert_validity 表示对等体的SSL证书的有效期限state 表示连接状态（例如： starting/tuning/opening/running/flow/blocking/blocked/closing/closed）channels 表示正在使用连接的通道数量protocol 表示正在使用的AMQP的版本号。注意，如果一个客户端需要一个AMQP 0-9 连接，我们将其作为 AMQP 0-9-1auth_mechanism 表示使用SASL认证机制，如PLANuser 表示和该连接相关联的用户名vhost 表示vhost名称timeout 表示连接超时/协商心跳间隔，单位为秒frame_max 表示最大的frame大小（byte）channel_max 表示该连接上通道的最大数量client_properties 表示在连接建立期间，有客户端传送的消息属性recv_oct 表示接受到的八位字节recv_cnt 表示接受到的包send_oct 表示发送的八位字节send_cnt 表示发送的包send_pend 表示发送的队列大小connected_at 表示该连接被建立的日期和时间的时间戳格式如果没有指定任何connectioninfoitem，那么将展示：user/peer_host/peer_port/流量控制和内存块状态之后的时间 rabbitmqctl list_channels [channelinfoitem …]返回所有当前的通道的信息，通道即一个执行大多数AMQP命令的逻辑容器。这包括由普通AMQP连接的一部分通道、由各种插件和其他扩展程序创建的通道。channelinfoitem 参数用于指示要包括在结果中的哪些连接信息项，结果中的列顺序将与参数的顺序相匹配。channelinfoitem 可以从以下列表中获取任何值：pid 表示与该连接相关联的Erlang程序的ID号connection 表示与通道所属连接相关联的Erlang进程的ID号name 表示通道的可读性名称number 表示通道的号码，在连接中唯一表示它user 表示和该通道相关联的用户名vhost 表示通道操作所在的vhosttransactional 表示通道是否处于事务模式，返回 true，否则返回 falseconfirm 表示通道是否处于确认模式，返回 true, 否则返回 falseconsumer_count 表示通过通道检索消息的逻辑AMQP消费者数量messages_unacknowledged 表示通过通道发送过但还没收到反馈的消息的数量messages_uncommitted 表示尚未提交的事务中接受的消息数acks_uncommitted 表示尚未提交的事务中接受的确认数messages_unconfirmed 表示尚未确认已发布的消息数量。在不处于确认模式中的通道上，该值为0prefetch_count 表示新消费者的QoS预取限制，如果没有限制则为0global_prefetch_count 表示整个通道的QoS预取限制，如果没有限制则为0如果没有指定任何 channelinfoitem 项，那么将展示 pid/user/consumer_count/messages_unacknowledged rabbitmqctl list_consumers [-p vhost]列出消费者，例如对一个队列的消息流的订阅者。每一行用tab字符分隔：订阅的队列名称、创建和管理订阅的通道id、在通道中唯一标识订阅的消费者tag、消息传输到订阅者之后是否需要确认的bool值、代表预取限制的整数（0表示none）、订阅者的其他参数 rabbitmqctl status展示broker的状态信息，例如在当前Erlang节点上正在运行的应用、RabbitMQ和Erlang版本号、OS名称、内存和文件描述统计信息 rabbitmqctl node_health_checkRabbitMQ节点的健康检查，验证 Rabbit 应用正在运行，list_queues和list_channels返回，警告没有被设置rabbitmqctl node_health_check -n rabbit@stringer上述例子，表示对RabbitMQ节点进行健康检查` rabbitmqctl environment在每个正在运行的应用程序的应用程序环境中，显示每个变量的名称和值 rabbitmqctl report生成服务器状态报告，其中包括用于支持目的的所有服务器状态信息的并置。当伴随支持请求时，输出应该被重定向到一个文件rabbitmqctl report &gt; server_report.txt rabbitmqctl eval {expr}评估一个任务Erlang表达式rabbitmqctl eval &apos;node().&apos;上述例子，将返回 rabbitmqctl 已经连接的节点名称 Miscellaneous(繁杂)rabbitmqctl close_connection {connectionpid} {explanation}&#123;connectionpid&#125; 表示待关闭连接的Erlang进程的ID号&#123;explanation&#125; 表示解释字符串指引broker去关闭与ID为 &#123;connectionid&#125; Erlang进程相关联的连接，作为AMQP连接关闭协议的一部分，它也会向连接的客户端传递 &#123;explanation&#125; 字符串rabbitmqctl close_connection &quot;&lt;rabbit@tanto.4262.0&gt;&quot; &quot;go away&quot;上述例子，表示关闭与ID号为 &quot;&lt;rabbit@tanto.4262.0&gt;&quot; 的Erlang进程相关联的连接，并向连接的客户端传输解释性语句 &quot;go away&quot;。 rabbitmqctl trace_on [-p vhost]vhost 表示要启动跟踪的虚拟机名称。开始跟踪。注意，跟踪状态不是持久的，如果服务重启，它将恢复为关闭 rabbitmqctl trace_off [-p vhost]作用:停止跟踪 rabbitmqctl set_vm_memory_high_watermark {fraction}&#123;fraction&#125; 触发流量控制的新内存阈值分数，大于或等于0的浮点数 rabbitmqctl set_vm_memory_high_watermark absolute {memory_limit}&#123;memory_limit&#125; 触发流量控制的新内存限制，以字节表示，大于或等于0的整数或作为具有存储单元（例如： 512M或者1G），可用的单位有：k/kiB： kibibytes(2^10字节)； M/MiB: mebibytes(2^20字节)； G/GiB: gibibytes(2^30字节)kB: kilobytes(10^3); MB: megabytes(10^6); GB: gigabytes(10^9) rabbitmqctl set_disk_free_limit {disk_limit}&#123;disk_limit&#125; 下限为字节整数或具有存储单元的字符串（参见 vm_memory_high_watermark 命令），例如： 512M或1G，一旦可用磁盘空间达到限制，将会设置磁盘告警 rabbitmqctl set_disk_free_limit mem_relative {fraction}&#123;fraction&#125; 相对于可用RAM的限制，为非负的浮点数。低于1.0的值可能是危险的，应小心使用。 rabbitmqctl encode [–decode] [value] [passphrase] [–list-ciphers] [–list-hashes] [–cipher cipher] [–hash hash] [–iterations iteraions][--decode] 表示解密输入值的标志位。例如: rabbitmqctl encode --decode &apos;&#123;encrypted,&apos;&lt;&lt;&quot;...&quot;&gt;&gt;&#125;&apos; mypassphrase[value] [passphrase] 表示加密和解密的值、密码。 例如： rabbitmqctl encode &apos;&lt;&lt;&quot;guest&quot;&gt;&gt;&apos; mypassphrase 。例如： rabbitmqctl encode --decode &apos;&#123;encrypted,&apos;&lt;&lt;&quot;...&quot;&gt;&gt;&#125;&apos; mypassphrase [--list-ciphers] 表示列出支持的密码标志位。例如： rabbitmqctl encode --list-ciphers。[--list-hashes] 表示列出支持的哈希算法标志位。例如： rabbitmqctl encode --list-hashes。[--cipher cipher] [--hash hash] [--iterations iterations] 表示用于指定加密设置的选项，它们可以独立使用。例如： rabbitmqctl encode --cipher blowfish_cfb64 --hash sha256 --iterations 1000 &apos;&lt;&lt;&quot;guest&quot;&gt;&gt;&apos; mypassphrase","categories":[],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"https://youxia999.github.io/tags/rabbitmq/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【java应用开发脚手架系列】spring boot amqp","slug":"spring-boot-1x-boot-amqp","date":"2017-12-20T13:50:39.000Z","updated":"2019-12-27T11:59:45.653Z","comments":true,"path":"2017/12/20/spring-boot-1x-boot-amqp/","link":"","permalink":"https://youxia999.github.io/2017/12/20/spring-boot-1x-boot-amqp/","excerpt":"","text":"关于spring boot 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"【java语言开发脚手架系列】spring webmvc","slug":"spring-boot-1x-boot-webmvc","date":"2017-12-13T13:52:03.000Z","updated":"2019-12-27T11:59:45.680Z","comments":true,"path":"2017/12/13/spring-boot-1x-boot-webmvc/","link":"","permalink":"https://youxia999.github.io/2017/12/13/spring-boot-1x-boot-webmvc/","excerpt":"","text":"关于spring boot 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"【java语言开发脚手架系列】spring-boot-data-redis","slug":"spring-boot-1x-boot-data-redis","date":"2017-12-06T09:14:49.000Z","updated":"2019-12-27T11:59:45.673Z","comments":true,"path":"2017/12/06/spring-boot-1x-boot-data-redis/","link":"","permalink":"https://youxia999.github.io/2017/12/06/spring-boot-1x-boot-data-redis/","excerpt":"","text":"概述https://docs.spring.io/spring-data/redis/docs/1.8.9.RELEASE/reference/html/https://www.cnblogs.com/EasonJim/p/7805665.html PS:如果能看懂英文版的官方文档，就不建议继续往下阅读浪费时间，谢谢。 关于spring boot/cloud 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"【微服务架构java解决方案】spring-cloud概述","slug":"spring-cloud-1x-talka-bout-cloud-14","date":"2017-11-30T02:45:49.000Z","updated":"2020-02-06T04:14:19.653Z","comments":true,"path":"2017/11/30/spring-cloud-1x-talka-bout-cloud-14/","link":"","permalink":"https://youxia999.github.io/2017/11/30/spring-cloud-1x-talka-bout-cloud-14/","excerpt":"","text":"概述参考资料https://spring.io/blog/2017/11/27/spring-cloud-edgware-release-available 关于spring boot/cloud 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。 关于spring cloud springcloud非商业公司部分spring-cloud-commonSpring Cloud Commons是一组在不同Spring Cloud实现中使用的抽象和公共类（例如，Spring Cloud Netflix与Spring Cloud Consul）不会单独拿出来分享。 Spring-Cloud-GatewaySpring-Cloud-Gateway提供了一个用于在Spring MVC之上构建API网关的库。后面找机会展开学习和分享spring-cloud-gateway。 Spring-Cloud-StreamSpring Cloud Stream是基于spring integration框架的一个用于构建与共享消息传递系统相连的高度可扩展的事件驱动的微服务的框架。后面找机会展开学习和分享spring-cloud-stream。 Spring-Cloud-SleuthSpring Cloud Sleuth为Spring Cloud大量借用Dapper，和HTrace实施分布式跟踪的一套解决方案。后面找机会展开学习和分享spring-cloud-sleuth。 spring-cloud商业公司部分spring-cloud-netfixSpring Cloud Netflix通过自动配置、Spring环境、其他Spring编程模型等技术为Spring Boot应用程序提供Netflix OSS集成。这家公司很良心，后面找机会展开学习和分享spring-cloud-netfix。 spring-cloud-awsspring-cloud-aws是为更好的集成aws而提供的一套spring框架。没有机会上aws云，就不入坑了。 附录","categories":[],"tags":[{"name":"spring cloud解决方案","slug":"spring-cloud解决方案","permalink":"https://youxia999.github.io/tags/spring-cloud解决方案/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【java应用开发脚手架系列】spring boot将rpc服务的业务逻辑暴露成http服务","slug":"rpc_service_expose_http","date":"2017-10-28T11:41:34.000Z","updated":"2019-12-27T11:59:45.636Z","comments":true,"path":"2017/10/28/rpc_service_expose_http/","link":"","permalink":"https://youxia999.github.io/2017/10/28/rpc_service_expose_http/","excerpt":"","text":"本文参考资料https://juejin.im/post/5908a811da2f60005d188aee 定义数据模型请求协议请求url：http://url/接口/方法请求参数：param:&#123;json串&#125; 服务端对请求的封装public class HttpRequest &#123; private String param; private String service; private String method; set/get方法&#125; 服务端的通用应答public class HttpResponse implements Serializable &#123; private boolean success; private String code; private String description; set/get方法&#125; 具体实现application.yml配置conf: package: com.xiaogang.dubbo.service``` ## 配置类 @Componentpublic class HttpProviderConf { @Value(&quot;${conf.package}&quot;) private String usePackageString; private List&lt;String&gt; userPackage; public List&lt;String&gt; getUserPackage() { return userPackage; } public void setUserPackage(List&lt;String&gt; userPackage) { this.userPackage =Arrays.asList(StringUtils.split(usePackageString,&quot;,&quot;)); } }## 业务类 import com.alibaba.fastjson.JSON;import com.xiaogang.config.HttpProviderConf;import com.xiaogang.config.HttpRequest;import com.xiaogang.config.HttpResponse;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.BeansException;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.stereotype.Controller;import org.springframework.util.CollectionUtils;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.ResponseBody; import javax.servlet.http.HttpServletRequest;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;import java.net.InetAddress;import java.net.UnknownHostException;import java.util.HashMap;import java.util.Map; @Controller@RequestMapping(“/ThriftAPI”)public class ThriftServiceController implements ApplicationContextAware { private final static Logger logger= LoggerFactory.getLogger(ThriftServiceController.class); private ApplicationContext applicationContext; @Autowired private HttpProviderConf httpProviderConf; private final Map&lt;String, Class&lt;?&gt;&gt; cacheMap = new HashMap&lt;String, Class&lt;?&gt;&gt;(); @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext=applicationContext; } @ResponseBody @RequestMapping(value = &quot;/{service}/{method}&quot;,method = RequestMethod.POST) public String api(HttpServletRequest httpServletRequest, HttpRequest httpRequest, @PathVariable String service, @PathVariable String method){ logger.info(&quot;ip-{},http-request {}&quot;,getIp(httpServletRequest), JSON.toJSONString(httpRequest)); String invoke= invoke(httpRequest,service,method); logger.info(&quot;callback {}&quot;,invoke); return invoke; } private String invoke(HttpRequest httpRequest, String service, String method) { httpRequest.setMethod(method); httpRequest.setService(service); HttpResponse httpResponse=new HttpResponse(); if (!CollectionUtils.isEmpty(httpProviderConf.getUserPackage())){ boolean isPac=false; for(String pac:httpProviderConf.getUserPackage()){ if(service.startsWith(pac)){ isPac=true; break; } } if (!isPac){ logger.error(&quot;service is not correct service is {}&quot;,service); httpResponse.setCode(&quot;2&quot;); httpResponse.setSuccess(false); httpResponse.setDescription(&quot;service is not conrect&quot;); } } Class&lt;?&gt; serviceClass=cacheMap.get(service); if (serviceClass==null){ try { serviceClass=Class.forName(service); cacheMap.put(service,serviceClass); } catch (ClassNotFoundException e) { e.printStackTrace(); } } if (method == null){ logger.error(&quot;method is not correct,method=&quot;+method); httpResponse.setCode(&quot;2&quot;); httpResponse.setSuccess(false); httpResponse.setDescription(&quot;method is not correct,method=&quot;+method); } Method[] methodArray=serviceClass.getMethods(); Method targetMethod=null; Boolean isMethod=false; for (Method method1:methodArray){ if (method1.getName().equals(method)){ targetMethod=method1; isMethod=true; break; } } if (!isMethod){ logger.error(&quot;method is not correct,method=&quot;+method); httpResponse.setCode(&quot;3&quot;); httpResponse.setSuccess(false); httpResponse.setDescription(&quot;method is not correct,method=&quot;+method); return JSON.toJSONString(httpResponse); } Object bean=this.applicationContext.getBean(serviceClass); Object result=null; if (targetMethod!=null){ Class&lt;?&gt;[] parameterTypes=targetMethod.getParameterTypes(); try{ if (parameterTypes.length==0){ result=targetMethod.invoke(bean); return JSON.toJSONString(result); }else if(parameterTypes.length==1){ Object json=JSON.parseObject(httpRequest.getParam(),parameterTypes[0]); result=targetMethod.invoke(bean,json); return JSON.toJSONString(result); }else { logger.error(&quot;can only have one parameter&quot;); httpResponse.setCode(&quot;2&quot;); httpResponse.setSuccess(false); httpResponse.setDescription(&quot;can only have one parameter&quot;); } } catch (InvocationTargetException inException){ logger.error(&quot;{}&quot;,inException); }catch(IllegalAccessException illException){ logger.error(&quot;{}&quot;,illException); } return JSON.toJSONString(httpResponse); } return null; } private Object getIp(HttpServletRequest httpServletRequest) { if (httpServletRequest == null) return null; String s = httpServletRequest.getHeader(&quot;X-Forwarded-For&quot;); if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) { s = httpServletRequest.getHeader(&quot;Proxy-Client-IP&quot;); } if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) { s = httpServletRequest.getHeader(&quot;WL-Proxy-Client-IP&quot;); } if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) { s = httpServletRequest.getHeader(&quot;HTTP_CLIENT_IP&quot;); } if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) { s = httpServletRequest.getHeader(&quot;HTTP_X_FORWARDED_FOR&quot;); } if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) { s = httpServletRequest.getRemoteAddr(); } if (&quot;127.0.0.1&quot;.equals(s) || &quot;0:0:0:0:0:0:0:1&quot;.equals(s)) try { s = InetAddress.getLocalHost().getHostAddress(); } catch (UnknownHostException unknownhostexception) { return &quot;&quot;; } return s; } }`","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"centos7二进制安装go sdk和java sdk","slug":"centos7-binary-install-software","date":"2017-10-25T11:45:35.000Z","updated":"2020-02-06T04:15:12.675Z","comments":true,"path":"2017/10/25/centos7-binary-install-software/","link":"","permalink":"https://youxia999.github.io/2017/10/25/centos7-binary-install-software/","excerpt":"","text":"安装go sdk下载二进制包和解压wget https://dl.google.com/go/go1.8.4.linux-amd64.tar.gz (访问不了，可能要搭梯子)tar zxvf go1.8.4.linux-amd64.tar.gz -C /usr/local/ 配置profilemkdir -p /data/server/go_program vim /etc/profileGOPATH=/data/server/go_programGOROOT=/usr/local/goGOBIN=/usr/local/go/binexport PATH=$PATH:$GOBIN 然后执行source /etc/profile 安装java sdk注册一个oracle账号直接到官网上注册一个。 下载二进制包wget https://download.oracle.com/otn/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz?AuthParam=1556950816_9e12f84633d6ef39d26f9c17c827a965mv jdk-8u102-linux-x64.tar.gz?AuthParam=1556950816_9e12f84633d6ef39d26f9c17c827a965 jdk-8u102-linux-x64.tar.gztar zxvf jdk-8u102-linux-x64.tar.gz -C /usr/local/ 解压和配置profileexport JAVA_HOME=/usr/local/jdk1.8.0_161 export JRE_HOME=$JAVA_HOME/jre export CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin 然后执行source /etc/profile","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【java语言开发脚手架系列】spring boot核心类和扩展点(web)","slug":"spring-boot-1x-boot-core-2","date":"2017-10-22T12:46:32.000Z","updated":"2019-12-27T11:59:45.660Z","comments":true,"path":"2017/10/22/spring-boot-1x-boot-core-2/","link":"","permalink":"https://youxia999.github.io/2017/10/22/spring-boot-1x-boot-core-2/","excerpt":"","text":"springboot1.5.x web程序的核心类和扩展点关于spring boot/cloud 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。 web核心类和扩展点 附录","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"【java应用开发脚手架系列】spring boot核心类和扩展点","slug":"spring-boot-1x-boot-core","date":"2017-10-22T12:46:32.000Z","updated":"2019-12-27T11:59:45.666Z","comments":true,"path":"2017/10/22/spring-boot-1x-boot-core/","link":"","permalink":"https://youxia999.github.io/2017/10/22/spring-boot-1x-boot-core/","excerpt":"","text":"概述本文参考资料官方blog: https://spring.io/blog/2017/10/17/spring-boot-1-5-8-available-now官方文档: https://docs.spring.io/spring-boot/docs/1.5.8.RELEASE/reference/pdf/spring-boot-reference.pdf 温馨提示如果能看到英文版的官方文档，就不建议继续往下阅读浪费时间，谢谢。 本文背景从2016年接触，2017年全面升级到spring boot，差不多也有一段时间了。spring boot也升级了多个版本。 关于spring boot系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。 springboot1.5.x（非web）的核心类写一个非web的demo程序，并调试下程序，得出如下一张图(一图胜千言)。总结起来就两条主线: springapplication构造函数springboot构造函数，有扩展点：applicationListener、applicationContextinitializer。Application启动运行过程中各种事件的监听器，如SpringApplicationEvent、ApplicationContextEvent、Environment相关Event等诸多事件。很多非官方的框架都是基于这两个扩展点做的开发。 springapplication.runrun第一步(实例化和初始化bean)这一步有扩展点：initializingbean、BeanPostProcessor、 run第二步(触发监听)springapplication构造函数初始化的applicationlistener、applicationContextinitializer有活干了。 run第三步(自动装配)这一步有扩展点：EnableAutoConfiguration springboot 扩展点initializingbean可以在bean初始化完成，所有属性设置完成后执行特定逻辑，例如对自动装配对属性进行验证等等,将mysql的数据导入到redis中。 BeanFactoryPostProcessor是beanFactory后置处理器，支持在bean factory标准初始化完成后，对bean factory进行一些额外处理。在讲context初始化流程时介绍过，这时所有的bean的描述信息已经加载完毕，但是还没有进行bean初始化。 BeanPostProcessor提供了在bean初始化之前和之后插入自定义逻辑的能力。与BeanFactoryPostProcessor的区别是处理的对象不同，BeanFactoryPostProcessor是对beanfactory进行处理，BeanPostProcessor是对bean进行处理。 扩展点applicationListener很重要的一个扩展点。监听各种事件扩展这个类，可以关闭banner的打印。 applicationContextinitializer这个类一般做业务开发时，很少有场景会用到。 EnableAutoConfiguration启用自动配置，加载所有的spring.factories配置文件的配置类。 附录","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"【java应用开发脚手架系列】spring boot阶段性总结","slug":"spring-boot-1x-talk-about_boot","date":"2017-10-21T03:28:53.000Z","updated":"2019-12-27T11:59:45.696Z","comments":true,"path":"2017/10/21/spring-boot-1x-talk-about_boot/","link":"","permalink":"https://youxia999.github.io/2017/10/21/spring-boot-1x-talk-about_boot/","excerpt":"","text":"概述本文背景从2016年接触，2017年全面升级到spring boot，差不多也有一段时间了。spring boot也升级了多个版本。 准备资料官方blog: https://spring.io/blog/2017/10/17/spring-boot-1-5-8-available-now官方文档: https://docs.spring.io/spring-boot/docs/1.5.8.RELEASE/reference/pdf/spring-boot-reference.pdf 关于spring boot 1.X系列spring.io在spring boot和springcloud推出之前，开源了很多的框架（见下文附录），但是除了spring framework自身，影响力都不够。直到spring boot、spring cloud系列框架推出，才成了spring框架的扩展中最有影响力的两个(或者说集大成者)。Spring Boot的设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通俗点理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了很多的jar包，比如散落在各处的spring框架：spring amqp、spring data redis client、spring web service，进而集大成。Spring Cloud为开发人员提供了快速构建分布式系统中一些常见模式的工具，例如配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁定，领导选举，分布式会话，集群状态。从技术实现上来说，也是集成开源届的各种方案比如 client、rabbitmq client、eureka client。 spring boot 1.5.X版本版本特性我们看下官方文档中的spring boot1.5.X版本的特性： springApplication（这个肯定得用） Externalized Configuration（这个肯定得用） profile（这个不怎么用） logging（这个不怎么用） develop web application（这个肯定得用） Security（这个没有怎么用） Working with SQL databases（这个没有怎么用） Working with NoSQL technologies（这个没怎么用） caching（这个肯定得用） messaging（这个肯定得用） Calling REST services（这个没用） Validation（这个没用） Sending email（这个没用） Distributed Transactions with JTA（这个没用） Spring Integration（这个肯定得用） Spring Session（这个没用） Monitoring and management over JMX（这个没用） Testing（这个肯定得用） websocket（这个没用） web services（这个没用） Spring Boot Actuator: Production-ready features（这个没用，其实要用了，就没啥自定义开发啥事了） 源码预览 附录","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"【转载】什么是云原生","slug":"cloud_native_concept","date":"2017-10-09T13:26:11.000Z","updated":"2020-02-06T04:11:51.201Z","comments":true,"path":"2017/10/09/cloud_native_concept/","link":"","permalink":"https://youxia999.github.io/2017/10/09/cloud_native_concept/","excerpt":"","text":"","categories":[],"tags":[{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"centos7下自定义脚本开机自启动","slug":"centos7-self-shell-reboot","date":"2017-08-30T04:31:17.000Z","updated":"2020-02-06T04:15:12.793Z","comments":true,"path":"2017/08/30/centos7-self-shell-reboot/","link":"","permalink":"https://youxia999.github.io/2017/08/30/centos7-self-shell-reboot/","excerpt":"","text":"备注本命令可能很久才会用一次 修改/etc/rc.local#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run &apos;chmod +x /etc/rc.d/rc.local&apos; to ensure# that this script will be executed during boot.touch /var/lock/subsys/local/bin/su - zookeeper -c &quot;/data/zookeeper/bin/zkServer.sh start &amp;&quot; # 启动zookeeper服务docker ps -a|awk &apos;&#123;print $1&#125;&apos;|xargs docker start #重启docker里面的服务/bin/su - bbs_abc -c &quot;/data/server/bbs/abc/bin/server.sh restart &amp;&quot; 给/etc/rc.d/rc.local执行权限chmod a+x /etc/rc.d/rc.local reboot查看效果服务已经启动了。","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"go语言运行时的工作过程","slug":"go_run_principle","date":"2017-08-21T10:41:34.000Z","updated":"2019-12-27T11:59:45.509Z","comments":true,"path":"2017/08/21/go_run_principle/","link":"","permalink":"https://youxia999.github.io/2017/08/21/go_run_principle/","excerpt":"","text":"","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"golang","slug":"golang","permalink":"https://youxia999.github.io/tags/golang/"}]},{"title":"玩转centos7之程序安装与启动(rpm、yum、systemd)","slug":"centos7-install-software","date":"2017-08-05T13:11:16.000Z","updated":"2020-02-06T04:15:12.752Z","comments":true,"path":"2017/08/05/centos7-install-software/","link":"","permalink":"https://youxia999.github.io/2017/08/05/centos7-install-software/","excerpt":"","text":"提示进行下面的操作前，最好修改下dns(查找速度)：vim /etc/resolv.conf升级curl、nss：yum update nss curl 程序安装包技术rpmrpm包与源码包的对比 rpm help命令 本机已安装的包 程序安装关键技术yumrepo概念repo文件是Fedora中yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的细节内容。例如我们将从哪里下载需要安装或者升级的软件包，repo文件中的设置内容将被yum读取和应用！更多的源介绍，可以参考: https://youxia999.github.io/2017/08/05/centos7-repo-introduce/ yum介绍yum(Yellow dog Updater, Modified)是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。YUM的工作原理并不复杂，每一个 RPM软件的头（header）里面都会纪录该软件的依赖关系，那么如果可以将该头的内容纪录下来并且进行分析，可以知道每个软件在安装之前需要额外安装 哪些基础软件。也就是说，在服务器上面先以分析工具将所有的RPM档案进行分析，然后将该分析纪录下来，只要在进行安装或升级时先查询该纪录的文件，就可以知道所有相关联的软件。 yum配置文件解析cat /etc/yum.conf[main]cachedir=/var/cache/yum/$basearch/$releaseverkeepcache=0debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release 各个文件存放在/etc/yum.repos.d目录下。 更换yum源一直有一个现实的问题：在国内下载某些国外的软件会很慢。所以，一般会选择aliyun或者163的源仓库。更换步骤： 备份原来的yum源cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 设置aliyun的yum源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 添加EPEL源EPEL (http://fedoraproject.org/wiki/EPEL) 是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。装上 EPEL后，可以像在 Fedora 上一样，可以通过 yum install package-name，安装更多软件。wget -P /etc/yum.repos.d/ http://mirrors.aliyun.com/repo/epel-7.repo 清理缓存并生成新的缓存yum clean all yum makecache 其他的源暂时不需要更新内核，所以，就不用装elrepo源。自此，结束。 yum其他命令输入man yum可以看到参考手册： 程序启动关键技术systemdsystemd的历史Systemd目的是要取代Unix时代以来一直在使用的init系统，兼容SysV和LSB的启动脚本，而且够在进程启动过程中更有效地引导加载服务。Systemd的很多概念来源于苹果Mac OS操作系统上的launchd，不过launchd专用于苹果系统，因此长期未能获得应有的广泛关注。Systemd借鉴了很多launchd的思想。Systemd是Linux系统中最新的初始化系统（init），它主要的设计目标是克服SysV init 固有的缺点，提高系统的启动速度。systemd和ubuntu的upstart 是竞争对手，预计会取代 UpStart，然而在Ubuntu 15.04采用systemd作为默认引导程序，debian8也开始使用systemd。足以看出systemd的流行度。 systemd的特点CentOS 7使用systemd替换了SysV。下面是一些特别要注意的和之前主要版本的RHEL不再兼容的部分。1.systemd对运行级别支持有限为了保存兼容，systemd提供一定数量的target单元，可以直接和运行级别对应，也可以被早期的分布式的运行级别命令支持。不是所有的target都可以被映射到运行级别，在这种情况下，使用runlevel命令有可能会返回一个为N的不知道的运行级别，所以推荐尽量避免在RHEL7中使用runlevel命令。 2.systemd不支持像init脚本那样的个性化命令。除了一些标准命令参数例如：start、stop、status，SysV init脚本可以根据需要支持想要的任何参数，通过参数提供附加的功能，因为SysV init的服务器脚本实际上就是shell脚本，命令参数实际上就是shell子函数。举个例子，RHEL6的iptables服务脚本可以执行panic命令行参数，这个参数可以让系统立即进入紧急模式，丢弃所有的进入和发出的数据包。但是类似这样的命令行参数在systemd中是不支持的，systemd只支持在配置文件中指定命令行参数。 3.systemd不支持和没有从systemd启动的服务通讯当systemd启动服务的时候，他保存进程的主ID以便于追踪，systemctl工具使用进程PID查询和管理服务。相反的，如果用户从命令行启动特定的服务，systemctl命令是没有办法判断这个服务的状态是启动还是运行的。 4.systemd可以只停止运行的服务在RHEL6及之前的版本，当关闭系统的程序启动之后，RHEL6的系统会执行/etc/rc0.d/下所有服务脚本的关闭操作，不管服务是处于运行或者根本没有运行的状态。而systemd可以做到只关闭在运行的服务，这样可以大大节省关机的时间。 5.不能从标准输出设备读到系统服务信息。systemd启动服务的时候，将标准输出信息定向到/dev/null，以免打扰用户。 6.systemd不继承任何上下文环境。systemd不继承任何上下文环境，如用户或者会话的HOME或者PATH的环境变量。每个服务得到的是干净的上下文环境。 7.SysV init脚本依赖性。当systemd启动SysV init脚本，systemd在运行的时候，从LinuxStandardBase(LSB)Linux标准库头文件读取服务的依赖信息并继承。 8.超时机制。为了防止系统被卡住，所有的服务有5分钟的超时机制。 systemd的架构和优缺点Systemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合，违反”keep simple, keep stupid”的Unix 哲学。下图是systemd的架构图（图片来自网络）： systemd的概念模型单元的概念在RHEL7之前，服务管理是分布式的被SysV init或UpStart通过/etc/rc.d/init.d下的脚本管理。这些脚本是经典的Bash脚本，允许管理员控制服务的状态。在RHEL7中，这些脚本被服务单元文件替换。系统初始化需要做的事情非常多。需要启动后台服务，比如启动 SSHD 服务；需要做配置工作，比如挂载文件系统。这个过程中的每一步都被 systemd 抽象为一个配置单元，即 unit。Systemd可以管理所有系统资源。不同的资源统称为 Unit（单位）。可以认为一个服务是一个配置单元；一个挂载点是一个配置单元；一个交换分区的配置是一个配置单元；等等。systemd 将配置单元归纳为以下一些不同的类型。然而，systemd 正在快速发展，新功能不断增加。所以配置单元类型可能在不久的将来继续增加。systemd中有许多单元类型，服务单元文件的扩展名是.service，同脚本的功能相似。例如有查看、启动、停止、重启、启用或者禁止服务的参数。下面说明下，systemd单元文件放置位置：/run/systemd/system #单元运行时创建，这个目录优先于按照目录/etc/systemd/system #系统管理员创建和管理的单元目录，优先级最高/usr/lib/systemd/system/system #默认单元文件安装目录有了对systemd的基本认识后，下面就开始来介绍相关的操作命令。但是注意一点，systemd并不是一个命令，而是一组命令，涉及到系统管理的方方面面。 systemd命令1.查看当前系统的所有Unitsystemctl list-units #列出正在运行的Unitsystemctl list-units --all #列出所有Unit，包括没有找到配置文件的或者启动失败的systemctl list-units --all --state=inactive #列出所有没有运行的Unitsystemctl list-units --failed #列出所有加载失败的Unitsystemctl list-units --type=service #列出所有正在运行的、类型为service的Unit 2.unit.service管理命令systemctl start name.service #启动一个服务systemctl stop name.service #关闭一个服务systemctl restart name.service #重启一个服务systemctl reload name.service #重载一个服务systemctl try-restart name.service #仅当服务运行的时候，重启服务systemctl kill name.service # 杀死一个服务的所有子进程systemctl daemon-reload #重载所有修改过的配置文件systemctl enable name.service #允许服务开机启动systemclt disable name.service #禁止服务开机启动systemctl list-dependencies nginx.service #命令列出一个Unit的所有依赖systemctl show httpd.service #显示某个Unit的所有底层参数systemctl show -p CPUShares httpd.service #显示某个Unit的指定属性的值systemctl set-property httpd.service CPUShares=500 # 设置某个Unit的指定属性 3.电源管理命令（systemctl）systemctl reboot #重启机器systemctl poweroff #关机systemctl suspend #待机systemctl hibernate #休眠systemctl hybrid-sleep #混合休眠模式（同时休眠到硬盘并待机） 4.系统引导性能分析命令（systemd-analyze） 5.systemd的日志服务systemd 自带日志服务 journald，该日志服务的设计初衷是克服现有的 syslog 服务的缺点。比如：syslog不安全，消息的内容无法验证。每一个本地进程都可以声称自己是 Apache PID 4711，而 syslog 也就相信并保存到磁盘上。数据没有严格的格式，非常随意。自动化的日志分析器需要分析人类语言字符串来识别消息。一方面此类分析困难低效；此外日志格式的变化会导致分析代码需要更新甚至重写。Systemd Journal 用二进制格式保存所有日志信息，用户使用 journalctl 命令来查看日志信息。无需自己编写复杂脆弱的字符串分析处理程序。 journalctl #查看所有日志（默认情况下 ，只保存本次启动的日志）journalctl -k #查看内核日志（不显示应用日志）journalctl -b #查看系统本次启动的日志journalctl -b -0journalctl -b -1 # 查看上一次启动的日志（需更改设置）journalctl --since=&quot;2012-10-30 18:17:16&quot; # 查看指定时间的日志journalctl --since &quot;20 min ago&quot; # 查看指定时间的日志journalctl --since yesterday # 查看指定时间的日志journalctl --since &quot;2015-01-10&quot; --until &quot;2015-01-11 03:00&quot; # 查看指定时间的日志journalctl --since 09:00 --until &quot;1 hour ago&quot; # 查看指定时间的日志journalctl -n # 显示尾部的最新10行日志journalctl -n 20 # 显示尾部指定行数的日志journalctl -f # 实时滚动显示最新日志journalctl /usr/lib/systemd/systemd # 查看指定服务的日志journalctl _PID=1 # 查看指定进程的日志journalctl /usr/bin/bash # 查看某个路径的脚本的日志journalctl _UID=33 --since today # 查看指定用户的日志journalctl -u nginx.service # 查看某个 Unit 的日志journalctl -u nginx.service --since todayjournalctl -u nginx.service -f # 实时滚动显示某个 Unit 的最新日志journalctl -u nginx.service -u php-fpm.service --since today # 合并显示多个 Unit 的日志journalctl -p err -b # 查看指定优先级（及其以上级别）的日志，共有8级-0: emerg、1: alert、2: crit、3: err、4: warning、5: notice、6: info、7: debugjournalctl --no-pager # 日志默认分页输出，--no-pager 改为正常的标准输出journalctl -b -u nginx.service -o json # 以JSON格式（单行）输出journalctl -b -u nginx.serviceqq -o json-pretty # 以JSON格式（多行）输出，可读性更好journalctl --disk-usage # 显示日志占据的硬盘空间journalctl --vacuum-size=1G # 指定日志文件占据的最大空间journalctl --vacuum-time=1years # 指定日志文件保存多久 例子(mysqld为例)# systemd service file for MySQL forking server[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/data/mysql/run/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Execute pre and post scripts as rootPermissionsStartOnly=true# Needed to create system tables#ExecStartPre=/home/bin/mysqld_pre_systemd# Start main serviceExecStart=/data/mysql/bin/mysqld --daemonize --pid-file=/data/mysql/run/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementationEnvironmentFile=-/etc/sysconfig/mysq# Sets open_files_limitLimitNOFILE = 65535Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"玩转centos7之程序安装与启动(rpm、yum、systemd)","slug":"centos7-rpm-install-software","date":"2017-08-05T13:11:16.000Z","updated":"2020-02-06T04:15:12.782Z","comments":true,"path":"2017/08/05/centos7-rpm-install-software/","link":"","permalink":"https://youxia999.github.io/2017/08/05/centos7-rpm-install-software/","excerpt":"","text":"提示进行下面的操作前，最好修改下dns(查找速度)：vim /etc/resolv.conf升级curl、nss：yum update nss curl 程序安装包技术rpmrpm介绍见附录。 rpm包与源码包的对比 rpm help命令 本机已安装的包 程序安装关键技术yumrepo概念repo文件是Fedora中yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的细节内容。例如我们将从哪里下载需要安装或者升级的软件包，repo文件中的设置内容将被yum读取和应用！更多的源介绍，可以参考: https://youxia999.github.io/2017/08/05/centos7-repo-introduce/ yum介绍yum(Yellow dog Updater, Modified)是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。YUM的工作原理并不复杂，每一个 RPM软件的头（header）里面都会纪录该软件的依赖关系，那么如果可以将该头的内容纪录下来并且进行分析，可以知道每个软件在安装之前需要额外安装 哪些基础软件。也就是说，在服务器上面先以分析工具将所有的RPM档案进行分析，然后将该分析纪录下来，只要在进行安装或升级时先查询该纪录的文件，就可以知道所有相关联的软件。 yum配置文件解析cat /etc/yum.conf[main]cachedir=/var/cache/yum/$basearch/$releaseverkeepcache=0debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release 各个文件存放在/etc/yum.repos.d目录下。 更换yum源一直有一个现实的问题：在国内下载某些国外的软件会很慢。所以，一般会选择aliyun或者163的源仓库。更换步骤： 备份原来的yum源cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 设置aliyun的yum源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 添加EPEL源EPEL (http://fedoraproject.org/wiki/EPEL) 是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。装上 EPEL后，可以像在 Fedora 上一样，可以通过 yum install package-name，安装更多软件。wget -P /etc/yum.repos.d/ http://mirrors.aliyun.com/repo/epel-7.repo 清理缓存并生成新的缓存yum clean all yum makecache 其他的源暂时不需要更新内核，所以，就不用装elrepo源。自此，结束。 yum其他命令输入man yum可以看到参考手册： 程序启动关键技术systemdsystemd的历史Systemd目的是要取代Unix时代以来一直在使用的init系统，兼容SysV和LSB的启动脚本，而且够在进程启动过程中更有效地引导加载服务。Systemd的很多概念来源于苹果Mac OS操作系统上的launchd，不过launchd专用于苹果系统，因此长期未能获得应有的广泛关注。Systemd借鉴了很多launchd的思想。Systemd是Linux系统中最新的初始化系统（init），它主要的设计目标是克服SysV init 固有的缺点，提高系统的启动速度。systemd和ubuntu的upstart 是竞争对手，预计会取代 UpStart，然而在Ubuntu 15.04采用systemd作为默认引导程序，debian8也开始使用systemd。足以看出systemd的流行度。 systemd的特点CentOS 7使用systemd替换了SysV。下面是一些特别要注意的和之前主要版本的RHEL不再兼容的部分。1.systemd对运行级别支持有限为了保存兼容，systemd提供一定数量的target单元，可以直接和运行级别对应，也可以被早期的分布式的运行级别命令支持。不是所有的target都可以被映射到运行级别，在这种情况下，使用runlevel命令有可能会返回一个为N的不知道的运行级别，所以推荐尽量避免在RHEL7中使用runlevel命令。 2.systemd不支持像init脚本那样的个性化命令。除了一些标准命令参数例如：start、stop、status，SysV init脚本可以根据需要支持想要的任何参数，通过参数提供附加的功能，因为SysV init的服务器脚本实际上就是shell脚本，命令参数实际上就是shell子函数。举个例子，RHEL6的iptables服务脚本可以执行panic命令行参数，这个参数可以让系统立即进入紧急模式，丢弃所有的进入和发出的数据包。但是类似这样的命令行参数在systemd中是不支持的，systemd只支持在配置文件中指定命令行参数。 3.systemd不支持和没有从systemd启动的服务通讯当systemd启动服务的时候，他保存进程的主ID以便于追踪，systemctl工具使用进程PID查询和管理服务。相反的，如果用户从命令行启动特定的服务，systemctl命令是没有办法判断这个服务的状态是启动还是运行的。 4.systemd可以只停止运行的服务在RHEL6及之前的版本，当关闭系统的程序启动之后，RHEL6的系统会执行/etc/rc0.d/下所有服务脚本的关闭操作，不管服务是处于运行或者根本没有运行的状态。而systemd可以做到只关闭在运行的服务，这样可以大大节省关机的时间。 5.不能从标准输出设备读到系统服务信息。systemd启动服务的时候，将标准输出信息定向到/dev/null，以免打扰用户。 6.systemd不继承任何上下文环境。systemd不继承任何上下文环境，如用户或者会话的HOME或者PATH的环境变量。每个服务得到的是干净的上下文环境。 7.SysV init脚本依赖性。当systemd启动SysV init脚本，systemd在运行的时候，从LinuxStandardBase(LSB)Linux标准库头文件读取服务的依赖信息并继承。 8.超时机制。为了防止系统被卡住，所有的服务有5分钟的超时机制。 systemd的架构和优缺点Systemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合，违反”keep simple, keep stupid”的Unix 哲学。下图是systemd的架构图（图片来自网络）： systemd的概念模型单元的概念在RHEL7之前，服务管理是分布式的被SysV init或UpStart通过/etc/rc.d/init.d下的脚本管理。这些脚本是经典的Bash脚本，允许管理员控制服务的状态。在RHEL7中，这些脚本被服务单元文件替换。系统初始化需要做的事情非常多。需要启动后台服务，比如启动 SSHD 服务；需要做配置工作，比如挂载文件系统。这个过程中的每一步都被 systemd 抽象为一个配置单元，即 unit。Systemd可以管理所有系统资源。不同的资源统称为 Unit（单位）。可以认为一个服务是一个配置单元；一个挂载点是一个配置单元；一个交换分区的配置是一个配置单元；等等。systemd 将配置单元归纳为以下一些不同的类型。然而，systemd 正在快速发展，新功能不断增加。所以配置单元类型可能在不久的将来继续增加。systemd中有许多单元类型，服务单元文件的扩展名是.service，同脚本的功能相似。例如有查看、启动、停止、重启、启用或者禁止服务的参数。下面说明下，systemd单元文件放置位置：/run/systemd/system #单元运行时创建，这个目录优先于按照目录/etc/systemd/system #系统管理员创建和管理的单元目录，优先级最高/usr/lib/systemd/system/system #默认单元文件安装目录有了对systemd的基本认识后，下面就开始来介绍相关的操作命令。但是注意一点，systemd并不是一个命令，而是一组命令，涉及到系统管理的方方面面。 systemd命令1.查看当前系统的所有Unitsystemctl list-units #列出正在运行的Unitsystemctl list-units --all #列出所有Unit，包括没有找到配置文件的或者启动失败的systemctl list-units --all --state=inactive #列出所有没有运行的Unitsystemctl list-units --failed #列出所有加载失败的Unitsystemctl list-units --type=service #列出所有正在运行的、类型为service的Unit 2.unit.service管理命令systemctl start name.service #启动一个服务systemctl stop name.service #关闭一个服务systemctl restart name.service #重启一个服务systemctl reload name.service #重载一个服务systemctl try-restart name.service #仅当服务运行的时候，重启服务systemctl kill name.service # 杀死一个服务的所有子进程systemctl daemon-reload #重载所有修改过的配置文件systemctl enable name.service #允许服务开机启动systemclt disable name.service #禁止服务开机启动systemctl list-dependencies nginx.service #命令列出一个Unit的所有依赖systemctl show httpd.service #显示某个Unit的所有底层参数systemctl show -p CPUShares httpd.service #显示某个Unit的指定属性的值systemctl set-property httpd.service CPUShares=500 # 设置某个Unit的指定属性 3.电源管理命令（systemctl）systemctl reboot #重启机器systemctl poweroff #关机systemctl suspend #待机systemctl hibernate #休眠systemctl hybrid-sleep #混合休眠模式（同时休眠到硬盘并待机） 4.系统引导性能分析命令（systemd-analyze） 5.systemd的日志服务systemd 自带日志服务 journald，该日志服务的设计初衷是克服现有的 syslog 服务的缺点。比如：syslog不安全，消息的内容无法验证。每一个本地进程都可以声称自己是 Apache PID 4711，而 syslog 也就相信并保存到磁盘上。数据没有严格的格式，非常随意。自动化的日志分析器需要分析人类语言字符串来识别消息。一方面此类分析困难低效；此外日志格式的变化会导致分析代码需要更新甚至重写。Systemd Journal 用二进制格式保存所有日志信息，用户使用 journalctl 命令来查看日志信息。无需自己编写复杂脆弱的字符串分析处理程序。 journalctl #查看所有日志（默认情况下 ，只保存本次启动的日志）journalctl -k #查看内核日志（不显示应用日志）journalctl -b #查看系统本次启动的日志journalctl -b -0journalctl -b -1 # 查看上一次启动的日志（需更改设置）journalctl --since=&quot;2012-10-30 18:17:16&quot; # 查看指定时间的日志journalctl --since &quot;20 min ago&quot; # 查看指定时间的日志journalctl --since yesterday # 查看指定时间的日志journalctl --since &quot;2015-01-10&quot; --until &quot;2015-01-11 03:00&quot; # 查看指定时间的日志journalctl --since 09:00 --until &quot;1 hour ago&quot; # 查看指定时间的日志journalctl -n # 显示尾部的最新10行日志journalctl -n 20 # 显示尾部指定行数的日志journalctl -f # 实时滚动显示最新日志journalctl /usr/lib/systemd/systemd # 查看指定服务的日志journalctl _PID=1 # 查看指定进程的日志journalctl /usr/bin/bash # 查看某个路径的脚本的日志journalctl _UID=33 --since today # 查看指定用户的日志journalctl -u nginx.service # 查看某个 Unit 的日志journalctl -u nginx.service --since todayjournalctl -u nginx.service -f # 实时滚动显示某个 Unit 的最新日志journalctl -u nginx.service -u php-fpm.service --since today # 合并显示多个 Unit 的日志journalctl -p err -b # 查看指定优先级（及其以上级别）的日志，共有8级-0: emerg、1: alert、2: crit、3: err、4: warning、5: notice、6: info、7: debugjournalctl --no-pager # 日志默认分页输出，--no-pager 改为正常的标准输出journalctl -b -u nginx.service -o json # 以JSON格式（单行）输出journalctl -b -u nginx.serviceqq -o json-pretty # 以JSON格式（多行）输出，可读性更好journalctl --disk-usage # 显示日志占据的硬盘空间journalctl --vacuum-size=1G # 指定日志文件占据的最大空间journalctl --vacuum-time=1years # 指定日志文件保存多久 例子(mysqld为例)# systemd service file for MySQL forking server[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/data/mysql/run/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Execute pre and post scripts as rootPermissionsStartOnly=true# Needed to create system tables#ExecStartPre=/home/bin/mysqld_pre_systemd# Start main serviceExecStart=/data/mysql/bin/mysqld --daemonize --pid-file=/data/mysql/run/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementationEnvironmentFile=-/etc/sysconfig/mysq# Sets open_files_limitLimitNOFILE = 65535Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false 附录","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【转载】centos7源介绍与软件安装","slug":"centos7-repo-introduce","date":"2017-08-05T08:52:46.000Z","updated":"2020-02-06T04:15:12.772Z","comments":true,"path":"2017/08/05/centos7-repo-introduce/","link":"","permalink":"https://youxia999.github.io/2017/08/05/centos7-repo-introduce/","excerpt":"","text":"原文地址http://yinflying.top/2017/03/373 笔记版","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【转载】centos7启动以及systemd详细说明","slug":"centos7-bootstrap","date":"2017-08-05T06:09:32.000Z","updated":"2020-02-06T04:15:12.702Z","comments":true,"path":"2017/08/05/centos7-bootstrap/","link":"","permalink":"https://youxia999.github.io/2017/08/05/centos7-bootstrap/","excerpt":"","text":"原文地址https://www.cnblogs.com/mfyang/p/7275966.html 标记版","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"构建自定义基础镜像","slug":"docker-build-basic-image","date":"2017-06-28T14:24:01.000Z","updated":"2020-02-06T04:15:12.942Z","comments":true,"path":"2017/06/28/docker-build-basic-image/","link":"","permalink":"https://youxia999.github.io/2017/06/28/docker-build-basic-image/","excerpt":"","text":"本文背景最近一年，时不时都在玩docker，有时候想实操一些特性或者功能的时候，发现从hub.docker.com上pull下的镜像，不是ubuntu就是最简版的linux,或者jre，好多命令都没有，甚是烦躁。今天终于忍不住，下定决心自己构建一个基础镜像。 编写dockerfile一直觉得centos7不错，也用习惯了。所以，基础镜像就基于centosFROM centos:7ADD jdk-8u102-linux-x64.tar.gz /usr/local/ ENV JAVA_HOME /usr/local/jdk1.8.0_102ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV PATH $PATH:$JAVA_HOME/bin 执行构建命令docker build -t centos7java:8 . 后续操作可以执行docker tag到私有仓库url，然后push到私有仓库","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"zookeeper技术概念和原理","slug":"zookeeper-introduce","date":"2017-04-20T11:17:59.000Z","updated":"2020-02-06T04:16:21.985Z","comments":true,"path":"2017/04/20/zookeeper-introduce/","link":"","permalink":"https://youxia999.github.io/2017/04/20/zookeeper-introduce/","excerpt":"","text":"参考从paxos到zookeeper做了一份思维导图","categories":[],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://youxia999.github.io/tags/zookeeper/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"docker镜像导出与导入","slug":"docker-import-and-export","date":"2017-03-25T02:40:46.000Z","updated":"2020-02-06T04:15:12.952Z","comments":true,"path":"2017/03/25/docker-import-and-export/","link":"","permalink":"https://youxia999.github.io/2017/03/25/docker-import-and-export/","excerpt":"","text":"本文背景在镜像从一个docker集群向另外一个docker集群转移时，而不想向docker中央仓库时push，下面的命令可以用的到。 docker savedocker save 镜像id &gt; 镜像名.tar 这个命令会生成一个tar包。将这个命令上传至目标docker集群的某一台机器上。 docker load和docker tagdocker load &lt; 镜像名.tar 这个命令可以导入到docker中，但是这时候还不能用。还需要打上tag。docker tag imageId 目标标签 docker push保险起见，最好push到对应的镜像仓库。","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【转载】从java到golang快速入门","slug":"from-java-to-go","date":"2017-03-22T04:28:01.000Z","updated":"2019-12-27T11:59:45.477Z","comments":true,"path":"2017/03/22/from-java-to-go/","link":"","permalink":"https://youxia999.github.io/2017/03/22/from-java-to-go/","excerpt":"","text":"原文地址https://www.flysnow.org/2016/12/28/from-java-to-golang.html 标记版","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"golang","slug":"golang","permalink":"https://youxia999.github.io/tags/golang/"}]},{"title":"玩转centos7之ssh内网穿透","slug":"centos7_shh","date":"2017-03-15T11:11:16.000Z","updated":"2020-02-06T04:15:12.813Z","comments":true,"path":"2017/03/15/centos7_shh/","link":"","permalink":"https://youxia999.github.io/2017/03/15/centos7_shh/","excerpt":"","text":"ssh命令作用一般是用于登录操作。但是输入ssh然后回车，会发现： 从command可以看出，ssh不局限于登录，还可以做隧道。 ssh做隧道进行内网穿透下面这个截图详细解析了常见的参数。","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"docker1.10安装mysql5.6主从","slug":"docker-mysql-master-slave","date":"2017-03-13T06:42:36.000Z","updated":"2020-02-06T04:16:21.596Z","comments":true,"path":"2017/03/13/docker-mysql-master-slave/","link":"","permalink":"https://youxia999.github.io/2017/03/13/docker-mysql-master-slave/","excerpt":"","text":"本文可用于测试环境，正式环境，主从还是需要部署在不同的节点上。 安装过程建数据存储目录mkdir mysql_master/mkdir mysql_slave/chown -R mysql:mysql mysql_master/chown -R mysql:mysql mysql_slave/ 准备my.cnf配置文件mysql主配置文件# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.## This program is free software; you can redistribute it and/or modify# it under the terms of the GNU General Public License as published by# the Free Software Foundation; version 2 of the License.## This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the# GNU General Public License for more details.## You should have received a copy of the GNU General Public License# along with this program; if not, write to the Free Software# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA## The MySQL Community Server configuration file.## For explanations see# http://dev.mysql.com/doc/mysql/en/server-system-variables.html[client]port = 3306socket = /var/run/mysqld/mysqld.sock[mysqld_safe]pid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.socknice = 0[mysqld]user = mysqlpid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.sockport = 3306basedir = /usrdatadir = /var/lib/mysqltmpdir = /tmplc-messages-dir = /usr/share/mysqlexplicit_defaults_for_timestamplog-bin = mysql-bin server-id = 1 # Instead of skip-networking the default is now to listen only on# localhost which is more compatible and is not less secure.#bind-address = 127.0.0.1#log-error = /var/log/mysql/error.log# Recommended in standard MySQL setupsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# * IMPORTANT: Additional settings that can override those from this file!# The files must end with &apos;.cnf&apos;, otherwise they&apos;ll be ignored.#!includedir /etc/mysql/conf.d/ mysql从配置文件# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.## This program is free software; you can redistribute it and/or modify# it under the terms of the GNU General Public License as published by# the Free Software Foundation; version 2 of the License.## This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the# GNU General Public License for more details.## You should have received a copy of the GNU General Public License# along with this program; if not, write to the Free Software# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA## The MySQL Community Server configuration file.## For explanations see# http://dev.mysql.com/doc/mysql/en/server-system-variables.html[client]port = 3306socket = /var/run/mysqld/mysqld.sock[mysqld_safe]pid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.socknice = 0[mysqld]user = mysqlpid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.sockport = 3306basedir = /usrdatadir = /var/lib/mysqltmpdir = /tmplc-messages-dir = /usr/share/mysqlexplicit_defaults_for_timestamplog-bin = mysql-bin server-id = 2 # Instead of skip-networking the default is now to listen only on# localhost which is more compatible and is not less secure.#bind-address = 127.0.0.1#log-error = /var/log/mysql/error.log# Recommended in standard MySQL setupsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# * IMPORTANT: Additional settings that can override those from this file!# The files must end with &apos;.cnf&apos;, otherwise they&apos;ll be ignored.#!includedir /etc/mysql/conf.d/ 启动mysql主从docker run -d -e MYSQL_ROOT_PASSWORD=admin --add-host www.slave.com:10.2.1.31 --name mysql_master -v /docker/mysql_cluster/master/conf/my.cnf:/etc/mysql/my.cnf -v /docker/mysql_cluster/master/data:/var/lib/mysql -p 3308:3306 mysql:5.6 docker run -d -e MYSQL_ROOT_PASSWORD=admin --add-host www.master.com:10.2.1.31 --name mysql_slave -v /docker/mysql_cluster/slave/conf/my.cnf:/etc/mysql/my.cnf -v /docker/mysql_cluster/slave/data:/var/lib/mysql -p 3309:3306 mysql:5.6 iptables防火墙放通权限iptables -I INPUT 1 -p tcp -m state --state NEW --dport 3308 -j ACCEPT 在mysql主服务上创建用户及授权CREATE USER &apos;rep01&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT REPLICATION SLAVE ON *.* TO &apos;rep01&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;SHOW MASTER STATUS;#show variables like &apos;%server_uuid%&apos;; 在mysql从服务上创建用户及授权CREATE USER &apos;rep01&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;change master to master_heartbeat_period = 10; CHANGE MASTER TO MASTER_HOST=&apos;www.master.com&apos;, MASTER_PORT=3308, MASTER_USER=&apos;rep01&apos;, MASTER_PASSWORD=&apos;123456&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000004&apos;, MASTER_LOG_POS=472;START SLAVE;show slave status; 验证效果在主服务上建立数据库/表，插入数据create database test_base;use test_base;CREATE TABLE `test_base_bank` ( `id` int(11) NOT NULL AUTO_INCREMENT, `create_time` datetime DEFAULT NULL, `modify_time` datetime DEFAULT NULL, `bank_code` varchar(10) DEFAULT NULL COMMENT &apos;银行编码&apos;, `bank_name` varchar(30) DEFAULT NULL COMMENT &apos;银行名称&apos;, `enable` varchar(10) DEFAULT NULL COMMENT &apos;是否可用&apos;, `single_payment_limit` varchar(20) DEFAULT NULL COMMENT &apos;单笔限额【元】&apos;, `day_payment_limit` varchar(20) DEFAULT NULL COMMENT &apos;日累计限额【元】&apos;, `month_payment_limit` varchar(20) DEFAULT NULL COMMENT &apos;月累计限额【元】&apos;, `bank_logo` varchar(155) DEFAULT NULL COMMENT &apos;银行logo&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;INSERT INTO `test_base`.`test_base_bank` (`id`, `create_time`, `modify_time`, `bank_code`, `bank_name`, `enable`, `single_payment_limit`, `day_payment_limit`, `month_payment_limit`, `bank_logo`) VALUES (&apos;2&apos;, &apos;2015-10-26 17:42:53&apos;, &apos;2015-11-10 22:39:37&apos;, &apos;ABC&apos;, &apos;中国农业银行&apos;, &apos;true&apos;, &apos;1&apos;, &apos;1&apos;, &apos;1&apos;, &apos;http://***/images/bank/ABC_large.png&apos;); 查看从服务器上的数据","categories":[],"tags":[{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"docker1.10安装redis主从","slug":"docker-redis-master-slave","date":"2017-03-13T05:13:50.000Z","updated":"2020-02-06T04:16:21.617Z","comments":true,"path":"2017/03/13/docker-redis-master-slave/","link":"","permalink":"https://youxia999.github.io/2017/03/13/docker-redis-master-slave/","excerpt":"","text":"本文可用于测试环境。 redis主从的工作原理1.如果设置了一个Slave，无论是第一次连接还是重连到Master，它都会发出一个SYNC命令；2.当Master收到SYNC命令之后，会做两件事： a) Master执行BGSAVE：后台写数据到磁盘（rdb快照）； b) Master同时将新收到的写入和修改数据集的命令存入缓冲区（非查询类）；3.当Master在后台把数据保存到快照文件完成之后，Master会把这个快照文件传送给Slave，而Slave则把内存清空后，加载该文件到内存中；4.而Master也会把此前收集到缓冲区中的命令，通过Reids命令协议形式转发给Slave，Slave执行这些命令，实现和Master的同步；5.Master/Slave此后会不断通过异步方式进行命令的同步，达到最终数据的同步一致；6.需要注意的是Master和Slave之间一旦发生重连都会引发全量同步操作。但在2.8之后，也可能是部分同步操作。 安装redis创建配置文件目录mkdir -p /data/server/redis/conf 下载配置文件wget https://raw.githubusercontent.com/antirez/redis/3.0/redis.conf -O /data/server/redis/conf/redis.conf 修改配置文件cd /data/server/redis/confsed -i ‘s/# slaveof /slaveof redis-master 6379/g’ redis.conf 启动主从节点docker run --name redis-master -p 6379:6379 -d redisdocker run --link redis-master:redis-master -p 6380:6379 -v /data/server/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf --name redis-slave1 -d redis redis-server /usr/local/etc/redis/redis.confdocker run --link redis-master:redis-master -p 6381:6379 -v /data/server/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf --name redis-slave2 -d redis redis-server /usr/local/etc/redis/redis.confdocker run --link redis-master:redis-master -p 6382:6379 -v /data/server/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf --name redis-slave3 -d redis redis-server /usr/local/etc/redis/redis.conf redis主从实验","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://youxia999.github.io/tags/redis/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"高可用软件之keepalived+Haproxy","slug":"common-ha-software-haproxy","date":"2017-03-10T03:27:58.000Z","updated":"2020-02-06T04:16:21.529Z","comments":true,"path":"2017/03/10/common-ha-software-haproxy/","link":"","permalink":"https://youxia999.github.io/2017/03/10/common-ha-software-haproxy/","excerpt":"","text":"资料https://blog.51cto.com/kernal/1433907 背景交代本文使用软件版本keepalived: v1.2.13haproxy: v1.5.18centos: centos6.3、centos7.3lightttpd: 1.4.52 ip和部署规划192.168.128.50(主K)：centos6、keepalived、haproxy192.168.128.51(次K)：centos6、keepalived、haproxy192.168.128.85(real server1)：centos7、lighttpd192.168.128.97(real server2)：centos7、lighttpd 需要确保的事情安装软件并配置安装lighttpdyum install -y epel-release gcc gcc-c++ autoconf automake pcre-devel zip unzip libtool &amp;&amp; yum install -y lighttpd 配置lighttpd########################################################################### /etc/lighttpd/lighttpd.conf#### check /etc/lighttpd/conf.d/*.conf for the configuration of modules.#################################################################################################################################################### Some Variable definition which will make chrooting easier.#### if you add a variable here. Add the corresponding variable in the## chroot example aswell.##var.log_root = &quot;/data/lighttpd/log&quot;var.server_root = &quot;/data/lighttpd/www&quot;var.state_dir = &quot;/var/run&quot;var.home_dir = &quot;/data/lighttpd&quot;var.conf_dir = &quot;/etc/lighttpd&quot;#### run the server chrooted.#### This requires root permissions during startup.#### If you run Chrooted set the the variables to directories relative to## the chroot dir.#### example chroot configuration:###var.log_root = &quot;/logs&quot;#var.server_root = &quot;/&quot;#var.state_dir = &quot;/run&quot;#var.home_dir = &quot;/lib/lighttpd&quot;#var.vhosts_dir = &quot;/vhosts&quot;#var.conf_dir = &quot;/etc&quot;##server.chroot = &quot;/srv/www&quot;#### Some additional variables to make the configuration easier###### Base directory for all virtual hosts#### used in:## conf.d/evhost.conf## conf.d/simple_vhost.conf## vhosts.d/vhosts.template##var.vhosts_dir = server_root + &quot;/vhosts&quot;#### Cache for mod_compress#### used in:## conf.d/compress.conf##var.cache_dir = &quot;/data/lighttpd/cache&quot;#### Base directory for sockets.#### used in:## conf.d/fastcgi.conf## conf.d/scgi.conf##var.socket_dir = state_dir + &quot;/sockets&quot;#################################################################################################################################################### Load the modules.include &quot;modules.conf&quot;#################################################################################################################################################### Basic Configuration## ---------------------##server.port = 28080#### Use IPv6?###server.use-ipv6 = &quot;enable&quot;#### bind to a specific IP##server.bind = &quot;192.168.128.97&quot;#### Run as a different username/groupname.## This requires root permissions during startup.##server.username = &quot;lighttpd&quot;server.groupname = &quot;lighttpd&quot;#### enable core files.###server.core-files = &quot;disable&quot;#### Document root##server.document-root = server_root + &quot;/lighttpd&quot;#### The value for the &quot;Server:&quot; response field.#### It would be nice to keep it at &quot;lighttpd&quot;.###server.tag = &quot;lighttpd&quot;#### store a pid file##server.pid-file = state_dir + &quot;/lighttpd.pid&quot;#################################################################################################################################################### Logging Options## ------------------#### all logging options can be overwritten per vhost.#### Path to the error log file##server.errorlog = log_root + &quot;/error.log&quot;#### If you want to log to syslog you have to unset the## server.errorlog setting and uncomment the next line.###server.errorlog-use-syslog = &quot;enable&quot;#### Access log config##include &quot;conf.d/access_log.conf&quot;#### The debug options are moved into their own file.## see conf.d/debug.conf for various options for request debugging.##include &quot;conf.d/debug.conf&quot;#################################################################################################################################################### Tuning/Performance## --------------------#### corresponding documentation:## https://redmine.lighttpd.net/projects/lighttpd/wiki/Docs_Performance#### set the event-handler (read the performance section in the manual)#### possible options on linux are:#### select## poll## linux-sysepoll#### linux-sysepoll is recommended on kernel 2.6.##server.event-handler = &quot;linux-sysepoll&quot;#### The basic network interface for all platforms at the syscalls read()## and write(). Every modern OS provides its own syscall to help network## servers transfer files as fast as possible#### sendfile - is recommended for small files.## writev - is recommended for sending many large files##server.network-backend = &quot;sendfile&quot;#### As lighttpd is a single-threaded server, its main resource limit is## the number of file descriptors, which is set to 1024 by default (on## most systems).#### If you are running a high-traffic site you might want to increase this## limit by setting server.max-fds.#### Changing this setting requires root permissions on startup. see## server.username/server.groupname.#### By default lighttpd would not change the operation system default.## But setting it to 2048 is a better default for busy servers.#### With SELinux enabled, this is denied by default and needs to be allowed## by running the following once : setsebool -P httpd_setrlimit on#server.max-fds = 2048#### listen-backlog is the size of the listen() backlog queue requested when## the lighttpd server ask the kernel to listen() on the provided network## address. Clients attempting to connect() to the server enter the listen()## backlog queue and wait for the lighttpd server to accept() the connection.#### The out-of-box default on many operating systems is 128 and is identified## as SOMAXCONN. This can be tuned on many operating systems. (On Linux,## cat /proc/sys/net/core/somaxconn) Requesting a size larger than operating## system limit will be silently reduced to the limit by the operating system.#### When there are too many connection attempts waiting for the server to## accept() new connections, the listen backlog queue fills and the kernel## rejects additional connection attempts. This can be useful as an## indication to an upstream load balancer that the server is busy, and## possibly overloaded. In that case, configure a smaller limit for## server.listen-backlog. On the other hand, configure a larger limit to be## able to handle bursts of new connections, but only do so up to an amount## that the server can keep up with responding in a reasonable amount of## time. Otherwise, clients may abandon the connection attempts and the## server will waste resources servicing abandoned connections.#### It is best to leave this setting at its default unless you have modelled## your traffic and tested that changing this benefits your traffic patterns.#### Default: 1024###server.listen-backlog = 128#### Stat() call caching.#### lighttpd can utilize FAM/Gamin to cache stat call.#### possible values are:## disable, simple or fam.##server.stat-cache-engine = &quot;simple&quot;#### Fine tuning for the request handling#### max-connections == max-fds/2 (maybe /3)## means the other file handles are used for fastcgi/files##server.max-connections = 1024#### How many seconds to keep a keep-alive connection open,## until we consider it idle.#### Default: 5###server.max-keep-alive-idle = 5#### How many keep-alive requests until closing the connection.#### Default: 16###server.max-keep-alive-requests = 16#### Maximum size of a request in kilobytes.## By default it is unlimited (0).#### Uploads to your server cant be larger than this value.###server.max-request-size = 0#### Time to read from a socket before we consider it idle.#### Default: 60###server.max-read-idle = 60#### Time to write to a socket before we consider it idle.#### Default: 360###server.max-write-idle = 360#### Traffic Shaping## -----------------#### see /usr/share/doc/lighttpd/traffic-shaping.txt#### Values are in kilobyte per second.#### Keep in mind that a limit below 32kB/s might actually limit the## traffic to 32kB/s. This is caused by the size of the TCP send## buffer.#### per server:###server.kbytes-per-second = 128#### per connection:###connection.kbytes-per-second = 32#################################################################################################################################################### Filename/File handling## ------------------------#### files to check for if .../ is requested## index-file.names = ( &quot;index.php&quot;, &quot;index.rb&quot;, &quot;index.html&quot;,## &quot;index.htm&quot;, &quot;default.htm&quot; )##index-file.names += ( &quot;index.xhtml&quot;, &quot;index.html&quot;, &quot;index.htm&quot;, &quot;default.htm&quot;, &quot;index.php&quot;)#### deny access the file-extensions#### ~ is for backupfiles from vi, emacs, joe, ...## .inc is often used for code includes which should in general not be part## of the document-rooturl.access-deny = ( &quot;~&quot;, &quot;.inc&quot; )#### disable range requests for pdf files## workaround for a bug in the Acrobat Reader plugin.##$HTTP[&quot;url&quot;] =~ &quot;\\.pdf$&quot; &#123; server.range-requests = &quot;disable&quot;&#125;#### url handling modules (rewrite, redirect)###url.rewrite = ( &quot;^/$&quot; =&gt; &quot;/server-status&quot; )#url.redirect = ( &quot;^/wishlist/(.+)&quot; =&gt; &quot;http://www.example.com/$1&quot; )#### both rewrite/redirect support back reference to regex conditional using %n###$HTTP[&quot;host&quot;] =~ &quot;^www\\.(.*)&quot; &#123;# url.redirect = ( &quot;^/(.*)&quot; =&gt; &quot;http://%1/$1&quot; )#&#125;#### which extensions should not be handle via static-file transfer#### .php, .pl, .fcgi are most often handled by mod_fastcgi or mod_cgi##static-file.exclude-extensions = ( &quot;.php&quot;, &quot;.pl&quot;, &quot;.fcgi&quot;, &quot;.scgi&quot; )#### error-handler for all status 400-599###server.error-handler = &quot;/error-handler.html&quot;#server.error-handler = &quot;/error-handler.php&quot;#### error-handler for status 404###server.error-handler-404 = &quot;/error-handler.html&quot;#server.error-handler-404 = &quot;/error-handler.php&quot;#### Format: &lt;errorfile-prefix&gt;&lt;status-code&gt;.html## -&gt; ..../status-404.html for &apos;File not found&apos;###server.errorfile-prefix = &quot;/srv/www/htdocs/errors/status-&quot;#### mimetype mapping##include &quot;conf.d/mime.conf&quot;#### directory listing configuration##include &quot;conf.d/dirlisting.conf&quot;#### Should lighttpd follow symlinks?##server.follow-symlink = &quot;enable&quot;#### force all filenames to be lowercase?###server.force-lowercase-filenames = &quot;disable&quot;#### defaults to /var/tmp as we assume it is a local harddisk##server.upload-dirs = ( &quot;/var/tmp&quot; )#################################################################################################################################################### SSL Support## -------------#### To enable SSL for the whole server you have to provide a valid## certificate and have to enable the SSL engine.::#### ssl.engine = &quot;enable&quot;## ssl.pemfile = &quot;/path/to/server.pem&quot;#### The HTTPS protocol does not allow you to use name-based virtual## hosting with SSL. If you want to run multiple SSL servers with## one lighttpd instance you must use IP-based virtual hosting: ::#### Mitigate CVE-2009-3555 by disabling client triggered renegotation## This is enabled by default.#### IMPORTANT: this setting can only be used in the global scope.## It does *not* work inside conditionals### ssl.disable-client-renegotiation = &quot;enable&quot;#### $SERVER[&quot;socket&quot;] == &quot;10.0.0.1:443&quot; &#123;## ssl.engine = &quot;enable&quot;## ssl.pemfile = &quot;/etc/ssl/private/www.example.com.pem&quot;## ### # (Following SSL/TLS Deployment Best Practices 1.3 / 17 September 2013 from:## # https://www.ssllabs.com/projects/best-practices/index.html)## # - BEAST is considered mitigaed on client side now, and new weaknesses have been found in RC4,## # so it is strongly advised to disable RC4 ciphers (HIGH doesn&apos;t include RC4)## # - It is recommended to disable 3DES too (although disabling RC4 and 3DES breaks IE6+8 on Windows XP,## # so you might want to support 3DES for now - just remove the &apos;!3DES&apos; parts below).## # - The examples below prefer ciphersuites with &quot;Forward Secrecy&quot; (and ECDHE over DHE (alias EDH)), remove &apos;+kEDH +kRSA&apos;## # if you don&apos;t want that.## # - SRP and PSK are not supported anyway, excluding those (&apos;!kSRP !kPSK&apos;) just keeps the list smaller (easier to review)## # Check your cipher list with: openssl ciphers -v &apos;...&apos; (use single quotes as your shell won&apos;t like ! in double quotes)## ### # If you know you have RSA keys (standard), you can use:## ssl.cipher-list = &quot;PROFILE=SYSTEM&quot;## # The more generic version (without the restriction to RSA keys) is## # ssl.cipher-list = &quot;HIGH !aNULL !3DES +kEDH +kRSA !kSRP !kPSK&quot;## ### # Make the server prefer the order of the server side cipher suite instead of the client suite.## # This option is enabled by default, but only used if ssl.cipher-list is set.## ### # ssl.honor-cipher-order = &quot;enable&quot;## ### server.name = &quot;www.example.com&quot;#### server.document-root = &quot;/srv/www/vhosts/example.com/www/&quot;## &#125;#### If you have a .crt and a .key file, cat them together into a## single PEM file:## $ cat /etc/ssl/private/lighttpd.key /etc/ssl/certs/lighttpd.crt \\## &gt; /etc/ssl/private/lighttpd.pem###ssl.pemfile = &quot;/etc/ssl/private/lighttpd.pem&quot;#### optionally pass the CA certificate here.#####ssl.ca-file = &quot;&quot;#### and the CRL revocation list here.#####ssl.ca-crl-file = &quot;&quot;#################################################################################################################################################### custom includes like vhosts.###include &quot;conf.d/config.conf&quot;#include &quot;/etc/lighttpd/vhosts.d/*.conf&quot;#########################################################################","categories":[],"tags":[{"name":"haporxy","slug":"haporxy","permalink":"https://youxia999.github.io/tags/haporxy/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【转载】docker容器的网络模式","slug":"docker-network-introduce","date":"2017-02-14T05:13:58.000Z","updated":"2020-02-06T04:15:12.985Z","comments":true,"path":"2017/02/14/docker-network-introduce/","link":"","permalink":"https://youxia999.github.io/2017/02/14/docker-network-introduce/","excerpt":"","text":"原文地址https://cizixs.com/2016/06/12/docker-network-modes-explained/ 笔记版","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"docker1.10安装和初步使用","slug":"docker-1-10-install","date":"2017-02-05T01:54:55.000Z","updated":"2020-02-06T04:15:12.896Z","comments":true,"path":"2017/02/05/docker-1-10-install/","link":"","permalink":"https://youxia999.github.io/2017/02/05/docker-1-10-install/","excerpt":"","text":"本文背景背景经同事推荐，觉得docker对于我们这些centos菜鸟很不错（把安装步骤在dokcerfile写好了），建议学习一下。 参考资料https://hub.docker.com (很重要，类似于官方文档)https://cr.console.aliyun.com/cn-hangzhou/mirrors (镜像加速器，在国内绕不过去的点)http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/Packages/ （各版本的安装包） 安装过程下载rpm文件yum install -y --nogpgcheck http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/Packages/docker-engine-selinux-1.10.3-1.el7.centos.noarch.rpmyum install -y --nogpgcheck http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/Packages/docker-engine-1.10.3-1.el7.centos.x86_64.rpm 配置文件因为是yum安装，所以，systemd会有一个文件来管理启动。通过查看docker.service文件，知道我们的配置文件目录在/etc/sysconfig/docker.修改数据目录（别放到/var/lib目录,磁盘分分钟会爆）和加速器OPTIONS=&apos;--graph=/docker/dockbak/ --registry-mirror=https://csokz3oi.mirror.aliyuncs.com&apos; 这里没有配置私有的镜像仓库地址（线上环境还是要配置仓库，比如打标签、push镜像用） 其他可能会出问题的地方其他可能会出现问题的地方，基本上都是在启动命令这一行。有问题，修改就好ExecStart=/usr/bin/docker-current daemon \\ --exec-opt native.cgroupdriver=systemd \\ $OPTIONS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $ADD_REGISTRY \\ $BLOCK_REGISTRY \\ $INSECURE_REGISTRY 启动命令systemctl daemon-reloadsystemctl restart docker.service docker实操可以见docker help命令和官网指南","categories":[],"tags":[{"name":"容器技术","slug":"容器技术","permalink":"https://youxia999.github.io/tags/容器技术/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【java应用开发脚手架系列】mybatis-spring-boot-starter使用","slug":"spring-boot-1x-mybatis-starter-example","date":"2016-12-28T13:52:03.000Z","updated":"2019-12-27T11:59:45.688Z","comments":true,"path":"2016/12/28/spring-boot-1x-mybatis-starter-example/","link":"","permalink":"https://youxia999.github.io/2016/12/28/spring-boot-1x-mybatis-starter-example/","excerpt":"","text":"本文参考资料http://www.ityouknow.com/springboot/2016/11/25/spring-boot-multi-mybatis.html","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"mysql5.7安装和物理文件解读","slug":"mysql5-7-install","date":"2016-12-21T09:02:30.000Z","updated":"2020-02-06T04:16:21.750Z","comments":true,"path":"2016/12/21/mysql5-7-install/","link":"","permalink":"https://youxia999.github.io/2016/12/21/mysql5-7-install/","excerpt":"","text":"安装规划不要安装在根目录下，尽量挂在数据盘下。通用做法:数据盘/data。 mysql下载、安装和配置下载、解压、环境变量配置groupadd mysqluseradd -g mysql -s /sbin/nologin mysqlcd /root/tar -zxvf mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz -C /datacd /data/ln -s mysql-5.7.17-linux-glibc2.5-x86_64 mysqlecho &quot;export PATH=$PATH:/data/mysql/bin&quot; &gt;&gt; /etc/profilesource /etc/profile 创建数据、日志、配置文件目录mkdir -p /data/mysql/&#123;data,binlogs,log,etc,run&#125;chown -R mysql.mysql /data/mysql/&#123;data,binlogs,log,etc,run&#125; 增加配置文件编译版本的mysql，配置文件只允许在/etc/my.cnf(或者/etc/mysql/my.cnf、/usr/local/mysql/etc/my.cnf).vim etc/my.cnf[client]port = 3306socket = /data/mysql/run/mysql.sock[mysqld]port = 3306basedir=/data/mysql/socket = /data/mysql/run/mysql.sockpid_file = /data/mysql/run/mysql.piddatadir = /data/mysql/datadefault_storage_engine = InnoDBmax_allowed_packet = 512Mmax_connections = 2048open_files_limit = 65535skip-name-resolvelower_case_table_names=1character-set-server = utf8mb4collation-server = utf8mb4_unicode_ciinit_connect=&apos;SET NAMES utf8mb4&apos;innodb_buffer_pool_size = 1024Minnodb_log_file_size = 2048Minnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 0key_buffer_size = 64Mlog-error = /data/mysql/log/mysql_error.loglog-bin = /data/mysql/binlogs/mysql-binslow_query_log = 1slow_query_log_file = /data/mysql/log/mysql_slow_query.loglong_query_time = 5tmp_table_size = 32Mmax_heap_table_size = 32Mquery_cache_type = 0query_cache_size = 0server-id=1 mysql初始化和物理文件解读参考 https://dev.mysql.com/doc/refman/5.7/en/programs-overview.html ，说明下mysql组件中最常用的三个组件: mysqldmysql daemon，mysql server。 mysqlThe command-line tool for interactively entering SQL statements or executing them from a file in batch mode) mysql_ssl_rsa_setupThis program creates the SSL certificate and key files and RSA key-pair files required to support secure connections, if those files are missing. mysql服务端启动核心-mysqld帮助 初始化mysql数据库mysqld --initialize-insecure --user=mysql --basedir=/data/mysql --datadir=/data/mysql/datamysql_ssl_rsa_setup --basedir=/data/mysql --datadir=/data/mysql/data/ 查看数据库物理文件cd /data/mysql/data/ mysql系统数据库在MySQL5.7.17中，系统数据库包括information_schema，mysql，sys，performance_schema information_schema库提供了数据库的元数据信息，是数据库的数据，比如数据库的名字，数据库中的表名，字段名，字段类型等，可以说是数据库的数据字典信息。这个库中的信息并非物理地保存在表中，而是动态地去读取其他文件得到的，比如上面一开始提到的共享表空间，对于用户数据中的对象，比如表结构等，都保存在共享表空间中，information_schema库中的一些信息可以认为是直接映射到共享表空间中的信息的。因此第一个截图中，并没有information_schema的路径（文件夹） performance_schema库是数据库性能相关的信息的数据，记录的是数据库服务器的性能参数。1）保留进程等待信息，包括锁，互斥变量，文件信息等。2）保存历史事件汇总信息，为MySQL服务器性能评估提供参考信息3）配置型选项，来决定是否记录一些与性能相关的信息，比如profile信息等，参考http://www.cnblogs.com/wy123/p/6979499.html sys库可以根据sys库中的数据快速了解系统的运行信息，方便地查询出来数据库的信息，在性能瓶颈，自动化吧运维等方面都有很大的帮助sys库中的信息是通过视图的方式，将information_schema和performance_schema库中的数据结合起来，可以得到更加直观和容易理解的信息 mysql库存储了系统的用户权限信息及帮助信息，新建的用户，用户的权限信息的都存储在MySQL库。比如在修改MySQL的root密码的时候，都要先use mysql这个系统库，然后再执行用户，授权等操作。对于innodb表，如果是独立的表空间的话，数据库中的表结构以及数据都存储在数据库的路径下（而不是在共享表空间中ibdata1文件中）但是数据中的其他对象，包括undo信息，也即数据被修改之后，事务提交之间的版本信息，仍然存储在共享表空间的ibdata1文件中 基于ibdata1文件的共享表空间对于innodb，innodb_file_per_table选项决定了是否启动独立表空间，MySQL5.7中是默认启动的，也就是说MySQL的用户数据库将使用独立表空间来存储数据。 基于ibtmp1文件的临时表空间临时表空间是存储全局级，回话级，事物级，检索级临时表对象的地方，有参数innodb_temp_data_file_path可以看到临时表空间的信息。 基于ib_logfileN的重做日志redo日志默认情况下有两个文件，也即：ib_logfile0和ib_logfile1。如果在数据库启动的过程中没有这两个文件，系统会默认自动生成这两个文件。默认情况下，ib_logfile0和ib_logfile1是两个独立的日志文件（可以配置的更多个ib_logfile文件），但是redo日志的写入在逻辑上对于ib_logfile0和ib_logfile1是连续的。重做日志是MySQL事物处理的核心文件，事务处理的核心之一是一致性，也就是说要么全做，要么全不做。 基于mysql-bin.X的binlog文件bin-log日志记录数据中发生的写入性操作（增删改），但不记录查询操作，语句以事件的方式保存，描述了数据的更改过程，此日志对发生灾难时数据恢复、主从同步中起到了极为重要的作用。 mysql配置systemd启动mysql配置systemd启动cd /usr/lib/systemd/systemtouch mysqld.service vim mysqld.service 最终的文件如下: # systemd service file for MySQL forking server[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/data/mysql/run/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Execute pre and post scripts as rootPermissionsStartOnly=true# Needed to create system tables#ExecStartPre=/home/bin/mysqld_pre_systemd# Start main serviceExecStart=/data/mysql/bin/mysqld --daemonize --pid-file=/data/mysql/run/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementationEnvironmentFile=-/etc/sysconfig/mysq# Sets open_files_limitLimitNOFILE = 65535Restart=on-failureRestartPreventExitStatus=1PrivateTmp=false mysql启动systemctl daemon-reloadsystemctl enable mysqld.servicesystemctl is-enabled mysqldsystemctl start mysqld.service mysql修改root密码和访问权限mysql -h localhost -u rootflush privileges;use mysql;UPDATE user SET authentication_string = PASSWORD(&apos;123456&apos;), password_expired = &apos;N&apos; WHERE User = &apos;root&apos; AND Host = &apos;localhost&apos;;UPDATE user SET Host = &apos;%&apos; WHERE User = &apos;root&apos;;flush privileges;","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://youxia999.github.io/tags/mysql/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"mysql配置文件解读","slug":"mysql-conf-introduce","date":"2016-12-21T08:18:14.000Z","updated":"2020-02-06T04:16:21.738Z","comments":true,"path":"2016/12/21/mysql-conf-introduce/","link":"","permalink":"https://youxia999.github.io/2016/12/21/mysql-conf-introduce/","excerpt":"","text":"本文参考资料https://mysqlrelease.com/2016/12/announcing-mysql-server-5-7-17-5-6-35-and-5-5-54/https://dev.mysql.com/doc/relnotes/mysql/5.7/en/https://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-17.htmlhttps://dev.mysql.com/doc/refman/5.7/en/programs-overview.html mysql核心参数解释mysqld配置mysqld程序basic配置# 以下选项会被MySQL客户端应用读取。注意只有MySQL附带的客户端应用程序保证可以读取这段内容。如果你想你自己的MySQL应用程序获取这些值。需要在MySQL客户端库初始化的时候指定这些选项。# For advice on how to change settings please see http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html# *** DO NOT EDIT THIS FILE. It&apos;s a template which will be copied to the# *** default location during install, and will be replaced if you# *** upgrade to a newer version of MySQL.[mysqld]# Remove leading # and set to the amount of RAM for the most important data cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M# ★★★这里很重要️能让MySQL登陆链接变快速skip-name-resolve# Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.# log_bin# These are commonly set, remove the # and set as required.# 使用给定目录作为根目录(安装目录)。# basedir = .....# 从给定目录读取数据库文件。# datadir = .....# 为mysqld程序指定一个存放进程ID的文件(仅适用于UNIX/Linux系统);# pid-file = .....# 指定MsSQL侦听的端口# port = .....# server_id = .....# 为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(Linux下默认是/var/lib/mysql/mysql.sock文件)# socket = .....sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION# 一般配置选项datadir = /var/lib/mysqlpid-file = /var/run/mysqld/mysqld.pidport = 3306socket = /var/run/mysqld/mysqld.socklog-error = /var/log/mysql/error.log# 设置character-set-server=utf8# 指定MySQL可能的连接数量。当MySQL主线程在很短时间内接收到非常多的连接请求，该参数生效，主线程花费很短时间检查连接并且启动一个新线程。# back_log参数的值指出在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中。# 如果系统在一个短时间内有很多连接，则需要增大该参数的值，该参数值指定到来的TCP/IP连接的侦听队列的大小。# 试图设定back_log高于你的操作系统的限制将是无效的。默认值为50。对于Linux系统推荐设置为小于512的整数。# back_log 是操作系统在监听队列中所能保持的连接数,队列保存了在 MySQL 连接管理器线程处理之前的连接.# 如果你有非常高的连接率并且出现 “connection refused” 报错,你就应该增加此处的值.# 检查你的操作系统文档来获取这个变量的最大值.如果将back_log设定到比你操作系统限制更高的值，将会没有效果back_log = 300# 不在 TCP/IP 端口上进行监听.如果所有的进程都是在同一台服务器连接到本地的 mysqld,# 这样设置将是增强安全的方法所有 mysqld 的连接都是通过 Unix Sockets 或者命名管道进行的.# 注意在 Windows下如果没有打开命名管道选项而只是用此项(通过 “enable-named-pipe” 选项) 将会导致 MySQL 服务没有任何作用!#skip-networking# MySQL 服务所允许的同时会话数的上限# 其中一个连接将被 SUPER 权限保留作为管理员登录.# 即便已经达到了连接数的上限.max_connections = 3000# 每个客户端连接最大的错误允许数量,如果达到了此限制.这个客户端将会被 MySQL 服务阻止直到执行了 “FLUSH HOSTS” 或者服务重启# 非法的密码以及其他在链接时的错误会增加此值.查看 “Aborted_connects” 状态来获取全局计数器.max_connect_errors = 50# 所有线程所打开表的数量.增加此值就增加了 mysqld 所需要的文件描述符的数量。# 这样你需要确认在 [mysqld_safe] 中 “open-files-limit” 变量设置打开文件数量允许至少等于 table_cache 的值table_open_cache = 4096# 允许外部文件级别的锁. 打开文件锁会对性能造成负面影响，所以只有在你在同样的文件上运行多个数据库实例时才使用此选项(注意仍会有其他约束!)# 或者你在文件层面上使用了其他一些软件依赖来锁定 MyISAM 表#external-locking# 服务所能处理的请求包的最大大小以及服务所能处理的最大的请求大小(当与大的 BLOB 字段一起工作时相当必要)。每个连接独立的大小，大小动态增加max_allowed_packet = 32M# 在一个事务中binlog为了记录SQL状态所持有的cache大小。如果你经常使用大的,多声明的事务,你可以增加此值来获取更大的性能.# 所有从事务来的状态都将被缓冲在 binlog 缓冲中然后在提交后一次性写入到 binlog 中如果事务比此值大, 会使用磁盘上的临时文件来替代.# 此缓冲在每个连接的事务第一次更新状态时被创建binlog_cache_size = 4M# 独立的内存表所允许的最大容量。此选项为了防止意外创建一个超大的内存表导致永尽所有的内存资源。max_heap_table_size = 128M# 随机读取数据缓冲区使用内存(read_rnd_buffer_size)：和顺序读取相对应，# 当 MySQL 进行非顺序读取（随机读取）数据块的时候，会利用&gt;这个缓冲区暂存读取的数据# 如根据索引信息读取表数据，根据排序后的结果集与表进行 Join 等等# 总的来说，就是当数据块的读取需要满足&gt;一定的顺序的情况下，MySQL 就需要产生随机读取，进而使用到 read_rnd_buffer_size 参数所设置的内存缓冲区read_rnd_buffer_size = 16M# 排序缓冲被用来处理类似 ORDER BY 以及 GROUP BY 队列所引起的排序# 如果排序后的数据无法放入排序缓冲,一个用来替代的基于磁盘的合并分类会被使用# 查看 “Sort_merge_passes” 状态变量。# 在排序发生时由每个线程分配# 每个需要进行排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY或GROUP BY操作。# 注意：该参数对应的分配内存是每连接独占！如果有100个连接，那么实际分配的总共排序缓冲区大小为100×6=600MBsort_buffer_size = 16M# 此缓冲被使用来优化全联合(FULL JOINS 不带索引的联合)。# 类似的联合在极大多数情况下有非常糟糕的性能表现,但是将此值设大能够减轻性能影响。# 通过 “Select_full_join” 状态变量查看全联合的数量# 当全联合发生时,在每个线程中分配join_buffer_size = 16M# 缓存可重用的线程数# thread_cache = 8# 避免MySQL的外部锁定，减少出错几率增强稳定性。# skip-locking # 我们在 cache 中保留多少线程用于重用# 当一个客户端断开连接后,如果 cache 中的线程还少于 thread_cache_size,则客户端线程被放入cache 中。# 这可以在你需要大量新连接的时候极大的减少线程创建的开销# (一般来说如果你有好的线程模型的话,这不会有明显的性能提升。)thread_cache_size = 16# 此允许应用程序给予线程系统一个提示在同一时间给予渴望被运行的线程的数量。# 此值只对于支持 thread_concurrency() 函数的系统有意义( 例如Sun Solaris)。# 你可可以尝试使用 [CPU数量]*(2..4) 来作为 thread_concurrency 的值#****(此属性对当前环境无效)****# thread_concurrency = 8# 查询缓冲常被用来缓冲 SELECT 的结果并且在下一次同样查询的时候不再执行直接返回结果。# 打开查询缓冲可以极大的提高服务器速度, 如果你有大量的相同的查询并且很少修改表。# 查看 “Qcache_lowmem_prunes” 状态变量来检查是否当前值对于你的负载来说是否足够高。# 注意: 在你表经常变化的情况下或者如果你的查询原文每次都不同,# 查询缓冲也许引起性能下降而不是性能提升。query_cache_size = 128M# 只有小于此设定值的结果才会被缓冲# 此设置用来保护查询缓冲,防止一个极大的结果集将其他所有的查询结果都覆盖。query_cache_limit = 4M# 被全文检索索引的最小的字长。# 你也许希望减少它，如果你需要搜索更短字的时候。# 注意在你修改此值之后，你需要重建你的 FULLTEXT 索引ft_min_word_len = 8# 如果你的系统支持 memlock() 函数，你也许希望打开此选项用以让运行中的 mysql 在在内存高度紧张的时候，数据在内存中保持锁定并且防止可能被 swapping out# 此选项对于性能有益#memlock# 当创建新表时作为默认使用的表类型，# 如果在创建表示没有特别执行表类型，将会使用此值#****(此属性对当前环境无效)****#default_table_type = InnoDB# 线程使用的堆大小. 此容量的内存在每次连接时被预留.# MySQL 本身常不会需要超过 64K 的内存# 如果你使用你自己的需要大量堆的 UDF 函数或者你的操作系统对于某些操作需要更多的堆，你也许需要将其设置的更高一点.thread_stack = 512K# 设定默认的事务隔离级别.可用的级别如下:# READ-UNCOMMITTED， READ-COMMITTED， REPEATABLE-READ， SERIALIZABLEtransaction_isolation = REPEATABLE-READ# 内部(内存中)临时表的最大大小# 如果一个表增长到比此值更大，将会自动转换为基于磁盘的表。# 此限制是针对单个表的，而不是总和。tmp_table_size = 128M# 打开二进制日志功能。# 在复制(replication)配置中，作为 MASTER 主服务器必须打开此项# 如果你需要从你最后的备份中做基于时间点的恢复，你也同样需要二进制日志。log-bin=mysql-bin# 如果你在使用链式从服务器结构的复制模式 (A-&gt;B-&gt;C)，# 你需要在服务器B上打开此项。# 此选项打开在从线程上重做过的更新的日志， 并将其写入从服务器的二进制日志。#log_slave_updates# 打开全查询日志。 所有的由服务器接收到的查询 (甚至对于一个错误语法的查询)# 都会被记录下来。 这对于调试非常有用， 在生产环境中常常关闭此项。#log# 将警告打印输出到错误 log 文件。 如果你对于 MySQL 有任何问题# 你应该打开警告 log 并且仔细审查错误日志，查出可能的原因。#log_warnings# 记录慢速查询。 慢速查询是指消耗了比 “long_query_time” 定义的更多时间的查询。如果 log_long_format 被打开，那些没有使用索引的查询也会被记录。# 如果你经常增加新查询到已有的系统内的话。 一般来说这是一个好主意，#log_slow_queries# 有的使用了比这个时间(以秒为单位)更多的查询会被认为是慢速查询。# 不要在这里使用“1″, 否则会导致所有的查询,甚至非常快的查询页被记录下来(由于 MySQL 目前时间的精确度只能达到秒的级别)。long_query_time = 6# 在慢速日志中记录更多的信息。一般此项最好打开。打开此项会记录使得那些没有使用索引的查询也被作为到慢速查询附加到慢速日志里#log_long_format# 此目录被MySQL用来保存临时文件。例如,# 它被用来处理基于磁盘的大型排序,和内部排序一样。# 以及简单的临时表。# 如果你不创建非常大的临时文件,将其放置到 swapfs/tmpfs 文件系统上也许比较好# 另一种选择是你也可以将其放置在独立的磁盘上。# 你可以使用”;”来放置多个路径# 他们会按照 roud-robin 方法被轮询使用。#tmpdir = /tmp mysqld主从复制相关的设置# 唯一的服务辨识号,数值位于 1 到 2^32-1之间。# 此值在master和slave上都需要设置。# 如果 “master-host” 没有被设置,则默认为1, 但是如果忽略此选项,MySQL不会作为master生效。server-id = 1# 复制的Slave (去掉master段的注释来使其生效)## 为了配置此主机作为复制的slave服务器,你可以选择两种方法:## 1) 使用 CHANGE MASTER TO 命令 (在我们的手册中有完整描述) -# 语法如下:## CHANGE MASTER TO MASTER_HOST=, MASTER_PORT=,# MASTER_USER=, MASTER_PASSWORD= ;## 你需要替换掉，等被尖括号包围的字段以及使用master的端口号替换 (默认3306)。## 例子:## CHANGE MASTER TO MASTER_HOST=’125.564.12.1′, MASTER_PORT=3306,# MASTER_USER=’joe’, MASTER_PASSWORD=’secret’;## 或者## 2) 设置以下的变量. 不论如何, 在你选择这种方法的情况下， 然后第一次启动复制(甚至不成功的情况下，# 例如如果你输入错密码在master-password字段并且slave无法连接)，# slave会创建一个 master.info 文件，并且之后任何对于包含在此文件内的参数的变化都会被忽略# 并且由 master.info 文件内的内容覆盖， 除非你关闭slave服务， 删除 master.info 并且重启slave 服务。# 由于这个原因，你也许不想碰一下的配置(注释掉的) 并且使用 CHANGE MASTER TO (查看上面) 来代替## 所需要的唯一id号位于 2 和 2^32 – 1之间# (并且和master不同)# 如果master-host被设置了.则默认值是2# 但是如果省略,则不会生效#server-id = 2## 复制结构中的master – 必须#master-host =## 当连接到master上时slave所用来认证的用户名 – 必须#master-user =## 当连接到master上时slave所用来认证的密码 – 必须#master-password =## master监听的端口.# 可选 – 默认是3306#master-port =# 使得slave只读。只有用户拥有SUPER权限和在上面的slave线程能够修改数据。# 你可以使用此项去保证没有应用程序会意外的修改slave而不是master上的数据#read_only mysqld INNODB相关选项# 如果你的 MySQL 服务包含 InnoDB 支持但是并不打算使用的话,# 使用此选项会节省内存以及磁盘空间,并且加速某些部分#skip-innodb# 附加的内存池被 InnoDB 用来保存 metadata 信息(5.6中不再推荐使用)# 如果 InnoDB 为此目的需要更多的内存,它会开始从 OS 这里申请内存.# 由于这个操作在大多数现代操作系统上已经足够快, 你一般不需要修改此值.# SHOW INNODB STATUS 命令会显示当先使用的数量.#****(此属性对当前环境无效)****#innodb_additional_mem_pool_size = 64M# InnoDB使用一个缓冲池来保存索引和原始数据, 不像 MyISAM.# 这里你设置越大,这能保证你在大多数的读取操作时使用的是内存而不是硬盘,在存取表里面数据时所需要的磁盘 I/O 越少.# 在一个独立使用的数据库服务器上,你可以设置这个变量到服务器物理内存大小的80%# 不要设置过大,否则,由于物理内存的竞争可能导致操作系统的换页颠簸.# 注意在32位系统上你每个进程可能被限制在 2-3.5G 用户层面内存限制,# 所以不要设置的太高.innodb_buffer_pool_size = 6G# InnoDB 将数据保存在一个或者多个数据文件中成为表空间.# 如果你只有单个逻辑驱动保存你的数据,一个单个的自增文件就足够好了.# 其他情况下.每个设备一个文件一般都是个好的选择.# 你也可以配置 InnoDB 来使用裸盘分区 – 请参考手册来获取更多相关内容innodb_data_file_path = ibdata1:10M:autoextend# 设置此选项如果你希望InnoDB表空间文件被保存在其他分区.# 默认保存在MySQL的datadir中.#innodb_data_home_dir =# 用来同步IO操作的IO线程的数量.# 此值在Unix下被硬编码为8,但是在Windows磁盘I/O可能在一个大数值下表现的更好.#innodb_file_io_threads = 8# 如果你发现 InnoDB 表空间损坏, 设置此值为一个非零值可能帮助你导出你的表.# 从1开始并且增加此值知道你能够成功的导出表.#innodb_force_recovery=1# 在 InnoDb 核心内的允许线程数量.# 最优值依赖于应用程序,硬件以及操作系统的调度方式.# 过高的值可能导致线程的互斥颠簸.innodb_thread_concurrency = 16# 如果设置为1 ,InnoDB 会在每次提交后刷新(fsync)事务日志到磁盘上,# 这提供了完整的 ACID 行为.# 如果你愿意对事务安全折衷, 并且你正在运行一个小的事物, 你可以设置此值到0或者2来减少由事务日志引起的磁盘I/O# 0代表日志只大约每秒写入日志文件并且日志文件刷新到磁盘.# 2代表日志写入日志文件在每次提交后,但是日志文件只有大约每秒才会刷新到磁盘上.innodb_flush_log_at_trx_commit = 2#（说明：如果是游戏服务器，建议此值设置为2；如果是对数据安全要求极高的应用，建议设置为1；#设置为0性能最高，但如果发生故障，数据可能会有丢失的危险！#默认值1的意思是每一次事务提交或事务外的指令都需要把日志写入（flush）硬盘，这是很费时的。#特别是使用电池供电缓存（Battery backed up cache）时。#设成2对于很多运用，特别是从MyISAM表转过来的是可以的，它的意思是不写入硬盘而是写入系统缓存。#日志仍然会每秒flush到硬盘，所以你一般不会丢失超过1-2秒的更新。#设成0会更快一点，但安全方面比较差，即使MySQL挂了也可能会丢失事务的数据。而值2只会在整个操作系统挂了时才可能丢数据。）# 加速 InnoDB 的关闭. 这会阻止 InnoDB 在关闭时做全清除以及插入缓冲合并.# 这可能极大增加关机时间, 但是取而代之的是 InnoDB 可能在下次启动时做这些操作.#innodb_fast_shutdown# 用来缓冲日志数据的缓冲区的大小.# 当此值快满时, InnoDB 将必须刷新数据到磁盘上.# 由于基本上每秒都会刷新一次,所以没有必要将此值设置的太大(甚至对于长事务而言)innodb_log_buffer_size = 16M# 在日志组中每个日志文件的大小.# 你应该设置日志文件总合大小到你缓冲池大小的25%~100%# 来避免在日志文件覆写上不必要的缓冲池刷新行为.# 不论如何, 请注意一个大的日志文件大小会增加恢复进程所需要的时间.innodb_log_file_size = 512M# 在日志组中的文件总数.# 通常来说2~3是比较好的.innodb_log_files_in_group = 3# InnoDB 的日志文件所在位置. 默认是 MySQL 的 datadir.# 你可以将其指定到一个独立的硬盘上或者一个RAID1卷上来提高其性能#innodb_log_group_home_dir# 在 InnoDB 缓冲池中最大允许的脏页面的比例.# 如果达到限额, InnoDB 会开始刷新他们防止他们妨碍到干净数据页面.# 这是一个软限制,不被保证绝对执行.innodb_max_dirty_pages_pct = 90# InnoDB 用来刷新日志的方法.# 表空间总是使用双重写入刷新方法# 默认值是 “fdatasync”, 另一个是 “O_DSYNC”.# 一般来说，如果你有硬件 RAID 控制器，并且其独立缓存采用 write-back 机制，并有着电池断电保护，那么应该设置配置为 O_DIRECT# 否则，大多数情况下应将其设为 fdatasync#innodb_flush_method=fdatasync# 在被回滚前,一个 InnoDB 的事务应该等待一个锁被批准多久.# InnoDB 在其拥有的锁表中自动检测事务死锁并且回滚事务.# 如果你使用 LOCK TABLES 指令, 或者在同样事务中使用除了 InnoDB 以外的其他事务安全的存储引擎# 那么一个死锁可能发生而 InnoDB 无法注意到.# 这种情况下这个 timeout 值对于解决这种问题就非常有帮助.innodb_lock_wait_timeout = 120# 这项设置告知InnoDB是否需要将所有表的数据和索引存放在共享表空间里（innodb_file_per_table = OFF） 或者为每张表的数据单独放在一个.ibd文件（innodb_file_per_table = ON）# 每张表一个文件允许你在drop、truncate或者rebuild表时回收磁盘空间# 这对于一些高级特性也是有必要的，比如数据压缩,但是它不会带来任何性能收益innodb_file_per_table = on mysqldump配置[mysqldump]# 不要在将内存中的整个结果写入磁盘之前缓存. 在导出非常巨大的表时需要此项quick max_allowed_packet = 32M mysql客户端[mysql]no-auto-rehashdefault-character-set=utf8# 仅仅允许使用键值的 UPDATEs 和 DELETEs .safe-updates myisamchk配置[myisamchk]key_buffer = 16Msort_buffer_size = 16Mread_buffer = 8Mwrite_buffer = 8M mysqlhotcopy配置[mysqlhotcopy]interactive-timeout mysqld_safe配置[mysqld_safe]# 增加每个进程的可打开文件数量.# 警告: 确认你已经将全系统限制设定的足够高!# 打开大量表需要将此值设大open-files-limit = 8192 MySQL客户端[client]default-character-set=utf8","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://youxia999.github.io/tags/mysql/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"【java应用开发脚手架系列】spring boot 1.4.0体验","slug":"spring_boot_1.4.0_exprience","date":"2016-07-30T09:13:39.000Z","updated":"2019-12-27T11:59:45.729Z","comments":true,"path":"2016/07/30/spring_boot_1.4.0_exprience/","link":"","permalink":"https://youxia999.github.io/2016/07/30/spring_boot_1.4.0_exprience/","excerpt":"","text":"本文参考资料https://docs.spring.io/spring/docs/4.3.0.RELEASE/spring-framework-reference/htmlsingle/https://docs.spring.io/spring/docs/4.3.0.RELEASE/spring-framework-reference/htmlsingle/#mvc-ann-methodshttps://github.com/spring-projects/spring-boot/wiki/Spring-Boot-1.4-Release-Noteshttps://docs.spring.io/spring-boot/docs/1.4.0.RELEASE/reference/htmlsingle/","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"redis通信协议及与mysql互通","slug":"redis-protocol","date":"2016-07-21T11:58:10.000Z","updated":"2020-02-06T04:16:21.873Z","comments":true,"path":"2016/07/21/redis-protocol/","link":"","permalink":"https://youxia999.github.io/2016/07/21/redis-protocol/","excerpt":"","text":"本文参考资料https://redis.io/topics/protocol redis-cli命令与redis通信协议redis-cli命令redis命令在从提交到返回处理结果的过程中，消耗的时间我们称之为RTT（往返时间）。在需要批量执行redis 命令的场景下，如果命令单条逐个执行，那么总共花费的时间是命令条数 N * RTT。redis 提供了管道技术来提高批量执行效率，即将多个命令打包发送给redis服务端，所有命令执行完后，再将所有结果打包返回。在所有命令执行结束前，redis服务器会缓存已执行结束的结果。在redis-cli命令行中，使用redis管道技术时，我们通常将待执行的命令放到一个文本里，比如commands.txt，然后使用命令：cat commands.txt | redis-cli --pipe 去读取文本里的命令，然后打包已pipe管道的方式发送给redis服务端。其他命令见 redis通信协议Redis服务器与客户端通过RESP（REdis Serialization Protocol）协议通信。协议用 \\r\\n 做间隔。对于简单的字符串，以 + 开头，例如 :+OK\\r\\n。对于错误消息，以 - 开头 ，例如:-ERR unknown command &apos;foobar&apos;\\r\\n-WRONGTYPE Operation against a key holding the wrong kind of value\\r\\n 对于整数，以:开头，例如::100\\r\\n 对于大字符串，以$开头，接着跟上字符串长度的数字。 最长512MB 。 例如:$6\\r\\nfoobar\\r\\n 代表一个长6的字符串， foobar $0\\r\\n\\r\\n 长度为0 的空字符串$-1\\r\\n Null 对于数组， 以 * 开头， 接上数组元素个数。 加数组元素*0\\r\\n 一个空的数组*2\\r\\n$3\\r\\nfoo\\r\\n$3\\r\\nbar\\r\\n 一个有两个元素的数组 foo bar 数组可以有更多复杂的用法，具体的建议去看官方文档。此处就不一一介绍了 go与redis通信（2018年补充）func main() &#123; tcpAddr, err := net.ResolveTCPAddr(&quot;tcp4&quot;, &quot;10.2.1.30:6379&quot;) conn, err := net.DialTCP(&quot;tcp&quot;, nil, tcpAddr) if err != nil &#123; fmt.Println(err, conn) return &#125; req := &quot;*3\\r\\n&quot; + &quot;$3\\r\\n&quot; + &quot;set\\r\\n&quot; + &quot;$3\\r\\n&quot; + &quot;foo\\r\\n&quot; + &quot;$3\\r\\n&quot; + &quot;bar\\r\\n&quot; conn.Write([]byte(req)) req = &quot;*2\\r\\n&quot; + &quot;$3\\r\\n&quot; + &quot;get\\r\\n&quot; + &quot;$3\\r\\n&quot; + &quot;foo\\r\\n&quot; buffer := make([]byte, 2048) conn.Write([]byte(req)) time.Sleep(10 * time.Millisecond) conn.Read(buffer) fmt.Println(string(buffer))&#125; mysql数据导入redismysql数据导入redis创建表DROP TABLE IF EXISTS `t_area`;CREATE TABLE `t_area` ( `id` int(11) NOT NULL AUTO_INCREMENT, `pid` int(11) NOT NULL, `areaname` varchar(255) NOT NULL, `arealevel` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;-- ------------------------------ Records of t_area-- ----------------------------INSERT INTO `t_area` VALUES (&apos;1&apos;, &apos;0&apos;, &apos;北京&apos;, &apos;0&apos;);INSERT INTO `t_area` VALUES (&apos;2&apos;, &apos;0&apos;, &apos;上海&apos;, &apos;0&apos;);INSERT INTO `t_area` VALUES (&apos;3&apos;, &apos;0&apos;, &apos;广州&apos;, &apos;0&apos;);INSERT INTO `t_area` VALUES (&apos;4&apos;, &apos;0&apos;, &apos;深圳&apos;, &apos;0&apos;);INSERT INTO `t_area` VALUES (&apos;5&apos;, &apos;0&apos;, &apos;杭州&apos;, &apos;0&apos;); 编写脚本SELECT CONCAT( &quot;*4\\r\\n&quot;, &apos;$&apos;, LENGTH(redis_cmd), &apos;\\r\\n&apos;, redis_cmd, &apos;\\r\\n&apos;, &apos;$&apos;, LENGTH(redis_key), &apos;\\r\\n&apos;, redis_key, &apos;\\r\\n&apos;, &apos;$&apos;, LENGTH(hkey), &apos;\\r\\n&apos;, hkey, &apos;\\r\\n&apos;, &apos;$&apos;, LENGTH(hval), &apos;\\r\\n&apos;, hval, &apos;\\r&apos;)FROM ( SELECT &apos;HSET&apos; as redis_cmd, CONCAT(&apos;pid:&apos; ,pid) AS redis_key, id AS hkey, areaname AS hval FROM t_area) AS t; 测试脚本mysql -uroot -p -h mysql服务器所在ip 数据库名 --skip-column-names --raw &lt; redis.sql 正式执行mysql -uroot -p -h mysql服务器所在ip 数据库名 --skip-column-names --raw &lt; redis.sql |/usr/local/redis-3.2.12/bin/redis-cli -p 6379 --pipe","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://youxia999.github.io/tags/redis/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"redis3.2.1安装和基本操作","slug":"redis3-2-1-install","date":"2016-07-21T09:07:03.000Z","updated":"2020-02-06T04:16:21.895Z","comments":true,"path":"2016/07/21/redis3-2-1-install/","link":"","permalink":"https://youxia999.github.io/2016/07/21/redis3-2-1-install/","excerpt":"","text":"简单介绍官网介绍（最后更新于2019-02-01）:Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker.It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes with radius queries and streams.Redis has built-in replication, Lua scripting, LRU eviction, transactions and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster. redis安装与配置解压、编译wget http://download.redis.io/releases/redis-3.2.1.tar.gztar xvf redis-3.2.1.tar.gz cd redis-3.2.1make &amp;&amp; make installmkdir -p /usr/local/redis/binmkdir -p /usr/local/redis/etccp redis.conf /usr/local/redis/etc/cp src/redis-server src/redis-cli src/redis-benchmark /usr/local/redis/bin/cd /usr/local/redis/bin/ 编辑配置文件修改redis配置文件内容vim /usr/local/redis/etc/redis.confbind 192.168.128.199logfile /var/log/redis/redis.log 加入到环境变量echo “export PATH=$PATH:/usr/local/redis/bin/“ /etc/profilesource /etc/profile 启动/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf 开始操作[root@localhost bin]# redis-cli -h 192.168.128.199192.168.128.199:6379&gt; echo helloredis&quot;helloredis&quot; redis常用数据类型和操作命令正如官网上提到的，redis支持多种数据类型:strings, hashes, lists, sets, sorted sets。命令总览: string适用场景和命令hash适用场景和命令list适用场景和命令list的特性适用于哪些先进先出、先进后出的排序。 set适用场景和命令 sorted sets适用场景和命令","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://youxia999.github.io/tags/redis/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"redis3.2.1配置文件解读","slug":"redis-config-introduce","date":"2016-07-21T02:34:49.000Z","updated":"2020-02-06T04:16:21.863Z","comments":true,"path":"2016/07/21/redis-config-introduce/","link":"","permalink":"https://youxia999.github.io/2016/07/21/redis-config-introduce/","excerpt":"","text":"本文参考http://yijiebuyi.com/blog/bc2b3d3e010bf87ba55267f95ab3aa71.htmlhttp://download.redis.io/releases/ redis3.2.1配置文件解读redis配置文件分13部分 文件引用 网络设置 通用配置 数据快照 主从同步 安全 限制 只追加模型 lua脚本配置 集群 慢日志查询 延迟监控 事件通知 高级配置 引用# 不同redis server可以使用同一个模版配置作为主配置，并引用其它配置文件用于本server的个性化设置# include并不会被CONFIG REWRITE命令覆盖。但是主配置文件的选项会被覆盖。# 想故意覆盖主配置的话就把include放文件前面，否则最好放末尾# include /path/to/local.conf# include /path/to/other.conf 网络# 不指定bind的话redis将会监听所有网络接口。这个配置是肯定需要指定的。# Examples:# bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1# 下面这个配置是只允许本地客户端访问。bind 127.0.0.1# 是否开启保护模式。默认开启，如果没有设置bind项的ip和redis密码的话，服务将只允许本地访 问。protected-mode yes# 端口设置，默认为 6379# 如果port设置为0 redis将不会监听tcp socketport 6379# 在高并发环境下需要一个高backlog值来避免慢客户端连接问题。注意Linux内核默默将这个值减小到/proc/sys/net/core/somaxconn的值，# 所以需要确认增大somaxconn和tcp_max_syn_backlog 两个值来达到需要的效果。tcp-backlog 511# 指定用来监听Unix套套接字的路径。没有默认值，没有指定的情况下Redis不会监听Unix socket# unixsocket /tmp/redis.sock# unixsocketperm 700# 客户端空闲多少秒后关闭连接（0为不关闭）timeout 0# tcp-keepalive设置。如果非零，则设置SO_KEEPALIVE选项来向空闲连接的客户端发送ACK，用途如下：# 1）能够检测无响应的对端# 2）让该连接中间的网络设备知道这个连接还存活# 在Linux上，这个指定的值(单位秒)就是发送ACK的时间间隔。注意：要关闭这个连接需要两倍的这个时间值。# 在其他内核上这个时间间隔由内核配置决定。从redis3.2.1开始默认值为300秒tcp-keepalive 300 通用# 是否将Redis作为守护进程运行。如果需要的话配置成&apos;yes&apos;。注意配置成守护进程后，Redis会将进程号写入文件/var/run/redis.piddaemonize no# 是否通过upstart或systemd管理守护进程。默认no没有服务监控，其它选项有upstart, systemd, autosupervised no# pid文件在redis启动时创建，退出时删除。最佳实践为配置该项。pidfile /var/run/redis_6379.pid# 配置日志级别。选项有debug, verbose, notice, warningloglevel notice# 日志名称。空字符串表示标准输出。注意如果redis配置为后台进程，标准输出中信息会发送到/dev/nulllogfile /var/log/redis/redis.log# 是否启动系统日志记录。# syslog-enabled no# 指定系统日志身份。# syslog-ident redis# 指定syslog设备。必须是user或LOCAL0 ~ LOCAL7之一。# syslog-facility local0# 设置数据库个数。默认数据库是 DB 0# 可以通过SELECT where dbid is a number between 0 and &apos;databases&apos;-1为每个连接使用不同的数据库。databases 16 数据快照# 持久化设置:# 下面的例子将会进行把数据写入磁盘的操作:# 900秒（15分钟）之后，且至少1次变更# 300秒（5分钟）之后，且至少10次变更# 60秒之后，且至少10000次变更# 不写磁盘的话就把所有 &quot;save&quot; 设置注释掉就行了。# 通过添加一条带空字符串参数的save指令也能移除之前所有配置的save指令，如: save &quot;&quot;save 900 1save 300 10save 60 10000# 默认情况下如果上面配置的RDB模式开启且最后一次的保存失败，redis 将停止接受写操作，让用户知道问题的发生。# 如果后台保存进程重新启动工作了，redis 也将自动的允许写操作。如果有其它监控方式也可关闭。stop-writes-on-bgsave-error yes# 是否在备份.rdb文件时是否用LZF压缩字符串，默认设置为yes。如果想节约cpu资源可以把它设置为no。rdbcompression yes# 因为版本5的RDB有一个CRC64算法的校验和放在了文件的末尾。这将使文件格式更加可靠,# 但在生产和加载RDB文件时，这有一个性能消耗(大约10%)，可以关掉它来获取最好的性能。# 生成的关闭校验的RDB文件有一个0的校验和，它将告诉加载代码跳过检查rdbchecksum yes# rdb文件名称dbfilename dump.rdb# 备份文件目录，文件名就是上面的 &quot;dbfilename&quot; 的值。累加文件也放这里。# 注意你这里指定的必须是目录，不是文件名。dir /Users/wuji/redis_data/ 主从同步# 主从同步配置。# 1) redis主从同步是异步的，但是可以配置在没有指定slave连接的情况下使master停止写入数据。# 2) 连接中断一定时间内，slave可以执行部分数据重新同步。# 3) 同步是自动的，slave可以自动重连且同步数据。# slaveof &lt;masterip&gt; &lt;masterport&gt;# master连接密码# masterauth &lt;master-password&gt;# 当一个slave失去和master的连接，或者同步正在进行中，slave的行为有两种可能：# 1) 如果 slave-serve-stale-data 设置为 &quot;yes&quot; (默认值)，slave会继续响应客户端请求，可能是正常数据，也可能是还没获得值的空数据。# 2) 如果 slave-serve-stale-data 设置为 &quot;no&quot;，slave会回复&quot;正在从master同步（SYNC with master in progress）&quot;来处理各种请求，除了 INFO 和 SLAVEOF 命令。slave-serve-stale-data yes# 你可以配置salve实例是否接受写操作。可写的slave实例可能对存储临时数据比较有用(因为写入salve# 的数据在同master同步之后将很容被删除)，但是如果客户端由于配# 置错误在写入时也可能产生一些问题。# 从Redis2.6默认所有的slave为只读# 注意:只读的slave不是为了暴露给互联网上不可信的客户端而设计的。它只是一个防止实例误用的保护层。# 一个只读的slave支持所有的管理命令比如config,debug等。为了限制你可以用&apos;rename-command&apos;来隐藏所有的管理和危险命令来增强只读slave的安全性。slave-read-only yes# 同步策略: 磁盘或socket，默认磁盘方式repl-diskless-sync no# 如果非磁盘同步方式开启，可以配置同步延迟时间，以等待master产生子进程通过socket传输RDB数据给slave。# 默认值为5秒，设置为0秒则每次传输无延迟。repl-diskless-sync-delay 5# slave根据指定的时间间隔向master发送ping请求。默认10秒。# repl-ping-slave-period 10# 同步的超时时间# 1）slave在与master SYNC期间有大量数据传输，造成超时# 2）在slave角度，master超时，包括数据、ping等# 3）在master角度，slave超时，当master发送REPLCONF ACK pings# 确保这个值大于指定的repl-ping-slave-period，否则在主从间流量不高时每次都会检测到超时# repl-timeout 60# 是否在slave套接字发送SYNC之后禁用 TCP_NODELAY# 如果选择yes，Redis将使用更少的TCP包和带宽来向slaves发送数据。但是这将使数据传输到slave上有延迟，Linux内核的默认配置会达到40毫秒。# 如果选择no，数据传输到salve的延迟将会减少但要使用更多的带宽。# 默认我们会为低延迟做优化，但高流量情况或主从之间的跳数过多时，可以设置为“yes”。repl-disable-tcp-nodelay no# 设置数据备份的backlog大小。# backlog是一个slave在一段时间内断开连接时记录salve数据的缓冲，所以一个slave在重新连接时，不必要全量的同步，而是一个增量同步就足够了，# 将在断开连接的这段时间内把slave丢失的部分数据传送给它。同步的backlog越大，slave能够进行增量同步并且允许断开连接的时间就越长。# backlog只分配一次并且至少需要一个slave连接。# repl-backlog-size 1mb# 当master在一段时间内不再与任何slave连接，backlog将会释放。以下选项配置了从最后一个# slave断开开始计时多少秒后，backlog缓冲将会释放。# 0表示永不释放backlog# repl-backlog-ttl 3600# slave的优先级是一个整数展示在Redis的Info输出中。如果master不再正常工作了，sentinel将用它来选择一个slave提升为master。# 优先级数字小的salve会优先考虑提升为master，所以例如有三个slave优先级分别为10，100，25，sentinel将挑选优先级最小数字为10的slave。# 0作为一个特殊的优先级，标识这个slave不能作为master，所以一个优先级为0的slave永远不会被# sentinel挑选提升为master。# 默认优先级为100slave-priority 100# 如果master少于N个延时小于等于M秒的已连接slave，就可以停止接收写操作。# N个slave需要是“oneline”状态。# 延时是以秒为单位，并且必须小于等于指定值，是从最后一个从slave接收到的ping（通常每秒发送）开始计数。# 该选项不保证N个slave正确同步写操作，但是限制数据丢失的窗口期。# 例如至少需要3个延时小于等于10秒的slave用下面的指令：# min-slaves-to-write 3# min-slaves-max-lag 10# 两者之一设置为0将禁用这个功能。# 默认 min-slaves-to-write 值是0（该功能禁用）并且 min-slaves-max-lag 值是10。 安全# 要求客户端在处理任何命令时都要验证身份和密码。# requirepass foobared# 命令重命名# 在共享环境下，可以为危险命令改变名字。比如，你可以为 CONFIG 改个其他不太容易猜到的名字，这样内部的工具仍然可以使用。# 例如：# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52# 也可以通过改名为空字符串来完全禁用一个命令# rename-command CONFIG &quot;&quot;# 请注意：改变命令名字被记录到AOF文件或被传送到从服务器可能产生问题。 限制# 设置最多同时连接的客户端数量。默认这个限制是10000个客户端，然而如果Redis服务器不能配置# 处理文件的限制数来满足指定的值，那么最大的客户端连接数就被设置成当前文件限制数减32（因为Redis服务器保留了一些文件描述符作为内部使用）# 一旦达到这个限制，Redis会关闭所有新连接并发送错误&apos;max number of clients reached&apos;# maxclients 10000# 不要使用比设置的上限更多的内存。一旦内存使用达到上限，Redis会根据选定的回收策略（参见：maxmemmory-policy）删除key。# 如果因为删除策略Redis无法删除key，或者策略设置为 &quot;noeviction&quot;，Redis会回复需要更多内存的错误信息给命令。例如，SET,LPUSH等等，但是会# 继续响应像Get这样的只读命令。# 在使用Redis作为LRU缓存，或者为实例设置了硬性内存限制的时候（使用 &quot;noeviction&quot; 策略）的时候，这个选项通常事很有用的。# 警告：当有多个slave连上达到内存上限时，master为同步slave的输出缓冲区所需内存不计算在使用内存中。这样当移除key时，就不会因网络问题 / # 重新同步事件触发移除key的循环，反过来slaves的输出缓冲区充满了key被移除的DEL命令，这将触发删除更多的key，直到这个数据库完全被清空为止。# 总之，如果你需要附加多个slave，建议你设置一个稍小maxmemory限制，这样系统就会有空闲的内存作为slave的输出缓存区(但是如果最大内存策略# 设置为&quot;noeviction&quot;的话就没必要了)# maxmemory &lt;bytes&gt;# 最大内存策略：如果达到内存限制了，Redis如何选择删除key。# volatile-lru -&gt; 根据LRU算法删除设置过期时间的key# allkeys-lru -&gt; 根据LRU算法删除任何key# volatile-random -&gt; 随机移除设置过过期时间的key# allkeys-random -&gt; 随机移除任何key# volatile-ttl -&gt; 移除即将过期的key(minor TTL)# noeviction -&gt; 不移除任何key，只返回一个写错误# 注意：对所有策略来说，如果Redis找不到合适的可以删除的key都会在写操作时返回一个错误。# 目前为止涉及的命令：set setnx setex append incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd sinter sinterstore #sunion sunionstore sdiff sdiffstore zadd zincrby zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby getset mset msetnx exec sort# 默认策略:# maxmemory-policy noeviction# LRU和最小TTL算法的实现都不是很精确，但是很接近（为了省内存），所以你可以用样本量做检测。 例如：默认Redis会检查3个key然后取最旧的那个，你可以通过下面的# 配置指令来设置样本的个数。默认值为5，数字越大结果越精确但是会消耗更多CPU。# maxmemory-samples 5 只追加模型# 默认情况下，Redis是异步的把数据导出到磁盘上。这种模式在很多应用里已经足够好，但Redis进程出问题或断电时可能造成一段时间的写操作丢失(这取决于配置的save指令)。# AOF是一种提供了更可靠的替代持久化模式，例如使用默认的数据写入文件策略（参见后面的配置）。# 在遇到像服务器断电或单写情况下Redis自身进程出问题但操作系统仍正常运行等突发事件时，Redis能只丢失1秒的写操作。# AOF和RDB持久化能同时启动并且不会有问题。# 如果AOF开启，那么在启动时Redis将加载AOF文件，它更能保证数据的可靠性。appendonly no# AOF文件名（默认：&quot;appendonly.aof&quot;）appendfilename &quot;appendonly.aof&quot;# fsync()系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。# Redis支持三种不同的模式：# no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。# always：每次写操作都立刻写入到aof文件。慢，但是最安全。# everysec：每秒写一次。折中方案。# 默认的 &quot;everysec&quot; 通常来说能在速度和数据安全性之间取得比较好的平衡。# appendfsync alwaysappendfsync everysec# appendfsync no# 如果AOF的同步策略设置成 &quot;always&quot; 或者 &quot;everysec&quot;，并且后台的存储进程（后台存储或写入AOF 日志）会产生很多磁盘I/O开销。某些Linux的配置下会使Redis因为 # fsync()系统调用而阻塞很久。 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。# 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止fsync()。# 这就意味着如果有子进程在进行保存操作，那么Redis就处于&quot;不可同步&quot;的状态。# 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定）# 如果把这个设置成&quot;yes&quot;带来了延迟问题，就保持&quot;no&quot;，这是保存持久数据的最安全的方式。no-appendfsync-on-rewrite no# 自动重写AOF文件。如果AOF日志文件增大到指定百分比，Redis能够通过 BGREWRITEAOF 自动重写AOF日志文件。# 工作原理：Redis记住上次重写时AOF文件的大小（如果重启后还没有写操作，就直接用启动时的AOF大小）# 这个基准大小和当前大小做比较。如果当前大小超过指定比例，就会触发重写操作。你还需要指定被重写日志的最小尺寸，# 这样避免了达到指定百分比但尺寸仍然很小的情况还要重写。# 指定百分比为0会禁用AOF自动重写特性。auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 如果设置为yes，如果一个因异常被截断的AOF文件被redis启动时加载进内存，redis将会发送日志通知用户。如果设置为no，erdis将会拒绝启动。# 此时需要用&quot;redis-check-aof&quot;工具修复文件。aof-load-truncated yes 集群# 只有开启了以下选项，redis才能成为集群服务的一部分# cluster-enabled yes# 配置redis自动生成的集群配置文件名。确保同一系统中运行的各redis实例该配置文件不要重名。# cluster-config-file nodes-6379.conf# 集群节点超时毫秒数。超时的节点将被视为不可用状态。# cluster-node-timeout 15000# 如果数据太旧，集群中的不可用master的slave节点会避免成为备用master。如果slave和master失联时间超过: # (node-timeout * slave-validity-factor) + repl-ping-slave-period则不会被提升为master。# 如node-timeout为30秒，slave-validity-factor为10, 默认default repl-ping-slave-period为10秒,失联时间超过310秒slave就不会成为master。# 较大的slave-validity-factor值可能允许包含过旧数据的slave成为master，同时较小的值可能会阻止集群选举出新master。#为了达到最大限度的高可用性，可以设置为0，即slave不管和master失联多久都可以提升为master# cluster-slave-validity-factor 10# 只有在之前master有其它指定数量的工作状态下的slave节点时，slave节点才能提升为master。默认为1#（即该集群至少有3个节点，1 master＋2 slaves，master宕机，仍有另外1个slave的情况下其中1个slave可以提升）# 测试环境可设置为0，生成环境中至少设置为1# cluster-migration-barrier 1# 默认情况下如果redis集群如果检测到至少有1个hash slot不可用，集群将停止查询数据。如果所有slot恢复则集群自动恢复。# 如果需要集群部分可用情况下仍可提供查询服务，设置为no。# cluster-require-full-coverage yes 慢查询日志# 慢查询日志，记录超过多少微秒的查询命令。查询的执行时间不包括客户端的IO执行和网络通信时间，只是查询命令执行时间。# 1000000等于1秒，设置为0则记录所有命令slowlog-log-slower-than 10000# 记录大小，可通过SLOWLOG RESET命令重置slowlog-max-len 128 延迟监控事件通知高级配置# 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，# hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值# Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，# 这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,# 当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。hash-max-zipmap-entries 512hash-max-zipmap-value 64 # list数据类型多少节点以下会采用去指针的紧凑存储格式。# list数据类型节点值大小小于多少字节会采用紧凑存储格式。list-max-ziplist-entries 512list-max-ziplist-value 64 # set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。set-max-intset-entries 512 # zsort数据类型多少节点以下会采用去指针的紧凑存储格式。# zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。zset-max-ziplist-entries 128zset-max-ziplist-value 64 # Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用# # 当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。## 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存activerehashing yes 附录：redis3.2.1配置文件(全)# Redis configuration file example.## Note that in order to read the configuration file, Redis must be started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### include /path/to/local.conf# include /path/to/other.conf################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the following bind directive, that will force Redis to listen only into# the IPv4 lookback interface address (this means Redis will be able to accept connections only from clients running into the same computer it is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 192.168.128.199# Protected mode is a layer of security protection, in order to avoid that Redis instances left open on the internet are accessed and exploited.# When protected mode is on and if:# 1) The server is not binding explicitly to a set of addresses using the &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain sockets.# By default protected mode is enabled. You should disable it only if you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces are explicitly listed using the &quot;bind&quot; directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344). If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence of communication. This is useful for two reasons:# 1) Detect dead peers.# 2) Take the connection alive from the point of view of network equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &apos;yes&apos; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup and removes it at exit.## When the server runs non daemonized, no pid file is created if none is specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level. This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes, and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &apos;databases&apos;-1databases 16################################ SNAPSHOTTING ################################## Save the DB on disk:# save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will automatically allow writes again.## However if you have setup your proper monitoring of the Redis server and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk, permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases? For default that&apos;s set to &apos;yes&apos; as it&apos;s almost always a win.# If you want to save some CPU in the saving child set it to &apos;no&apos; but the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file. This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &apos;dbfilename&apos; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Slave replication. Use slaveof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of slaves.# 2) Redis slaves are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition slaves automatically try to reconnect to masters# and resynchronize with them.## slaveof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the slave to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the slave request.## masterauth &lt;master-password&gt;# When a slave loses its connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to &apos;yes&apos; (the default) the slave will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale-data is set to &apos;no&apos; the slave will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO and SLAVEOF.#slave-serve-stale-data yes# You can configure a slave instance to accept writes or not. Writing against# a slave instance may be useful to store some ephemeral data (because data# written on a slave will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default slaves are read-only.## Note: read only slaves are not designed to be exposed to untrusted clients# on the internet. It&apos;s just a protection layer against misuse of the instance.# Still a read only slave exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only slaves using &apos;rename-command&apos; to shadow all the# administrative / dangerous commands.slave-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New slaves and reconnecting slaves that are not able to continue the replication# process just receiving differences, need to do what is called a &quot;full# synchronization&quot;. An RDB file is transmitted from the master to the slaves.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the slaves incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to slave sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more slaves# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new slaves arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple slaves# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the slaves.## This is important since once the transfer starts, it is not possible to serve# new slaves arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more slaves arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Slaves send PINGs to server in a predefined interval. It&apos;s possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of slave.# 2) Master timeout from the point of view of slaves (data, pings).# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60# Disable TCP_NODELAY on the slave socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to slaves. But this can add a delay for# the data to appear on the slave side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the slave side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and slaves are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# slave data when slaves are disconnected for some time, so that when a slave# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the slave missed while# disconnected.## The bigger the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a slave connected.## repl-backlog-size 1mb# After a master has no longer connected slaves for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last slave disconnected, for# the backlog buffer to be freed.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The slave priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a slave to promote into a# master if the master is no longer working correctly.## A slave with a low priority number is considered better for promotion, so# for instance if there are three slaves with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the slave as not able to perform the# role of master, so a slave with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.slave-priority 100# It is possible for a master to stop accepting writes if there are less than# N slaves connected, having a lag less or equal than M seconds.## The N slaves need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the slave, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough slaves# are available, to the specified number of seconds.## For example to require at least 3 slaves with a lag &lt;= 10 seconds use:## min-slaves-to-write 3# min-slaves-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to slaves may cause problems.################################### LIMITS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &apos;max number of clients reached&apos;.## maxclients 10000# Don&apos;t use more memory than the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&apos;t remove keys according to the policy, or if the policy is# set to &apos;noeviction&apos;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU cache, or to set# a hard memory limit for an instance (using the &apos;noeviction&apos; policy).## WARNING: If you have slaves attached to an instance with maxmemory on,# the size of the output buffers needed to feed the slaves are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of slaves is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is &apos;noeviction&apos;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; remove the key with an expire set using an LRU algorithm# allkeys-lru -&gt; remove any key according to the LRU algorithm# volatile-random -&gt; remove a random key with an expire set# allkeys-random -&gt; remove a random key, any key# volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)# noeviction -&gt; don&apos;t expire at all, just return an error on write operations## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU and minimal TTL algorithms are not precise algorithms but approximated algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was used less recently, you can change the sample size using the following# configuration directive.# The default of 5 produces good enough results. 10 Approximates very closely true LRU but costs a bit more CPU. 3 is very fast but not very accurate.## maxmemory-samples 5############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on the configured save points).# The Append Only File is an alternative persistence mode that provides much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is still running correctly# AOF and RDB persistence can be enabled at the same time without problems. If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.# Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk instead of waiting for more data in the output buffer. Some OS will really # flush data on disk, some other OS will just try to do it ASAP.# Redis supports three different modes:# no: don&apos;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.# The default is &quot;everysec&quot;, as that&apos;s usually the right compromise between speed and data safety. It&apos;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&apos;s snapshotting), or on the contrary, use &quot;always&quot; that&apos;s very slow but a bit safer than # everysec.# More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block our synchronous write(2) call.# In order to mitigate this problem it&apos;s possible to use the following option that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.# This means that while another child is saving, the durability of Redis is the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the default Linux settings).# If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling BGREWRITEAOF when the AOF log size grows by the specified percentage.# This is how it works: Redis remembers the size of the AOF file after the latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).# This base size is compared to the current size. If the current size is bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.# Specify a percentage of zero in order to disable the automatic AOF rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&apos;t happen when Redis itself crashes or aborts but the operating system still works correctly).# Redis can either exit with an error when this happens, or load as much data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.# If aof-load-truncated is set to yes, a truncated AOF file is loaded and the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart the server.## Note that if the AOF file will be found to be corrupted in the middle the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes will be found.aof-load-truncated yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&apos;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as &quot;mature&quot; we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can&apos;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A slave of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a slave to actually have a exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple slaves able to failover, they exchange messages# in order to try to give an advantage to the slave with the best# replication offset (more data from the master processed).# Slaves will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single slave computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the slave will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a slave will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * slave-validity-factor) + repl-ping-slave-period## So for example if node-timeout is 30 seconds, and the slave-validity-factor# is 10, and assuming a default repl-ping-slave-period of 10 seconds, the# slave will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large slave-validity-factor may allow slaves with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a slave at all.## For maximum availability, it is possible to set the slave-validity-factor# to a value of 0, which means, that slaves will always try to failover the# master regardless of the last time they interacted with the master.# (However they&apos;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-slave-validity-factor 10# Cluster slaves are able to migrate to orphaned masters, that are masters# that are left without working slaves. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&apos;t be failed over# in case of failure if it has no working slaves.## Slaves migrate to orphaned masters only if there are still at least a# given number of other working slaves for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a slave# will migrate only if there is at least 1 other working slave for its master# and so forth. It usually reflects the number of slaves you want for every# master in your cluster.## Default is 1 (slaves migrate only if their masters remain with at least# one slave). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&apos;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space. This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don&apos;t need# this feature and the feature has some overhead. Note that if you don&apos;t# specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space. The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements. For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&apos;t start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don&apos;t compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&apos;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&apos;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# slave -&gt; slave clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&apos;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and slave clients, since# subscribers and slaves receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Redis calls an internal function to perform many background tasks, like closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.# Not all tasks are performed with the same frequency, but Redis checks for tasks to perform according to the specified &quot;hz&quot; value.# By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be handled with more precision.# The range is between 1 and 500, however a value over 100 is usually not a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# When a child rewrites the AOF file, if the following option is enabled the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid big latency spikes.aof-rewrite-incremental-fsync yes","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://youxia999.github.io/tags/redis/"},{"name":"中间件技术","slug":"中间件技术","permalink":"https://youxia999.github.io/tags/中间件技术/"}]},{"title":"玩转centos7之局域网文件传输","slug":"centos7-file-transfer","date":"2016-06-15T11:30:16.000Z","updated":"2020-02-06T04:15:12.731Z","comments":true,"path":"2016/06/15/centos7-file-transfer/","link":"","permalink":"https://youxia999.github.io/2016/06/15/centos7-file-transfer/","excerpt":"","text":"在局域网不同的服务器上互传文件，是很平常的事。不过一般分两种情况：知道目标机器的账号密码，不知道目标机器的账号/密码。 知道目标机器的账号密码的情况下这种场景下，一般用scp。scp 文件名 账号@ip:/目录。 不知道目标机器的账号/密码的情况nc安装这种场景下，只能安装nc。(netcat)yum install nmap-ncat nc用法目标机器:nc -l 端口 &gt; 文件名 源机器:nc 目标机器ip 目标机器端口 &lt; 文件名 更多用法参考:","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"玩转centos7之格盘和挂载","slug":"centos7-disk-ops","date":"2016-06-15T11:11:16.000Z","updated":"2020-02-06T04:15:12.723Z","comments":true,"path":"2016/06/15/centos7-disk-ops/","link":"","permalink":"https://youxia999.github.io/2016/06/15/centos7-disk-ops/","excerpt":"","text":"运维同事在虚拟物理机成虚拟机时，一般会有个系统盘(一般就50G)，然后会分配一个数据盘（大小依据业务决定），挂在/data目录下。如果系统盘和数据盘放在一个文件里面，移动的时候，会有大坑。 问题來了重启之后，挂载丢失了。 解决之道格盘 查询uuid 挂载磁盘 修改/etc/fstab 最终效果","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"玩转centos7之目录解读","slug":"centos7-directory-introduce","date":"2016-02-14T11:11:16.000Z","updated":"2020-02-06T04:15:12.714Z","comments":true,"path":"2016/02/14/centos7-directory-introduce/","link":"","permalink":"https://youxia999.github.io/2016/02/14/centos7-directory-introduce/","excerpt":"","text":"安装下载iso文件，然后在vmware里面进行安装。安装完centos7之后，目录结构如下： 目录介绍下面解释下各个目录的作用。 根目录/每一个文件和目录从根目录开始。只有root用户具有该目录下的写权限。请注意，/root是root用户的主目录，这与/.不一样 /boot目录引导加载程序文件目录，包含引导加载程序相关的文件、内核的initrd、vmlinux、grub文件。例如：initramfs-3.10.0-514.el7.x86_64.img。 /bin是一个软链。主要存放：用户二进制文件。包含二进制可执行文件。在单用户模式下，你需要使用的常见Linux命令都位于此目录下。系统的所有用户使用的命令都设在这里。例如：ps、ls、ping、grep、cp /sbin目录是一个软链。主要存放：系统二进制文件就像/bin，/sbin同样也包含二进制可执行文件。但是，在这个目录下的linux命令通常由系统管理员使用，对系统进行维护。例如：iptables、reboot、fdisk、ifconfig、swapon命令 /etc目录主要存放：程序的配置文件(很重要的一个目录)包含所有程序所需的配置文件。也包含了用于启动/停止单个程序的启动和关闭shell脚本。例如：/etc/resolv.conf # dns服务器配置文件/etc/logrotate.conf # 日志管理工具的配置文件/etc/sshd/sshd_config #sshd服务的配置文件/etc/my.cnf # mysql服务器的配置文件 /dev目录主要存放:设备文件。包含设备文件。这些包括终端设备、USB或连接到系统的任何设备。例如：/dev/tty1、/dev/usbmon0 /proc主要存放:进程信息。包含系统进程的相关信息。这是一个虚拟的文件系统，例如：这是一个虚拟的文件系统，系统资源以文本信息形式存在。包含有关正在运行的进程的信息。例如：/proc/uptime # 系统的运行时间/proc/&#123;pid&#125; # 目录中存放包含的与特定pid相关的信息。/proc/sys # 很重要的一个目录。目录中存放与系统相关的一些信息:内核、网络、设备、调试信息等等 表示与给定IP地址所能建立的TCP连接的端口数限制。 /var目录var代表变量文件(很重要的一个目录)。这个目录下可以找到内容可能增长的文件。这包括:系统日志文件（/var/log）;包和数据库文件（/var/lib）;电子邮件（/var/mail）;打印队列（/var/spool）;锁文件（/var/lock）;多次重新启动需要的临时文件（/var/tmp）; 系统盘空间不够用，一般是因为程序的一些日志文件配置到这个目录下。 /tmp临时文件。包含系统和用户创建的临时文件。当系统重新启动时，这个目录下的文件都将被删除。 /usr用户程序 。包含二进制文件、库文件、文档和二级程序的源代码。/usr/bin中包含用户程序的二进制文件。如果你在/bin中找不到用户二进制文件，到/usr/bin目录看看。例如：at、awk、cc、less、scp。/usr/sbin中包含系统管理员的二进制文件。如果你在/sbin中找不到系统二进制文件，到/usr/sbin目录看看。例如：atd、cron、sshd、useradd、userdel。/usr/lib中包含了/usr/bin和/usr/sbin用到的库。/usr/local中包含了从源安装的用户程序。例如，当你从源安装Apache，它会在/usr/local/apache2中。 /home目录HOME目录。所有用户用home目录来存储他们的个人档案。例如：/home/john、/home/nikita /lib目录系统库，软链。包含支持位于/bin和/sbin下的二进制文件的库文件.库文件名为 ld或lib.so.*例如：ld-2.11.1.so，libncurses.so.5.7 /opt目录opt代表可选的、可选的附加应用程序。包含从个别厂商的附加应用程序。附加应用程序应该安装在/opt/或者/opt/的子目录下。 /mnt目录挂载目录。临时安装目录，系统管理员可以挂载文件系统。 /media目录用于挂载可移动设备的临时目录。举例来说，挂载CD-ROM的/media/cdrom，挂载软盘驱动器的/media/floppy; /srv目录服务数据目录，包含服务器特定服务相关的数据。例如，/srv/cvs包含cvs相关的数据。 总结其实，上面这只是系统盘目录，一般运维同事，在创建系统的时候，就给了50G。业务应用程序或者安装耗磁盘空间的程序，是不会放在上述目录的。如果安在上述目录，则每天都要清理磁盘空间，得累死。所以，除了运维同事在创建系统盘的同事，会再挂一块数据盘。一般以/data起头，供程序存放。","categories":[],"tags":[{"name":"CentOS操作","slug":"CentOS操作","permalink":"https://youxia999.github.io/tags/CentOS操作/"},{"name":"DevOps工具","slug":"DevOps工具","permalink":"https://youxia999.github.io/tags/DevOps工具/"}]},{"title":"【转载】jvm的加载与java程序的运行原理","slug":"java-vmloader-and-application-run","date":"2015-08-28T11:41:34.000Z","updated":"2019-12-27T11:59:45.530Z","comments":true,"path":"2015/08/28/java-vmloader-and-application-run/","link":"","permalink":"https://youxia999.github.io/2015/08/28/java-vmloader-and-application-run/","excerpt":"","text":"参考https://blog.csdn.net/on_1y/article/details/38761549http://hg.openjdk.java.net/jdk7u/jdk7u/hotspot/file/78dcbf58e349/ 批注版","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"重学设计模式","slug":"java-relearn-designpattern","date":"2015-08-04T13:07:23.000Z","updated":"2019-12-27T11:59:45.524Z","comments":true,"path":"2015/08/04/java-relearn-designpattern/","link":"","permalink":"https://youxia999.github.io/2015/08/04/java-relearn-designpattern/","excerpt":"","text":"设计模式简介设计模式分为三种类型，共23种： 创建型模式 单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。 结构型模式 适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。 行为型模式 模版方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、职责链模式(责任链模式)、访问者模式。 创建型模式工厂方法模式抽象产品类:public abstract class Product &#123; public void method1()&#123; &#125;;&#125; 具体产品类1：package com.xiaogang.abstractfactory;public class ConcreteProduct1 extends Product &#123;public void method2()&#123; System.out.println(&quot;-------------------ConcreteProduct1 method2----------------&quot;);&#125;&#125; 具体产品类2：package com.xiaogang.abstractfactory;public class ConcreteProduct2 extends Product &#123; public void method2()&#123; System.out.println(&quot;ConcreteProduct2 method2&quot;); &#125;&#125; 抽象工厂类:package com.xiaogang.abstractfactory;public abstract class Creator &#123; public abstract &lt;T extends Product&gt; T creator(Class&lt;T&gt; c);&#125; 具体工厂类:package com.xiaogang.abstractfactory;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class ConcreteCreator extends Creator &#123; @Override public &lt;T extends Product&gt; T creator(Class&lt;T&gt; c) &#123; Product product=null; try &#123; product=(Product) Class.forName(c.getName()).newInstance(); Method method=c.getMethod(&quot;method2&quot;,null); method.invoke(product,null); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; return (T)product; &#125;&#125; 测试类:package com.xiaogang.abstractfactory;public class Client &#123; public static void main(String[] args)&#123; Creator creator=new ConcreteCreator(); Product product=creator.creator(ConcreteProduct1.class); //((ConcreteProduct1) product).method2(); &#125;&#125; 结构型模式代理模式接口：package com.xiaogang.Proxy;public interface IGamePlayer &#123; public void login(String username,String password); public void killBoss(); public void upgrade();&#125; 具体类：package com.xiaogang.Proxy;public class GamePlayer implements IGamePlayer &#123; private String name=&quot;&quot;; public GamePlayer(String name) &#123; this.name = name; &#125; @Override public void login(String username, String password) &#123; System.out.println(&quot;login username: &quot;+username+&quot;,password: &quot;+password); &#125; @Override public void killBoss() &#123; System.out.println(&quot;kill boss&quot;); &#125; @Override public void upgrade() &#123; System.out.println(&quot;upgrade game&quot;); &#125;&#125; 代理类package com.xiaogang.Proxy;public class GamePlayerProxy &#123; private IGamePlayer iGamePlayer; public GamePlayerProxy(IGamePlayer iGamePlayer) &#123; this.iGamePlayer = iGamePlayer; &#125; public void killBoss()&#123; this.iGamePlayer.killBoss(); &#125; public void upgrade()&#123; this.iGamePlayer.upgrade(); &#125; public void login(String username,String password)&#123; this.iGamePlayer.login(username,password); &#125;&#125; 测试类：package com.xiaogang.Proxy;public class Client &#123; public static void main(String[] args) &#123; IGamePlayer gamePlayer=new GamePlayer(&quot;zhangsan&quot;); GamePlayerProxy gamePlayerProxy=new GamePlayerProxy(gamePlayer); gamePlayerProxy.login(&quot;zhangsan&quot;,&quot;password&quot;); &#125;&#125; 行为型模式观察者模式观察者接口:package com.xiaogang;public interface IObserver &#123; void update(String message);&#125; 主题接口:package com.xiaogang;public interface ISubject &#123; void addObserver(IObserver iObserver); void deleteObserver(IObserver iObserver); void inform();&#125; 观察者实现:package com.xiaogang;public class CompleteObserver implements IObserver &#123; @Override public void update(String message) &#123; System.out.println(&quot;observer receive &quot;+message); &#125;&#125; 主题实现:package com.xiaogang;import java.util.Vector;public class CompleteSubject implements ISubject &#123; private Vector&lt;IObserver&gt; observerVector=new Vector&lt;IObserver&gt;(); @Override public void addObserver(IObserver iObserver) &#123; observerVector.add(iObserver); &#125; @Override public void deleteObserver(IObserver iObserver) &#123; observerVector.removeElement(iObserver); &#125; @Override public void inform() &#123; System.out.println(&quot;message&quot;); for (IObserver iObserver:observerVector)&#123; iObserver.update(&quot;message&quot;); &#125; &#125;&#125; 测试类:package com.xiaogang;public class App &#123; public static void main( String[] args ) &#123; System.out.println( &quot;Hello World!&quot; ); ISubject completeSubject=new CompleteSubject(); completeSubject.addObserver(new CompleteObserver()); completeSubject.inform(); &#125;&#125;","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"java反射与动态代理","slug":"java-reflect-dynamic-proxy","date":"2015-08-02T14:09:24.000Z","updated":"2019-12-27T11:59:45.519Z","comments":true,"path":"2015/08/02/java-reflect-dynamic-proxy/","link":"","permalink":"https://youxia999.github.io/2015/08/02/java-reflect-dynamic-proxy/","excerpt":"","text":"动态代理package com.xiaogang.reflect;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;interface Subject&#123; public String say(String name,int age);&#125;class RealSubject implements Subject&#123; @Override public String say(String name, int age) &#123; return name + &quot; &quot; + age; &#125;&#125;class MyInvocationHandler implements InvocationHandler&#123; private Object object=null; public Object bind(Object object)&#123; this.object=object; return Proxy.newProxyInstance(object.getClass().getClassLoader(),object.getClass().getInterfaces(),this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; return method.invoke(this.object,args); &#125;&#125;public class ProxyHello &#123; public static void main(String[] args) &#123; MyInvocationHandler myInvocationHandler=new MyInvocationHandler(); Subject realSubject=(Subject) myInvocationHandler.bind(new RealSubject()); System.out.println(&quot;info:&quot;+realSubject.say(&quot;xiaogang&quot;,20)+&quot;\\n&quot;); &#125;&#125;","categories":[],"tags":[{"name":"编程语言技术","slug":"编程语言技术","permalink":"https://youxia999.github.io/tags/编程语言技术/"},{"name":"java","slug":"java","permalink":"https://youxia999.github.io/tags/java/"}]},{"title":"2014年技术学习总结","slug":"jishuzongjie-2015","date":"2015-01-01T02:35:14.000Z","updated":"2019-12-03T02:42:21.510Z","comments":true,"path":"2015/01/01/jishuzongjie-2015/","link":"","permalink":"https://youxia999.github.io/2015/01/01/jishuzongjie-2015/","excerpt":"","text":"背景本文背景新年了，要盘盘肚子里面的“存货”了。对于过去三年半的技术做一个简单的总结。 本文关键字软件架构模式 分层模式 服务化 技术总结","categories":[],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://youxia999.github.io/tags/杂项/"}]}]}